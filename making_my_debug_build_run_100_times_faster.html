<!--
This file has been auto-generated by main.c/main.bin from a markdown file of the same name.
Do not edit it by hand.
-->
<!DOCTYPE html>
<html>
<head>
<title>Making my debug build run 100x faster so that it is finally usable</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link type="application/atom+xml" href="/blog/feed.xml" rel="self">
<link rel="shortcut icon" type="image/ico" href="/blog/favicon.ico">
<link rel="stylesheet" type="text/css" href="main.css">
<link rel="stylesheet" href="https://unpkg.com/@highlightjs/cdn-assets@11.8.0/styles/default.min.css">
<script src="highlight.min.js"></script>
<!-- From https://github.com/odin-lang/odin-lang.org/blob/6f48c2cfb094a42dffd34143884fa958bd9c0ba2/themes/odin/layouts/partials/head.html#L71 -->
<script src="x86asm.min.js" defer></script>
<script src="main.js" defer></script>
<script type="module" src="search_index.js" defer></script>
<script type="module" src="search.js" defer></script>
</head>
<body>

<div id="banner">
    <div id="name">
        <img id="me" src="me.jpeg">
        <span>Philippe Gaultier</span>
    </div>
    <input id="search" placeholder="üîé Search" autocomplete=off>
    <ul>
      <li> <a href="/blog/body_of_work.html">Body of work</a> </li>
      <li> <a href="/blog/articles-by-tag.html">Tags</a> </li>
      <li> <a href="https://github.com/gaultier/resume/raw/master/Philippe_Gaultier_resume_en.pdf">
          Resume
        </a> </li>

      <li> <a href="/blog/feed.xml">
        <svg viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path fill-rule="evenodd" clip-rule="evenodd" d="M5.5 3.5C4.39543 3.5 3.5 4.39543 3.5 5.5V18.5C3.5 19.6046 4.39543 20.5 5.5 20.5H18.5C19.6046 20.5 20.5 19.6046 20.5 18.5V5.5C20.5 4.39543 19.6046 3.5 18.5 3.5H5.5ZM7 19C8.10457 19 9 18.1046 9 17C9 15.8954 8.10457 15 7 15C5.89543 15 5 15.8954 5 17C5 18.1046 5.89543 19 7 19ZM6.14863 10.5052C6.14863 10.0379 6.52746 9.65906 6.99478 9.65906C7.95949 9.65906 8.91476 9.84908 9.80603 10.2183C10.6973 10.5874 11.5071 11.1285 12.1893 11.8107C12.8715 12.4929 13.4126 13.3027 13.7817 14.194C14.1509 15.0852 14.3409 16.0405 14.3409 17.0052C14.3409 17.4725 13.9621 17.8514 13.4948 17.8514C13.0275 17.8514 12.6486 17.4725 12.6486 17.0052C12.6486 16.2627 12.5024 15.5275 12.2183 14.8416C11.9341 14.1556 11.5177 13.5324 10.9927 13.0073C10.4676 12.4823 9.84437 12.0659 9.15842 11.7817C8.47246 11.4976 7.73726 11.3514 6.99478 11.3514C6.52746 11.3514 6.14863 10.9725 6.14863 10.5052ZM7 5.15385C6.53268 5.15385 6.15385 5.53268 6.15385 6C6.15385 6.46732 6.53268 6.84615 7 6.84615C8.33342 6.84615 9.65379 7.10879 10.8857 7.61907C12.1176 8.12935 13.237 8.87728 14.1799 9.82015C15.1227 10.763 15.8707 11.8824 16.3809 13.1143C16.8912 14.3462 17.1538 15.6666 17.1538 17C17.1538 17.4673 17.5327 17.8462 18 17.8462C18.4673 17.8462 18.8462 17.4673 18.8462 17C18.8462 15.4443 18.5397 13.9039 17.9444 12.4667C17.3491 11.0294 16.4765 9.72352 15.3765 8.6235C14.2765 7.52349 12.9706 6.65091 11.5333 6.05558C10.0961 5.46026 8.55566 5.15385 7 5.15385Z" fill="#000000"/>
        </svg>
        </a> </li>

      <li> <a href="https://www.linkedin.com/in/philippegaultier/">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" data-supported-dps="24x24" fill="currentColor" class="mercado-match" width="24" height="24" focusable="false">
              <path d="M20.5 2h-17A1.5 1.5 0 002 3.5v17A1.5 1.5 0 003.5 22h17a1.5 1.5 0 001.5-1.5v-17A1.5 1.5 0 0020.5 2zM8 19H5v-9h3zM6.5 8.25A1.75 1.75 0 118.3 6.5a1.78 1.78 0 01-1.8 1.75zM19 19h-3v-4.74c0-1.42-.6-1.93-1.38-1.93A1.74 1.74 0 0013 14.19a.66.66 0 000 .14V19h-3v-9h2.9v1.3a3.11 3.11 0 012.7-1.4c1.55 0 3.36.86 3.36 3.66z"/>
            </svg>
        </a> </li>
      <li> <a href="https://github.com/gaultier">
        <svg height="32" aria-hidden="true" viewBox="0 0 24 24" version="1.1" width="32" data-view-component="true" class="octicon octicon-mark-github v-align-middle">
          <path d="M12.5.75C6.146.75 1 5.896 1 12.25c0 5.089 3.292 9.387 7.863 10.91.575.101.79-.244.79-.546 0-.273-.014-1.178-.014-2.142-2.889.532-3.636-.704-3.866-1.35-.13-.331-.69-1.352-1.18-1.625-.402-.216-.977-.748-.014-.762.906-.014 1.553.834 1.769 1.179 1.035 1.74 2.688 1.25 3.349.948.1-.747.402-1.25.733-1.538-2.559-.287-5.232-1.279-5.232-5.678 0-1.25.445-2.285 1.178-3.09-.115-.288-.517-1.467.115-3.048 0 0 .963-.302 3.163 1.179.92-.259 1.897-.388 2.875-.388.977 0 1.955.13 2.875.388 2.2-1.495 3.162-1.179 3.162-1.179.633 1.581.23 2.76.115 3.048.733.805 1.179 1.825 1.179 3.09 0 4.413-2.688 5.39-5.247 5.678.417.36.776 1.05.776 2.128 0 1.538-.014 2.774-.014 3.162 0 .302.216.662.79.547C20.709 21.637 24 17.324 24 12.25 24 5.896 18.854.75 12.5.75Z"/>
        </svg>
        </a> </li>
      <li> <a href="https://hachyderm.io/@pg">
        <svg width="75" height="79" viewBox="0 0 75 79" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M73.8393 17.4898C72.6973 9.00165 65.2994 2.31235 56.5296 1.01614C55.05 0.797115 49.4441 0 36.4582 0H36.3612C23.3717 0 20.585 0.797115 19.1054 1.01614C10.5798 2.27644 2.79399 8.28712 0.904997 16.8758C-0.00358524 21.1056 -0.100549 25.7949 0.0682394 30.0965C0.308852 36.2651 0.355538 42.423 0.91577 48.5665C1.30307 52.6474 1.97872 56.6957 2.93763 60.6812C4.73325 68.042 12.0019 74.1676 19.1233 76.6666C26.7478 79.2728 34.9474 79.7055 42.8039 77.9162C43.6682 77.7151 44.5217 77.4817 45.3645 77.216C47.275 76.6092 49.5123 75.9305 51.1571 74.7385C51.1797 74.7217 51.1982 74.7001 51.2112 74.6753C51.2243 74.6504 51.2316 74.6229 51.2325 74.5948V68.6416C51.2321 68.6154 51.2259 68.5896 51.2142 68.5661C51.2025 68.5426 51.1858 68.522 51.1651 68.5058C51.1444 68.4896 51.1204 68.4783 51.0948 68.4726C51.0692 68.4669 51.0426 68.467 51.0171 68.4729C45.9835 69.675 40.8254 70.2777 35.6502 70.2682C26.7439 70.2682 24.3486 66.042 23.6626 64.2826C23.1113 62.762 22.7612 61.1759 22.6212 59.5646C22.6197 59.5375 22.6247 59.5105 22.6357 59.4857C22.6466 59.4609 22.6633 59.4391 22.6843 59.422C22.7053 59.4048 22.73 59.3929 22.7565 59.3871C22.783 59.3813 22.8104 59.3818 22.8367 59.3886C27.7864 60.5826 32.8604 61.1853 37.9522 61.1839C39.1768 61.1839 40.3978 61.1839 41.6224 61.1516C46.7435 61.008 52.1411 60.7459 57.1796 59.7621C57.3053 59.7369 57.431 59.7154 57.5387 59.6831C65.4861 58.157 73.0493 53.3672 73.8178 41.2381C73.8465 40.7606 73.9184 36.2364 73.9184 35.7409C73.9219 34.0569 74.4606 23.7949 73.8393 17.4898Z" fill="url(#paint0_linear_549_34)"/>
        <path d="M61.2484 27.0263V48.114H52.8916V27.6475C52.8916 23.3388 51.096 21.1413 47.4437 21.1413C43.4287 21.1413 41.4177 23.7409 41.4177 28.8755V40.0782H33.1111V28.8755C33.1111 23.7409 31.0965 21.1413 27.0815 21.1413C23.4507 21.1413 21.6371 23.3388 21.6371 27.6475V48.114H13.2839V27.0263C13.2839 22.7176 14.384 19.2946 16.5843 16.7572C18.8539 14.2258 21.8311 12.926 25.5264 12.926C29.8036 12.926 33.0357 14.5705 35.1905 17.8559L37.2698 21.346L39.3527 17.8559C41.5074 14.5705 44.7395 12.926 49.0095 12.926C52.7013 12.926 55.6784 14.2258 57.9553 16.7572C60.1531 19.2922 61.2508 22.7152 61.2484 27.0263Z" fill="white"/>
        <defs>
        <linearGradient id="paint0_linear_549_34" x1="37.0692" y1="0" x2="37.0692" y2="79" gradientUnits="userSpaceOnUse">
        <stop stop-color="#6364FF"/>
        <stop offset="1" stop-color="#563ACC"/>
        </linearGradient>
        </defs>
        </svg>
        </a> </li>
      <li> <a href="https://bsky.app/profile/pgaultier.bsky.social">
        <svg fill="none" viewBox="0 0 64 57" width="32" style="width: 32px; height: 28.5px;"><path fill="#0085ff" d="M13.873 3.805C21.21 9.332 29.103 20.537 32 26.55v15.882c0-.338-.13.044-.41.867-1.512 4.456-7.418 21.847-20.923 7.944-7.111-7.32-3.819-14.64 9.125-16.85-7.405 1.264-15.73-.825-18.014-9.015C1.12 23.022 0 8.51 0 6.55 0-3.268 8.579-.182 13.873 3.805ZM50.127 3.805C42.79 9.332 34.897 20.537 32 26.55v15.882c0-.338.13.044.41.867 1.512 4.456 7.418 21.847 20.923 7.944 7.111-7.32 3.819-14.64-9.125-16.85 7.405 1.264 15.73-.825 18.014-9.015C62.88 23.022 64 8.51 64 6.55c0-9.818-8.578-6.732-13.873-2.745Z"/></svg>
        </a> </li>
    </ul>
</div>
<div id="search-matches" hidden>
</div>
<div id="pseudo-body">

<div class="article-prelude">
  <p><a href="/blog"> ‚è¥ Back to all articles</a></p>

  <p class="publication-date">Published on 2025-02-18</p>
</div>
<div class="article-title">
<h1>Making my debug build run 100x faster so that it is finally usable</h1>
  <div class="tags"> <a href="/blog/articles-by-tag.html#c" class="tag">C</a> <a href="/blog/articles-by-tag.html#simd" class="tag">SIMD</a> <a href="/blog/articles-by-tag.html#sha1" class="tag">SHA1</a> <a href="/blog/articles-by-tag.html#torrent" class="tag">Torrent</a> <a href="/blog/articles-by-tag.html#optimization" class="tag">Optimization</a> <a href="/blog/articles-by-tag.html#x86-64" class="tag">x86_64</a></div>
  </div>
 <strong>Table of contents</strong>
<ul>

  <li>
    <a href="#2864684395-why-is-it-a-problem-at-all-and-how-did-it-come-to-be">Why is it a problem at all and how did it come to be?</a>
  </li>

  <li>
    <a href="#2923246873-non-simd-implementation">Non-SIMD implementation</a>
<ul>

  <li>
    <a href="#1904669649-explanation">Explanation</a>
  </li>

  <li>
    <a href="#1805512826-the-code">The code</a>
  </li>

  <li>
    <a href="#2723950694-results">Results</a>
  </li>

  <li>
    <a href="#458888313-possible-optimizations">Possible optimizations</a>
  </li>
</ul>
  </li>

  <li>
    <a href="#3812694471-simd-sse-implementation">SIMD (SSE) implementation</a>
<ul>

  <li>
    <a href="#1057757599-explanation">Explanation</a>
  </li>

  <li>
    <a href="#121025924-the-code">The code</a>
  </li>

  <li>
    <a href="#226219880-results">Results</a>
  </li>
</ul>
  </li>

  <li>
    <a href="#1053355703-intel-sha-extension-implementation">Intel SHA extension implementation</a>
<ul>

  <li>
    <a href="#281525890-explanations">Explanations</a>
  </li>

  <li>
    <a href="#215954170-the-code">The code</a>
  </li>

  <li>
    <a href="#4270897102-results">Results</a>
  </li>
</ul>
  </li>

  <li>
    <a href="#121160688-openssl-hand-crafted-assembly-implementation">OpenSSL hand crafted assembly implementation</a>
  </li>

  <li>
    <a href="#2977180874-additional-improvements">Additional improvements</a>
  </li>

  <li>
    <a href="#3796851539-conclusion">Conclusion</a>
  </li>
</ul>

<p><em>SIMD and dedicated silicon to the rescue.</em></p>
<p><em>Discussions: <a href="https://old.reddit.com/r/C_Programming/comments/1is8aog/making_my_debug_build_run_100x_faster_so_that_it/">/r/C_Programming</a>, <a href="https://news.ycombinator.com/item?id=43087482">HN</a></em></p>
<p>I am writing a torrent application, to download and serve torrent files, in C, because it's fun. A torrent download is made of thousands of pieces of the same size, typically a few hundred KiB to a few MiB.  At start-up, the program reads the downloaded file from disk piece by piece, computes the <a href="https://en.wikipedia.org/wiki/SHA-1">SHA1</a> hash of the piece, and marks this piece as downloaded if the actual hash is indeed the expected hash. We get from the <code>.torrent</code> file the expected hash of each piece.</p>
<p>When we have not downloaded anything yet, the file is completely empty (but still of the right size - we use <code>ftruncate(2)</code> to size it properly even if empty from the get go), and nearly every piece has the wrong hash. Some pieces will accidentally have the right hash, since they are all zeroes in the file we are downloading - good news then, with this approach we do not even have to download them at all!. If we continue an interrupted download (for example the computer restarted), some pieces will have the right hash, and some not. When the download is complete, all pieces will have the correct hash. That way, we know what what pieces we need to download, if any.</p>
<p>I read that some torrent clients prefer to skip this verification at startup because they persist their state in a separate file (perhaps a Sqlite database), each time a new piece is downloaded (and its hash is verified). However I favor doing a from scratch verification at startup for a few reasons, over the 'state file' approach:</p>
<ul>
<li>We might have crashed in the middle of a previous download, before updating the state file, and the persisted state is out-of-sync with the download</li>
<li>There may have been data corruption at the disk level (not everybody runs ZFS and can detect that!)</li>
<li>We can continue a partial download started with a different torrent client - no need for format interoperability. The downloaded file is the source of truth.</li>
<li>Some other program might have corrupted/modified the download, unbeknownst to us and our state file</li>
</ul>
<p>For this reason I do not have a state file at all. It's simpler and a whole class of out-of-sync issues disappears.</p>
<p>So I have this big <a href="https://netbsd.org/mirrors/torrents/">NetBSD image</a> torrent that I primarily test with (by the way, thank you NetBSD maintainers for that!). It's not that big:</p>
<pre><code class="language-sh">$ du -h ./NetBSD-9.4-amd64.iso 
485M	./NetBSD-9.4-amd64.iso
</code></pre>
<p>But when I build my code in debug mode (no optimizations) with Address Sanitizer, to detect various issues early, startup takes <strong>20 to 30 seconds</strong> (hashing at roughly ~ <strong>18 KiB/s</strong>)! That's unbearable, especially when working in the debugger and inspecting some code that runs after the startup. We'd like to finish this verification under 1 second ideally. And making it fast is important, because until it finishes, we do not know which pieces we need to download so that blocks everything.</p>
<p>Let's see how we can speed it up.</p>
<h2 id="2864684395-why-is-it-a-problem-at-all-and-how-did-it-come-to-be">
  <a class="title" href="#2864684395-why-is-it-a-problem-at-all-and-how-did-it-come-to-be">Why is it a problem at all and how did it come to be?</a>
  <a class="hash-anchor" href="#2864684395-why-is-it-a-problem-at-all-and-how-did-it-come-to-be" aria-hidden="true" onclick="navigator.clipboard.writeText(this.href);"></a>
</h2>
<p>It's important to note that in my case, to reduce third-party dependencies, the SHA1 code is vendored in the source tree and comes from OpenBSD. It is plain C code, not using SIMD or such. It's good because I can read it and understand it.</p>
<p>I entertained depending on OpenSSL or such, but it feels wasteful to pull in such a huge amount of code just for SHA1. And building OpenSSL ourselves, to tweak the build flags, means depending on Perl (and Go, in the case of aws-lc), and a lot of stuff. And now I need to pick between OpenSSL, LibreSSL, BoringSSL, aws-lc, etc. And upgrade it when the weekly security vulnerability gets announced. I don't want any of it, if I can help it. Also I want to understand from top to bottom what code I depend on.</p>
<p>For a while, due to this slowness, I simply gave up using a debug build, instead I use minimal optimizations (<code>-O1</code>) with Address Sanitizer. It was much faster, but lots of functions and variables got optimized away, and the debugging experience was thus sub par. I needed to make my debug + Address Sanitizer build viable.  The debug build without Address Sanitizer is much faster: the startup 'only' takes around 2 seconds. But Address Sanitizer is very valuable, I want to be able to use it! And 2 seconds is still too long. Reducing the iteration cycle is often the deciding factor for software quality in my experience.</p>
<p>What's vexing is that from first principles, we know it could/should be much, much faster:</p>
<pre><code class="language-sh">$ hyperfine --shell=none --warmup 3 'sha1sum ./NetBSD-9.4-amd64.iso'
Benchmark 1: sha1sum ./NetBSD-9.4-amd64.iso
  Time (mean ¬± œÉ):     297.7 ms ¬±   3.2 ms    [User: 235.8 ms, System: 60.9 ms]
  Range (min ‚Ä¶ max):   293.7 ms ‚Ä¶ 304.2 ms    10 runs
</code></pre>
<p>Granted, computing the hash for the whole file should be slightly faster than computing the hash for N pieces, because the final step for SHA1 is about padding the data to make it 64 bytes aligned and extracting the digest value from the state computed so far with some bit operations. But still, it's a marginal difference.</p>
<p>Why is it so slow then? I can see on CPU profiles that the SHA1 function takes all of the startup time:</p>
<p><object alt="CPU Profile of the SIMD-less code, debug + Address Sanitizer build" data="cpu_profile_sha1_sw_debug_asan.svg" type="image/svg+xml"></object></p>
<p>The SHA1 code is simplistic, it does not use any SIMD or intrinsics directly. And that's fine, because when it's compiled with optimizations on, the compiler does a pretty good job at optimizing, and it's really fast, around ~300 ms. But the issue is that this code is working one byte at a time. And Address Sanitizer, with its nice runtime and bounds checks, makes each memory access <strong>very</strong> expensive. So we basically have just written a worst-case stress-test for Address Sanitizer.</p>
<p>Let's first review the simple SIMD-less C version to understand the baseline.</p>
<h2 id="2923246873-non-simd-implementation">
  <a class="title" href="#2923246873-non-simd-implementation">Non-SIMD implementation</a>
  <a class="hash-anchor" href="#2923246873-non-simd-implementation" aria-hidden="true" onclick="navigator.clipboard.writeText(this.href);"></a>
</h2>
<p>To isolate the issue, I have created a simple benchmark program. It reads the <code>.torrent</code> file, and the download file, in my case the <code>.iso</code> NetBSD image. Every piece gets hashed and this gets compared with the expected value (a SHA1 hash, or digest, is 20 bytes long). To simplify this example, I skip the decoding of the <code>.torrent</code> file, and hard-code the piece length, as well as where exactly in the file are the expected hashes. The only difficulty is that the last piece might be shorter than the others so we need to compute its exact length to avoid going out of bounds:</p>
<pre><code class="language-c">#include &lt;fcntl.h&gt;
#include &lt;inttypes.h&gt;
#include &quot;sha1_sw.c&quot;
#include &lt;stdbool.h&gt;
#include &lt;string.h&gt;
#include &lt;sys/mman.h&gt;
#include &lt;sys/stat.h&gt;
#include &lt;unistd.h&gt;

static bool is_piece_valid(uint8_t *piece, uint64_t piece_len,
                           uint8_t digest_expected[20]) {
  SHA_CTX ctx = {0};
  SHA1_Init(&amp;ctx);

  SHA1_Update(&amp;ctx, piece, piece_len);

  uint8_t digest_actual[20] = {0};
  SHA1_Final(digest_actual, &amp;ctx);

  return !memcmp(digest_actual, digest_expected, 20);
}

int main(int argc, char *argv[]) {
  if (3 != argc) {
    return 1;
  }

  int file_download = open(argv[1], O_RDONLY, 0600);
  if (!file_download) {
    return 1;
  }

  struct stat st_download = {0};
  if (-1 == fstat(file_download, &amp;st_download)) {
    return 1;
  }
  size_t file_download_size = (size_t)st_download.st_size;

  uint8_t *file_download_data = mmap(NULL, file_download_size, PROT_READ,
                                     MAP_FILE | MAP_PRIVATE, file_download, 0);
  if ((void*)-1 == file_download_data) {
    return 1;
  }

  int file_torrent = open(argv[2], O_RDONLY, 0600);
  if (!file_torrent) {
    return 1;
  }

  struct stat st_torrent = {0};
  if (-1 == fstat(file_torrent, &amp;st_torrent)) {
    return 1;
  }
  size_t file_torrent_size = (size_t)st_torrent.st_size;

  uint8_t *file_torrent_data = mmap(NULL, file_torrent_size, PROT_READ,
                                    MAP_FILE | MAP_PRIVATE, file_torrent, 0);
  if ((void*)-1 == file_torrent_data) {
    return 1;
  }
  // HACK
  // The piece hashes begin at offset 237 in the file.
  uint64_t file_torrent_data_offset = 237;
  file_torrent_data += file_torrent_data_offset;
  // The last character in the file must be ignored because it's the bencode dictionary closing character 'e'.
  file_torrent_size -= file_torrent_data_offset - 1;

  uint64_t piece_length = 262144;
  uint64_t pieces_count = file_download_size / piece_length +
                          ((0 == file_download_size % piece_length) ? 0 : 1);
  for (uint64_t i = 0; i &lt; pieces_count; i++) {
    uint8_t *data = file_download_data + i * piece_length;
    uint64_t piece_length_real = ((i + 1) == pieces_count)
                                     ? (file_download_size - i * piece_length)
                                     : piece_length;
    uint8_t *digest_expected = file_torrent_data + i * 20;

    if (!is_piece_valid(data, piece_length_real, digest_expected)) {
      return 1;
    }
  }
}
</code></pre>
<h3 id="1904669649-explanation">
  <a class="title" href="#1904669649-explanation">Explanation</a>
  <a class="hash-anchor" href="#1904669649-explanation" aria-hidden="true" onclick="navigator.clipboard.writeText(this.href);"></a>
</h3>
<ul>
<li>
<p>In Intel words, what is SHA1?</p>
<blockquote>
<p>SHA-1 produces a 160 bit (20 byte) hash value (digest), taking as an input a sequence of 1 or more 512 bit (64 byte) data blocks. The original source data also requires some padding according to the standard. The data is treated as an array of big-endian 32-bit values. Processing each of the 64-byte input blocks consists of 80 iterations also known as rounds.</p>
</blockquote>
<p>In this implementation:</p>
<ul>
<li>State initialization is done with <code>SHA1_Init</code>: the state is an array of 5 <code>uint32_t</code>, set to magic values defined by the standard.</li>
<li>Processing is done with <code>SHA1_Update</code>: processing works on chunks of 64 bytes. The result of the processing of a chunk is that the state is updated to new values. Incoming data is buffered into the current chunk until it reaches 64 bytes, and that's when the real computation kicks in with <code>SHA1_Transform</code>. This API allows for reading and hashing data in a streaming fashion by repeatedly calling <code>SHA1_Update</code>.</li>
<li>Padding and finalization is done with <code>SHA1_Final</code>: the last chunk is padded to 64 bytes if it is too short, processed, and the digest (the final 20 bytes we are after) is the current state, after endianness conversion.</li>
</ul>
</li>
<li>
<p>The SHA1 algorithm and some implementations support architectures where 1 byte is <em>not</em> 8 bits. But knowing that 1 byte <em>is indeed</em> 8 bits on our architecture unlocks a ton of performance as we'll see.</p>
</li>
<li>
<p>SHA1 expects data in big-endian but nearly all CPU nowadays are little-endian so we need to swap the bytes when loading the input data to do SHA1 computations, and back when storing the intermediate results (the SHA1 state). It is done here with lots of clever bit tricks, one <code>uint32_t</code> at a time.</p>
</li>
<li>
<p>The main loop operating on the 64 bytes chunk is unrolled, which avoids having conditionals in the middle of the loop, which might tank performance due to mispredicted branches. The algorithm lends itself to that really well:</p>
<pre><code class="language-text">  for i from 0 to 79
          if 0 ‚â§ i ‚â§ 19 then
            [..]
          else if 20 ‚â§ i ‚â§ 39
            [..]
          else if 40 ‚â§ i ‚â§ 59
            [..]
          else if 60 ‚â§ i ‚â§ 79
            [..]
</code></pre>
<p>So it's trivial to unroll each section. We'll see that every implementation does the unrolling.</p>
</li>
</ul>
<h3 id="1805512826-the-code">
  <a class="title" href="#1805512826-the-code">The code</a>
  <a class="hash-anchor" href="#1805512826-the-code" aria-hidden="true" onclick="navigator.clipboard.writeText(this.href);"></a>
</h3>
<details>
  <summary>Non-SIMD SHA1</summary>
<pre><code class="language-c">// sha1_sw.c

/*	$OpenBSD: sha1.c,v 1.27 2019/06/07 22:56:36 dtucker Exp $	*/

/*
 * SHA-1 in C
 * By Steve Reid &lt;steve@edmweb.com&gt;
 * 100% Public Domain
 *
 * Test Vectors (from FIPS PUB 180-1)
 * &quot;abc&quot;
 *   A9993E36 4706816A BA3E2571 7850C26C 9CD0D89D
 * &quot;abcdbcdecdefdefgefghfghighijhijkijkljklmklmnlmnomnopnopq&quot;
 *   84983E44 1C3BD26E BAAE4AA1 F95129E5 E54670F1
 * A million repetitions of &quot;a&quot;
 *   34AA973C D4C4DAA4 F61EEB2B DBAD2731 6534016F
 */

#include &lt;inttypes.h&gt;
#include &lt;string.h&gt;

#define SHA1_BLOCK_LENGTH 64
#define SHA1_DIGEST_LENGTH 20
#define SHA1_DIGEST_STRING_LENGTH (SHA1_DIGEST_LENGTH * 2 + 1)

typedef struct {
  uint32_t state[5];
  uint64_t count;
  uint8_t buffer[SHA1_BLOCK_LENGTH];
} SHA1_CTX;
#define rol(value, bits) (((value) &lt;&lt; (bits)) | ((value) &gt;&gt; (32 - (bits))))

/*
 * blk0() and blk() perform the initial expand.
 * I got the idea of expanding during the round function from SSLeay
 */
#define blk0(i)                                                                \
  (block-&gt;l[i] = (rol(block-&gt;l[i], 24) &amp; 0xFF00FF00) |                         \
                 (rol(block-&gt;l[i], 8) &amp; 0x00FF00FF))
#define blk(i)                                                                 \
  (block-&gt;l[i &amp; 15] = rol(block-&gt;l[(i + 13) &amp; 15] ^ block-&gt;l[(i + 8) &amp; 15] ^   \
                              block-&gt;l[(i + 2) &amp; 15] ^ block-&gt;l[i &amp; 15],       \
                          1))

/*
 * (R0+R1), R2, R3, R4 are the different operations (rounds) used in SHA1
 */
#define R0(v, w, x, y, z, i)                                                   \
  z += ((w &amp; (x ^ y)) ^ y) + blk0(i) + 0x5A827999 + rol(v, 5);                 \
  w = rol(w, 30);
#define R1(v, w, x, y, z, i)                                                   \
  z += ((w &amp; (x ^ y)) ^ y) + blk(i) + 0x5A827999 + rol(v, 5);                  \
  w = rol(w, 30);
#define R2(v, w, x, y, z, i)                                                   \
  z += (w ^ x ^ y) + blk(i) + 0x6ED9EBA1 + rol(v, 5);                          \
  w = rol(w, 30);
#define R3(v, w, x, y, z, i)                                                   \
  z += (((w | x) &amp; y) | (w &amp; x)) + blk(i) + 0x8F1BBCDC + rol(v, 5);            \
  w = rol(w, 30);
#define R4(v, w, x, y, z, i)                                                   \
  z += (w ^ x ^ y) + blk(i) + 0xCA62C1D6 + rol(v, 5);                          \
  w = rol(w, 30);

typedef union {
  uint8_t c[64];
  uint32_t l[16];
} CHAR64LONG16;

/*
 * Hash a single 512-bit block. This is the core of the algorithm.
 */
void SHA1Transform(uint32_t state[5], const uint8_t buffer[SHA1_BLOCK_LENGTH]) {
  uint32_t a, b, c, d, e;
  uint8_t workspace[SHA1_BLOCK_LENGTH];
  CHAR64LONG16 *block = (CHAR64LONG16 *)workspace;

  (void)memcpy(block, buffer, SHA1_BLOCK_LENGTH);

  /* Copy context-&gt;state[] to working vars */
  a = state[0];
  b = state[1];
  c = state[2];
  d = state[3];
  e = state[4];

  /* 4 rounds of 20 operations each. Loop unrolled. */
  R0(a, b, c, d, e, 0);
  R0(e, a, b, c, d, 1);
  R0(d, e, a, b, c, 2);
  R0(c, d, e, a, b, 3);
  R0(b, c, d, e, a, 4);
  R0(a, b, c, d, e, 5);
  R0(e, a, b, c, d, 6);
  R0(d, e, a, b, c, 7);
  R0(c, d, e, a, b, 8);
  R0(b, c, d, e, a, 9);
  R0(a, b, c, d, e, 10);
  R0(e, a, b, c, d, 11);
  R0(d, e, a, b, c, 12);
  R0(c, d, e, a, b, 13);
  R0(b, c, d, e, a, 14);
  R0(a, b, c, d, e, 15);
  R1(e, a, b, c, d, 16);
  R1(d, e, a, b, c, 17);
  R1(c, d, e, a, b, 18);
  R1(b, c, d, e, a, 19);
  R2(a, b, c, d, e, 20);
  R2(e, a, b, c, d, 21);
  R2(d, e, a, b, c, 22);
  R2(c, d, e, a, b, 23);
  R2(b, c, d, e, a, 24);
  R2(a, b, c, d, e, 25);
  R2(e, a, b, c, d, 26);
  R2(d, e, a, b, c, 27);
  R2(c, d, e, a, b, 28);
  R2(b, c, d, e, a, 29);
  R2(a, b, c, d, e, 30);
  R2(e, a, b, c, d, 31);
  R2(d, e, a, b, c, 32);
  R2(c, d, e, a, b, 33);
  R2(b, c, d, e, a, 34);
  R2(a, b, c, d, e, 35);
  R2(e, a, b, c, d, 36);
  R2(d, e, a, b, c, 37);
  R2(c, d, e, a, b, 38);
  R2(b, c, d, e, a, 39);
  R3(a, b, c, d, e, 40);
  R3(e, a, b, c, d, 41);
  R3(d, e, a, b, c, 42);
  R3(c, d, e, a, b, 43);
  R3(b, c, d, e, a, 44);
  R3(a, b, c, d, e, 45);
  R3(e, a, b, c, d, 46);
  R3(d, e, a, b, c, 47);
  R3(c, d, e, a, b, 48);
  R3(b, c, d, e, a, 49);
  R3(a, b, c, d, e, 50);
  R3(e, a, b, c, d, 51);
  R3(d, e, a, b, c, 52);
  R3(c, d, e, a, b, 53);
  R3(b, c, d, e, a, 54);
  R3(a, b, c, d, e, 55);
  R3(e, a, b, c, d, 56);
  R3(d, e, a, b, c, 57);
  R3(c, d, e, a, b, 58);
  R3(b, c, d, e, a, 59);
  R4(a, b, c, d, e, 60);
  R4(e, a, b, c, d, 61);
  R4(d, e, a, b, c, 62);
  R4(c, d, e, a, b, 63);
  R4(b, c, d, e, a, 64);
  R4(a, b, c, d, e, 65);
  R4(e, a, b, c, d, 66);
  R4(d, e, a, b, c, 67);
  R4(c, d, e, a, b, 68);
  R4(b, c, d, e, a, 69);
  R4(a, b, c, d, e, 70);
  R4(e, a, b, c, d, 71);
  R4(d, e, a, b, c, 72);
  R4(c, d, e, a, b, 73);
  R4(b, c, d, e, a, 74);
  R4(a, b, c, d, e, 75);
  R4(e, a, b, c, d, 76);
  R4(d, e, a, b, c, 77);
  R4(c, d, e, a, b, 78);
  R4(b, c, d, e, a, 79);

  /* Add the working vars back into context.state[] */
  state[0] += a;
  state[1] += b;
  state[2] += c;
  state[3] += d;
  state[4] += e;

  /* Wipe variables */
  a = b = c = d = e = 0;
}

/*
 * SHA1Init - Initialize new context
 */
void SHA1Init(SHA1_CTX *context) {

  /* SHA1 initialization constants */
  context-&gt;count = 0;
  context-&gt;state[0] = 0x67452301;
  context-&gt;state[1] = 0xEFCDAB89;
  context-&gt;state[2] = 0x98BADCFE;
  context-&gt;state[3] = 0x10325476;
  context-&gt;state[4] = 0xC3D2E1F0;
}

/*
 * Run your data through this.
 */
void SHA1Update(SHA1_CTX *context, const uint8_t *data, size_t len) {
  size_t i, j;

  j = (size_t)((context-&gt;count &gt;&gt; 3) &amp; 63);
  context-&gt;count += ((uint64_t)len &lt;&lt; 3);
  if ((j + len) &gt; 63) {
    (void)memcpy(&amp;context-&gt;buffer[j], data, (i = 64 - j));
    SHA1Transform(context-&gt;state, context-&gt;buffer);
    for (; i + 63 &lt; len; i += 64)
      SHA1Transform(context-&gt;state, (uint8_t *)&amp;data[i]);
    j = 0;
  } else {
    i = 0;
  }
  (void)memcpy(&amp;context-&gt;buffer[j], &amp;data[i], len - i);
}

/*
 * Add padding and return the message digest.
 */
void SHA1Pad(SHA1_CTX *context) {
  uint8_t finalcount[8];
  size_t i;

  for (i = 0; i &lt; 8; i++) {
    finalcount[i] = (uint8_t)((context-&gt;count &gt;&gt; ((7 - (i &amp; 7)) * 8)) &amp;
                              255); /* Endian independent */
  }
  SHA1Update(context, (uint8_t *)&quot;\200&quot;, 1);
  while ((context-&gt;count &amp; 504) != 448)
    SHA1Update(context, (uint8_t *)&quot;\0&quot;, 1);
  SHA1Update(context, finalcount, 8); /* Should cause a SHA1Transform() */
}

void SHA1Final(uint8_t digest[SHA1_DIGEST_LENGTH], SHA1_CTX *context) {
  size_t i;

  SHA1Pad(context);
  for (i = 0; i &lt; SHA1_DIGEST_LENGTH; i++) {
    digest[i] =
        (uint8_t)((context-&gt;state[i &gt;&gt; 2] &gt;&gt; ((3 - (i &amp; 3)) * 8)) &amp; 255);
  }
  explicit_bzero(context, sizeof(*context));
}
</code></pre>
</details>
<p>The <code>SHA1_xxx</code> functions are lifted from <a href="https://github.com/openssh/openssh-portable/blob/9e5bd74a85192c00a842f63d7ab788713b4284c3/openbsd-compat/sha1.c">OpenBSD</a> (there are similar variants, e.g. from <a href="https://sqlite.org/src/file/ext/misc/sha1.c">Sqlite</a>, etc - they are all nearly identical)</p>
<h3 id="2723950694-results">
  <a class="title" href="#2723950694-results">Results</a>
  <a class="hash-anchor" href="#2723950694-results" aria-hidden="true" onclick="navigator.clipboard.writeText(this.href);"></a>
</h3>
<p>When compiled in non-optimized mode with Address Sanitizer, we get this timing:</p>
<pre><code class="language-sh">$ hyperfine --warmup 3 './a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent'
Benchmark 1: ./a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent
  Time (mean ¬± œÉ):     26.312 s ¬±  0.734 s    [User: 26.164 s, System: 0.066 s]
  Range (min ‚Ä¶ max):   25.366 s ‚Ä¶ 27.780 s    10 runs
</code></pre>
<p>This is consistent with our real-life torrent program.</p>
<p>I experimented with doing a <code>read</code> syscall for each piece (that's what <code>sha1sum</code> does) versus using <code>mmap</code>, and there was no difference; additionally the system time is nothing compared to user time, so I/O is not the limiting factor - SHA1 computation is, as confirmed by the CPU profile.</p>
<h3 id="458888313-possible-optimizations">
  <a class="title" href="#458888313-possible-optimizations">Possible optimizations</a>
  <a class="hash-anchor" href="#458888313-possible-optimizations" aria-hidden="true" onclick="navigator.clipboard.writeText(this.href);"></a>
</h3>
<p>So what can we do about it?</p>
<ul>
<li>We can build the SHA1 code separately with optimizations on, always (and potentially without Address Sanitizer). That's a bit annoying, because I currently do a Unity build meaning there is only one compilation unit. So having suddenly multiple compilation units with different build flags makes the build system more complex. And clang has <a href="https://clang.llvm.org/docs/AttributeReference.html#optnone">annotations</a> to <em>lower</em> the optimization level for one function but not to <em>raise</em> it.</li>
<li>We can compute the hash of each piece in parallel for example in a thread pool, since each piece is independent. That works and that's what <a href="https://blog.libtorrent.org/2011/11/multi-threaded-piece-hashing/">libtorrent did/does</a>, but that assumes that the target computer has cores to spare, and it creates some complexity:
<ul>
<li>We need to implement a thread pool (spawning a new thread for each piece will not perform well) and pick a reasonable thread pool size given the number of cores, in a cross-platform way</li>
<li>We need a M:N scheduling logic to compute the hash of M pieces on N threads. It could be a work-stealing queue where each thread picks the next item when its finished with its piece, or we read the whole file in memory and split the data in equal parts for each thread to plow through (but beware that the data for each thread must be aligned with the piece size!)</li>
<li>We would have high contention: in the real program, right after we checked the actual hash against the expected hash, we update a bitfield to remember which piece is valid or not. Every thread would contend on this (probably more so with the work-stealing approach than with the 'split in equal parts' approach where we could avoid locking and contention entirely).</li>
</ul>
</li>
<li>We can implement SHA1 with SIMD. That way, it's much faster regardless of the build level. Essentially, we do not rely on the compiler auto-vectorization that only occurs at higher optimization levels, we do it directly. It has the nice advantage that we have guaranteed performance even when using a different compiler, or an older compiler that cannot do auto-vectorization properly, or if a new compiler version comes along and auto-vectorization broke for this code. Since it uses lots of heuristics, this may happen.</li>
</ul>
<p>So let's do SIMD and learn cool new stuff! The nice thing about it is that we can always in the future <em>also</em> compute hashes in parallel, as well as use SIMD; the two approaches compose well together. I am reminded of an old adage:</p>
<blockquote>
<p>You can't have multiple cores until you've shown you can use one efficiently.</p>
</blockquote>
<h2 id="3812694471-simd-sse-implementation">
  <a class="title" href="#3812694471-simd-sse-implementation">SIMD (SSE) implementation</a>
  <a class="hash-anchor" href="#3812694471-simd-sse-implementation" aria-hidden="true" onclick="navigator.clipboard.writeText(this.href);"></a>
</h2>
<p><a href="http://arctic.org/~dean/crypto/sha1.html">This</a> is an implementation from the early 2000s in the public domain. Yes, SSE, which is the first widespread SIMD instruction set, is from the nineties to early 2000s. More than 25 years ago! There's basically no reason to write SIMD-less code for performance sensitive code for a SIMD-friendly problem - every CPU we care about has SIMD! Well, we have two write separate implementations for x64 and ARM, and there were lots of additions to SSE over the years, that's the downside.</p>
<p>Intel references this implementation on their <a href="https://www.intel.com/content/www/us/en/developer/articles/technical/improving-the-performance-of-the-secure-hash-algorithm-1.html">website</a>. According to Intel, it was fundamental work at the time and influenced them. It's also not the fastest SSE implementation, the very article from Intel is about some performance enhancements they found for this code, but it has the advantage that if you have a processor from 2004 or after, it works, and it's simple.</p>
<h3 id="1057757599-explanation">
  <a class="title" href="#1057757599-explanation">Explanation</a>
  <a class="hash-anchor" href="#1057757599-explanation" aria-hidden="true" onclick="navigator.clipboard.writeText(this.href);"></a>
</h3>
<p>I really am a SIMD beginner but I found a few interesting nuggets of wisdom here:</p>
<ul>
<li>
<p>Just like the SIMD-less implementation, the loops are unrolled</p>
</li>
<li>
<p>Going from little-endian to big-endian (or back) is done with a SIMD shuffle. The way it works is by providing a bit mask that indicates which bits to copy from the source to the destination, and where to place them:</p>
<pre><code class="language-c">  // `0x1b` == `0b0001_1011`.
  // Will result in:
  // [31:0] == [127:96] (due to bits [1:0] being `11`).
  // [63:32] == [95:64] (due to bits [3:2] being `10`).
  // [95:64] == [63:32] (due to bits [5:4] being `01`).
  // [127:96] == [31:0] (due to bits [7:6] being `00`).
  // I.e.: Transform state to big-endian.
  ABCD = _mm_shuffle_epi32(ABCD, 0x1B);
</code></pre>
<p>It's nifty because we can copy the data in and out of SIMD registers, while also doing the endianness conversion, in one operation that typically compiles down to one assembly instruction. And this approach also works from a SIMD register to another SIMD register or inside the same register.</p>
</li>
<li>
<p>Typical SIMD code processes the data in groups of N bytes at a time, and the few excess bytes at the end use the normal SIMD-less code path. Here, we have to deal with an additional grouping: SHA1 processes data in chunks of 64 bytes and the last chunk is padded to be 64 bytes if it is too short. Hence, for the last short chunk we use the SIMD-less code path. We could try to be clever about doing the padding, and re-using the SIMD code path for this last chunk, since 64 bytes is a nice round number that is SIMD friendly, but this last chunk is not going to really make a difference in practice when we are dealing with megabytes or gigabytes of data.</p>
</li>
</ul>
<h3 id="121025924-the-code">
  <a class="title" href="#121025924-the-code">The code</a>
  <a class="hash-anchor" href="#121025924-the-code" aria-hidden="true" onclick="navigator.clipboard.writeText(this.href);"></a>
</h3>
<details>
    <summary>SHA1 with SSE</summary>
<pre><code class="language-c">typedef union {
  uint32_t u32[4];
  __m128i u128;
} v4si __attribute__((aligned(16)));

static const v4si K00_19 = {
    .u32 = {0x5a827999, 0x5a827999, 0x5a827999, 0x5a827999}};
static const v4si K20_39 = {
    .u32 = {0x6ed9eba1, 0x6ed9eba1, 0x6ed9eba1, 0x6ed9eba1}};
static const v4si K40_59 = {
    .u32 = {0x8f1bbcdc, 0x8f1bbcdc, 0x8f1bbcdc, 0x8f1bbcdc}};
static const v4si K60_79 = {
    .u32 = {0xca62c1d6, 0xca62c1d6, 0xca62c1d6, 0xca62c1d6}};

#define UNALIGNED 1
#if UNALIGNED
#define load(p) _mm_loadu_si128(p)
#else
#define load(p) (*p)
#endif

/*
        the first 16 bytes only need byte swapping

        prepared points to 4x uint32_t, 16-byte aligned

        W points to the 4 dwords which need preparing --
        and is overwritten with the swapped bytes
*/
#define prep00_15(prep, W)                                                     \
  do {                                                                         \
    __m128i r1, r2;                                                            \
                                                                               \
    r1 = (W);                                                                  \
    if (1) {                                                                   \
      r1 = _mm_shufflehi_epi16(r1, _MM_SHUFFLE(2, 3, 0, 1));                   \
      r1 = _mm_shufflelo_epi16(r1, _MM_SHUFFLE(2, 3, 0, 1));                   \
      r2 = _mm_slli_epi16(r1, 8);                                              \
      r1 = _mm_srli_epi16(r1, 8);                                              \
      r1 = _mm_or_si128(r1, r2);                                               \
      (W) = r1;                                                                \
    }                                                                          \
    (prep).u128 = _mm_add_epi32(K00_19.u128, r1);                              \
  } while (0)

/*
        for each multiple of 4, t, we want to calculate this:

        W[t+0] = rol(W[t-3] ^ W[t-8] ^ W[t-14] ^ W[t-16], 1);
        W[t+1] = rol(W[t-2] ^ W[t-7] ^ W[t-13] ^ W[t-15], 1);
        W[t+2] = rol(W[t-1] ^ W[t-6] ^ W[t-12] ^ W[t-14], 1);
        W[t+3] = rol(W[t]   ^ W[t-5] ^ W[t-11] ^ W[t-13], 1);

        we'll actually calculate this:

        W[t+0] = rol(W[t-3] ^ W[t-8] ^ W[t-14] ^ W[t-16], 1);
        W[t+1] = rol(W[t-2] ^ W[t-7] ^ W[t-13] ^ W[t-15], 1);
        W[t+2] = rol(W[t-1] ^ W[t-6] ^ W[t-12] ^ W[t-14], 1);
        W[t+3] = rol(  0    ^ W[t-5] ^ W[t-11] ^ W[t-13], 1);
        W[t+3] ^= rol(W[t+0], 1);

        the parameters are:

        W0 = &amp;W[t-16];
        W1 = &amp;W[t-12];
        W2 = &amp;W[t- 8];
        W3 = &amp;W[t- 4];

        and on output:
                prepared = W0 + K
                W0 = W[t]..W[t+3]
*/

/* note that there is a step here where i want to do a rol by 1, which
 * normally would look like this:
 *
 * r1 = psrld r0,$31
 * r0 = pslld r0,$1
 * r0 = por r0,r1
 *
 * but instead i do this:
 *
 * r1 = pcmpltd r0,zero
 * r0 = paddd r0,r0
 * r0 = psub r0,r1
 *
 * because pcmpltd and paddd are availabe in both MMX units on
 * efficeon, pentium-m, and opteron but shifts are available in
 * only one unit.
 */
#define prep(prep, XW0, XW1, XW2, XW3, K)                                      \
  do {                                                                         \
    __m128i r0, r1, r2, r3;                                                    \
                                                                               \
    /* load W[t-4] 16-byte aligned, and shift */                               \
    r3 = _mm_srli_si128((XW3), 4);                                             \
    r0 = (XW0);                                                                \
    /* get high 64-bits of XW0 into low 64-bits */                             \
    r1 = _mm_shuffle_epi32((XW0), _MM_SHUFFLE(1, 0, 3, 2));                    \
    /* load high 64-bits of r1 */                                              \
    r1 = _mm_unpacklo_epi64(r1, (XW1));                                        \
    r2 = (XW2);                                                                \
                                                                               \
    r0 = _mm_xor_si128(r1, r0);                                                \
    r2 = _mm_xor_si128(r3, r2);                                                \
    r0 = _mm_xor_si128(r2, r0);                                                \
    /* unrotated W[t]..W[t+2] in r0 ... still need W[t+3] */                   \
                                                                               \
    r2 = _mm_slli_si128(r0, 12);                                               \
    r1 = _mm_cmplt_epi32(r0, _mm_setzero_si128());                             \
    r0 = _mm_add_epi32(r0, r0); /* shift left by 1 */                          \
    r0 = _mm_sub_epi32(r0, r1); /* r0 has W[t]..W[t+2] */                      \
                                                                               \
    r3 = _mm_srli_epi32(r2, 30);                                               \
    r2 = _mm_slli_epi32(r2, 2);                                                \
                                                                               \
    r0 = _mm_xor_si128(r0, r3);                                                \
    r0 = _mm_xor_si128(r0, r2); /* r0 now has W[t+3] */                        \
                                                                               \
    (XW0) = r0;                                                                \
    (prep).u128 = _mm_add_epi32(r0, (K).u128);                                 \
  } while (0)

static inline uint32_t f00_19(uint32_t x, uint32_t y, uint32_t z) {
  /* FIPS 180-2 says this: (x &amp; y) ^ (~x &amp; z)
   * but we can calculate it in fewer steps.
   */
  return ((y ^ z) &amp; x) ^ z;
}

static inline uint32_t f20_39(uint32_t x, uint32_t y, uint32_t z) {
  return (x ^ z) ^ y;
}

static inline uint32_t f40_59(uint32_t x, uint32_t y, uint32_t z) {
  /* FIPS 180-2 says this: (x &amp; y) ^ (x &amp; z) ^ (y &amp; z)
   * but we can calculate it in fewer steps.
   */
  return (x &amp; z) | ((x | z) &amp; y);
}

static inline uint32_t f60_79(uint32_t x, uint32_t y, uint32_t z) {
  return f20_39(x, y, z);
}

#define step(nn_mm, xa, xb, xc, xd, xe, xt, input)                             \
  do {                                                                         \
    (xt) = (input) + f##nn_mm((xb), (xc), (xd));                               \
    (xb) = rol((xb), 30);                                                      \
    (xt) += ((xe) + rol((xa), 5));                                             \
  } while (0)

[[maybe_unused]]
static void sha1_sse_step(uint32_t *restrict H, const uint32_t *restrict inputu,
                          size_t num_steps) {
  const __m128i *restrict input = (const __m128i *)inputu;
  __m128i W0, W1, W2, W3;
  v4si prep0, prep1, prep2;
  uint32_t a, b, c, d, e, t;

  a = H[0];
  b = H[1];
  c = H[2];
  d = H[3];
  e = H[4];

  /* i've tried arranging the SSE2 code to be 4, 8, 12, and 16
   * steps ahead of the integer code.  12 steps ahead seems
   * to produce the best performance. -dean
   */
  W0 = load(&amp;input[0]);
  prep00_15(prep0, W0); /* prepare for 00 through 03 */
  W1 = load(&amp;input[1]);
  prep00_15(prep1, W1); /* prepare for 04 through 07 */
  W2 = load(&amp;input[2]);
  prep00_15(prep2, W2); /* prepare for 08 through 11 */
  for (;;) {
    W3 = load(&amp;input[3]);
    step(00_19, a, b, c, d, e, t, prep0.u32[0]); /* 00 */
    step(00_19, t, a, b, c, d, e, prep0.u32[1]); /* 01 */
    step(00_19, e, t, a, b, c, d, prep0.u32[2]); /* 02 */
    step(00_19, d, e, t, a, b, c, prep0.u32[3]); /* 03 */
    prep00_15(prep0, W3);
    step(00_19, c, d, e, t, a, b, prep1.u32[0]); /* 04 */
    step(00_19, b, c, d, e, t, a, prep1.u32[1]); /* 05 */
    step(00_19, a, b, c, d, e, t, prep1.u32[2]); /* 06 */
    step(00_19, t, a, b, c, d, e, prep1.u32[3]); /* 07 */
    prep(prep1, W0, W1, W2, W3, K00_19);         /* prepare for 16 through 19 */
    step(00_19, e, t, a, b, c, d, prep2.u32[0]); /* 08 */
    step(00_19, d, e, t, a, b, c, prep2.u32[1]); /* 09 */
    step(00_19, c, d, e, t, a, b, prep2.u32[2]); /* 10 */
    step(00_19, b, c, d, e, t, a, prep2.u32[3]); /* 11 */
    prep(prep2, W1, W2, W3, W0, K20_39);         /* prepare for 20 through 23 */
    step(00_19, a, b, c, d, e, t, prep0.u32[0]); /* 12 */
    step(00_19, t, a, b, c, d, e, prep0.u32[1]); /* 13 */
    step(00_19, e, t, a, b, c, d, prep0.u32[2]); /* 14 */
    step(00_19, d, e, t, a, b, c, prep0.u32[3]); /* 15 */
    prep(prep0, W2, W3, W0, W1, K20_39);
    step(00_19, c, d, e, t, a, b, prep1.u32[0]); /* 16 */
    step(00_19, b, c, d, e, t, a, prep1.u32[1]); /* 17 */
    step(00_19, a, b, c, d, e, t, prep1.u32[2]); /* 18 */
    step(00_19, t, a, b, c, d, e, prep1.u32[3]); /* 19 */

    prep(prep1, W3, W0, W1, W2, K20_39);
    step(20_39, e, t, a, b, c, d, prep2.u32[0]); /* 20 */
    step(20_39, d, e, t, a, b, c, prep2.u32[1]); /* 21 */
    step(20_39, c, d, e, t, a, b, prep2.u32[2]); /* 22 */
    step(20_39, b, c, d, e, t, a, prep2.u32[3]); /* 23 */
    prep(prep2, W0, W1, W2, W3, K20_39);
    step(20_39, a, b, c, d, e, t, prep0.u32[0]); /* 24 */
    step(20_39, t, a, b, c, d, e, prep0.u32[1]); /* 25 */
    step(20_39, e, t, a, b, c, d, prep0.u32[2]); /* 26 */
    step(20_39, d, e, t, a, b, c, prep0.u32[3]); /* 27 */
    prep(prep0, W1, W2, W3, W0, K20_39);
    step(20_39, c, d, e, t, a, b, prep1.u32[0]); /* 28 */
    step(20_39, b, c, d, e, t, a, prep1.u32[1]); /* 29 */
    step(20_39, a, b, c, d, e, t, prep1.u32[2]); /* 30 */
    step(20_39, t, a, b, c, d, e, prep1.u32[3]); /* 31 */
    prep(prep1, W2, W3, W0, W1, K40_59);
    step(20_39, e, t, a, b, c, d, prep2.u32[0]); /* 32 */
    step(20_39, d, e, t, a, b, c, prep2.u32[1]); /* 33 */
    step(20_39, c, d, e, t, a, b, prep2.u32[2]); /* 34 */
    step(20_39, b, c, d, e, t, a, prep2.u32[3]); /* 35 */
    prep(prep2, W3, W0, W1, W2, K40_59);
    step(20_39, a, b, c, d, e, t, prep0.u32[0]); /* 36 */
    step(20_39, t, a, b, c, d, e, prep0.u32[1]); /* 37 */
    step(20_39, e, t, a, b, c, d, prep0.u32[2]); /* 38 */
    step(20_39, d, e, t, a, b, c, prep0.u32[3]); /* 39 */

    prep(prep0, W0, W1, W2, W3, K40_59);
    step(40_59, c, d, e, t, a, b, prep1.u32[0]); /* 40 */
    step(40_59, b, c, d, e, t, a, prep1.u32[1]); /* 41 */
    step(40_59, a, b, c, d, e, t, prep1.u32[2]); /* 42 */
    step(40_59, t, a, b, c, d, e, prep1.u32[3]); /* 43 */
    prep(prep1, W1, W2, W3, W0, K40_59);
    step(40_59, e, t, a, b, c, d, prep2.u32[0]); /* 44 */
    step(40_59, d, e, t, a, b, c, prep2.u32[1]); /* 45 */
    step(40_59, c, d, e, t, a, b, prep2.u32[2]); /* 46 */
    step(40_59, b, c, d, e, t, a, prep2.u32[3]); /* 47 */
    prep(prep2, W2, W3, W0, W1, K40_59);
    step(40_59, a, b, c, d, e, t, prep0.u32[0]); /* 48 */
    step(40_59, t, a, b, c, d, e, prep0.u32[1]); /* 49 */
    step(40_59, e, t, a, b, c, d, prep0.u32[2]); /* 50 */
    step(40_59, d, e, t, a, b, c, prep0.u32[3]); /* 51 */
    prep(prep0, W3, W0, W1, W2, K60_79);
    step(40_59, c, d, e, t, a, b, prep1.u32[0]); /* 52 */
    step(40_59, b, c, d, e, t, a, prep1.u32[1]); /* 53 */
    step(40_59, a, b, c, d, e, t, prep1.u32[2]); /* 54 */
    step(40_59, t, a, b, c, d, e, prep1.u32[3]); /* 55 */
    prep(prep1, W0, W1, W2, W3, K60_79);
    step(40_59, e, t, a, b, c, d, prep2.u32[0]); /* 56 */
    step(40_59, d, e, t, a, b, c, prep2.u32[1]); /* 57 */
    step(40_59, c, d, e, t, a, b, prep2.u32[2]); /* 58 */
    step(40_59, b, c, d, e, t, a, prep2.u32[3]); /* 59 */

    prep(prep2, W1, W2, W3, W0, K60_79);
    step(60_79, a, b, c, d, e, t, prep0.u32[0]); /* 60 */
    step(60_79, t, a, b, c, d, e, prep0.u32[1]); /* 61 */
    step(60_79, e, t, a, b, c, d, prep0.u32[2]); /* 62 */
    step(60_79, d, e, t, a, b, c, prep0.u32[3]); /* 63 */
    prep(prep0, W2, W3, W0, W1, K60_79);
    step(60_79, c, d, e, t, a, b, prep1.u32[0]); /* 64 */
    step(60_79, b, c, d, e, t, a, prep1.u32[1]); /* 65 */
    step(60_79, a, b, c, d, e, t, prep1.u32[2]); /* 66 */
    step(60_79, t, a, b, c, d, e, prep1.u32[3]); /* 67 */
    prep(prep1, W3, W0, W1, W2, K60_79);
    step(60_79, e, t, a, b, c, d, prep2.u32[0]); /* 68 */
    step(60_79, d, e, t, a, b, c, prep2.u32[1]); /* 69 */
    step(60_79, c, d, e, t, a, b, prep2.u32[2]); /* 70 */
    step(60_79, b, c, d, e, t, a, prep2.u32[3]); /* 71 */

    --num_steps;
    if (num_steps == 0)
      break;

    input += 4;
    W0 = load(&amp;input[0]);
    prep00_15(prep2, W0); /* prepare for next 00 through 03 */
    W1 = load(&amp;input[1]);
    step(60_79, a, b, c, d, e, t, prep0.u32[0]); /* 72 */
    step(60_79, t, a, b, c, d, e, prep0.u32[1]); /* 73 */
    step(60_79, e, t, a, b, c, d, prep0.u32[2]); /* 74 */
    step(60_79, d, e, t, a, b, c, prep0.u32[3]); /* 75 */
    prep0 = prep2;        /* top of loop expects this in prep0 */
    prep00_15(prep2, W1); /* prepare for next 04 through 07 */
    W2 = load(&amp;input[2]);
    step(60_79, c, d, e, t, a, b, prep1.u32[0]); /* 76 */
    step(60_79, b, c, d, e, t, a, prep1.u32[1]); /* 77 */
    step(60_79, a, b, c, d, e, t, prep1.u32[2]); /* 78 */
    step(60_79, t, a, b, c, d, e, prep1.u32[3]); /* 79 */
    prep1 = prep2;        /* top of loop expects this in prep1 */
    prep00_15(prep2, W2); /* prepare for next 08 through 11 */
    /* e, t, a, b, c, d */
    H[0] += e;
    H[1] += t;
    H[2] += a;
    H[3] += b;
    H[4] += c;

    a = H[0];
    b = H[1];
    c = H[2];
    d = H[3];
    e = H[4];
  }
  /* no more input to prepare */
  step(60_79, a, b, c, d, e, t, prep0.u32[0]); /* 72 */
  step(60_79, t, a, b, c, d, e, prep0.u32[1]); /* 73 */
  step(60_79, e, t, a, b, c, d, prep0.u32[2]); /* 74 */
  step(60_79, d, e, t, a, b, c, prep0.u32[3]); /* 75 */
  /* no more input to prepare */
  step(60_79, c, d, e, t, a, b, prep1.u32[0]); /* 76 */
  step(60_79, b, c, d, e, t, a, prep1.u32[1]); /* 77 */
  step(60_79, a, b, c, d, e, t, prep1.u32[2]); /* 78 */
  step(60_79, t, a, b, c, d, e, prep1.u32[3]); /* 79 */
  /* e, t, a, b, c, d */
  H[0] += e;
  H[1] += t;
  H[2] += a;
  H[3] += b;
  H[4] += c;
}

</code></pre>
</details>
<p>Our <code>is_piece_valid</code> function now becomes:</p>
<pre><code class="language-c">static bool is_piece_valid(uint8_t *piece, uint64_t piece_len,
                           uint8_t digest_expected[20]) {
  SHA1_CTX ctx = {0};
  SHA1Init(&amp;ctx);

  // Process as many SHA1 64 bytes chunks as possible.
  uint64_t len_rounded_down = (piece_len / 64) * 64;
  uint64_t rem = piece_len % 64;
  uint64_t steps = len_rounded_down / 64;
  sha1_sse_step(ctx.state, piece, steps);

  // Process the excess.
  memcpy(ctx.buffer, piece + len_rounded_down, rem);

  // `count` is in bits: multiple the number of bytes by 8.
  ctx.count = piece_len * 8;

  uint8_t digest_actual[20] = {0};
  SHA1Final(digest_actual, &amp;ctx);

  return !memcmp(digest_actual, digest_expected, 20);
}
</code></pre>
<h3 id="226219880-results">
  <a class="title" href="#226219880-results">Results</a>
  <a class="hash-anchor" href="#226219880-results" aria-hidden="true" onclick="navigator.clipboard.writeText(this.href);"></a>
</h3>
<p>So predictably, since we now process 4 <code>uint32_t</code> at a time instead of one, we observe roughly a 4x speed-up (still in debug + Address Sanitizer mode):</p>
<pre><code class="language-sh">$ hyperfine --warmup 3 './a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent'
Benchmark 1: ./a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent
  Time (mean ¬± œÉ):      8.093 s ¬±  0.272 s    [User: 8.010 s, System: 0.060 s]
  Range (min ‚Ä¶ max):    7.784 s ‚Ä¶  8.684 s    10 runs
</code></pre>
<p>That's better but still not great. We could apply the tweaks suggested by Intel, but that probably would not give us the order of magnitude improvement we need. They cite x1.2 to x1.5 improvements in their article. We need more.</p>
<p>So... did you know that in all likelihood, your CPU has dedicated silicon to accelerate SHA1 computations? Let's use that! We paid for it, we get to use it!</p>
<h2 id="1053355703-intel-sha-extension-implementation">
  <a class="title" href="#1053355703-intel-sha-extension-implementation">Intel SHA extension implementation</a>
  <a class="hash-anchor" href="#1053355703-intel-sha-extension-implementation" aria-hidden="true" onclick="navigator.clipboard.writeText(this.href);"></a>
</h2>
<p>Despite the 'Intel' name, Intel as well as AMD CPUs have been shipping with this <a href="https://en.wikipedia.org/wiki/Intel_SHA_extensions">extension</a>, since around 2016-2017. It adds a few SIMD instructions dedicated to compute SHA1 (and SHA256, and other variants). Note that ARM also has an equivalent (albeit incompatible, of course) extension so the same can be done there.</p>
<p><em>There is an irony here, because 2017 is also the year where the first SHA1 public collision was published, which incited many developers to move away from SHA1...</em></p>
<p>The advantage is that the structure of the code can remain the same: we still are using 128 bits SIMD registers, still computing SHA1 chunks of 64 bytes at a time. It's just that a few operations get faster and the code is generally shorter and clearer, and the main part is branchless.</p>
<p>The implementation is a pure work of art, and comes from this <a href="https://github.com/noloader/SHA-Intrinsics/blob/master/sha1-x86.c">Github repository</a>. I have commented lots of it for clarity.</p>
<h3 id="281525890-explanations">
  <a class="title" href="#281525890-explanations">Explanations</a>
  <a class="hash-anchor" href="#281525890-explanations" aria-hidden="true" onclick="navigator.clipboard.writeText(this.href);"></a>
</h3>
<ul>
<li>
<p>The unit of work here is still 128 bits (or 4 <code>uint32_t</code>). Unfortunately, the SHA1 state that we are continuously updating, and from which the final digest is extracted, is <strong>5</strong> <code>uint32_t</code>. So we are in a pickle since it does not fit neatly in one SIMD register.  Thus, we have to do one SIMD operation on the first 4 <code>uint32_t</code>, named <code>ABCD</code>, and another one with the last <code>uint32_t</code>, named <code>E</code>. So this second operation is a bit wasteful: our 128 bits only contain 1/4 of useful data, and our CPU does computations on a bunch of zeroes which will be thrown away. But there is no other way: SIMD uses a different set of registers from the standard ones. We want to stay in SIMD land as much as possible, that's where the performance is.</p>
</li>
<li>
<p>Endianness conversion is done with one SIMD instruction, same as before (so 4 <code>uint32_t</code> at a time).</p>
</li>
<li>
<p>The SHA Intel extension provides 4 operations:</p>
<ul>
<li><code>sha1rnds4</code> to compute the next <code>ABCD</code> state</li>
<li><code>sha1nexte</code>: to compute the next <code>E</code> state (remember, <code>E</code> is alone in its 128 bits register)</li>
<li><code>sha1msg1</code> and <code>sha1msg2</code>: they perform the SHA1 computations solely based on the input data</li>
</ul>
<p>Thus we alternate between SHA1 computations with <code>sha1msg1/sha1msg2</code>, and state calculations with <code>sha1rnds4/sha1nexte</code>, always 4 <code>uint32_t</code> at a time.</p>
</li>
<li>
<p>What's a &quot;SHA computation&quot;? It's basically a recombination, or shuffling, of its input. For example, <code>sha1msg1</code> in pseudo-code does:</p>
<pre><code class="language-text">  W0 &lt;- SRC1[127:96] ;
  W1 &lt;- SRC1[95:64] ;
  W2 &lt;- SRC1[63: 32] ;
  W3 &lt;- SRC1[31: 0] ;
  W4 &lt;- SRC2[127:96] ;
  W5 &lt;- SRC2[95:64] ;
  DEST[127:96] &lt;- W2 XOR W0;
  DEST[95:64] &lt;- W3 XOR W1;
  DEST[63:32] &lt;- W4 XOR W2;
  DEST[31:0] &lt;- W5 XOR W3;
</code></pre>
<p>The first 16 rounds, we do that on the input data (i.e. the download file). But for the remaining rounds (SHA1 does 80 rounds for a 64 byte chunk), the input is computations from previous rounds.
<code>sha1msg2</code> does slightly different computations but still very similar.</p>
</li>
</ul>
<h3 id="215954170-the-code">
  <a class="title" href="#215954170-the-code">The code</a>
  <a class="hash-anchor" href="#215954170-the-code" aria-hidden="true" onclick="navigator.clipboard.writeText(this.href);"></a>
</h3>
<details>
    <summary>SHA1 with the Intel SHA extension</summary>
<pre><code class="language-c">// Process as many 64 bytes chunks as possible.
[[maybe_unused]]
static void sha1_sha_ext(uint32_t state[5], const uint8_t data[],
                         uint32_t length) {
  __m128i ABCD, ABCD_SAVE, E0, E0_SAVE, E1;
  __m128i MSG0, MSG1, MSG2, MSG3;
  const __m128i MASK =
      // As 16 u8: `0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15`.
      _mm_set_epi64x(0x0001020304050607ULL, 0x08090a0b0c0d0e0fULL);

  /* Load initial values */
  ABCD = _mm_loadu_si128((const __m128i *)(void *)state);
  E0 = _mm_set_epi32((int)state[4], 0, 0, 0);

  // Transform state to big-endian.
  ABCD = _mm_shuffle_epi32(ABCD, 0x1B);

  while (length &gt;= 64) {
    /* Save current state  */
    ABCD_SAVE = ABCD;
    E0_SAVE = E0;

    /* Rounds 0-3 */
    // Load `data[0:16]`.
    MSG0 = _mm_loadu_si128((const __m128i *)(void *)(data + 0));

    // Convert MSG0 to big-endian.
    MSG0 = _mm_shuffle_epi8(MSG0, MASK);
    // E0 += MSG0
    E0 = _mm_add_epi32(E0, MSG0);
    E1 = ABCD;
    //  Perform 4 rounds of SHA1 operation.
    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E0, 0);

    /* Rounds 4-7 */
    // Load `data[16:32]`.
    MSG1 = _mm_loadu_si128((const __m128i *)(void *)(data + 16));
    // Convert to big-endian.
    MSG1 = _mm_shuffle_epi8(MSG1, MASK);
    // Compute the SHA1 state variable E after 4 rounds.
    // It is added to the source operand (`E1`).
    E1 = _mm_sha1nexte_epu32(E1, MSG1);
    E0 = ABCD;

    //  Perform 4 rounds of SHA1 operation.
    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E1, 0);
    // Perform the intermediate calculation for the next four SHA1 message dwords (128 bits).
    MSG0 = _mm_sha1msg1_epu32(MSG0, MSG1);

    /* Rounds 8-11 */
    // Load `data[32:48]`.
    MSG2 = _mm_loadu_si128((const __m128i *)(void *)(data + 32));
    // Convert to big-endian.
    MSG2 = _mm_shuffle_epi8(MSG2, MASK);
    // Compute the SHA1 state variable E after 4 rounds.
    E0 = _mm_sha1nexte_epu32(E0, MSG2);
    E1 = ABCD;
    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E0, 0);
    MSG1 = _mm_sha1msg1_epu32(MSG1, MSG2);
    MSG0 = _mm_xor_si128(MSG0, MSG2);

    /* Rounds 12-15 */
    // Load `data[48:64]`.
    MSG3 = _mm_loadu_si128((const __m128i *)(void *)(data + 48));
    // Convert to big-endian.
    MSG3 = _mm_shuffle_epi8(MSG3, MASK);
    // Compute the SHA1 state variable E after 4 rounds.
    E1 = _mm_sha1nexte_epu32(E1, MSG3);
    E0 = ABCD;
    // Perform a final calculation for the next four SHA1 message dwords.
    MSG0 = _mm_sha1msg2_epu32(MSG0, MSG3);
    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E1, 0);
    MSG2 = _mm_sha1msg1_epu32(MSG2, MSG3);
    MSG1 = _mm_xor_si128(MSG1, MSG3);

    /* Rounds 16-19 */
    E0 = _mm_sha1nexte_epu32(E0, MSG0);
    E1 = ABCD;
    MSG1 = _mm_sha1msg2_epu32(MSG1, MSG0);
    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E0, 0);
    MSG3 = _mm_sha1msg1_epu32(MSG3, MSG0);
    MSG2 = _mm_xor_si128(MSG2, MSG0);

    /* Rounds 20-23 */
    E1 = _mm_sha1nexte_epu32(E1, MSG1);
    E0 = ABCD;
    MSG2 = _mm_sha1msg2_epu32(MSG2, MSG1);
    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E1, 1);
    MSG0 = _mm_sha1msg1_epu32(MSG0, MSG1);
    MSG3 = _mm_xor_si128(MSG3, MSG1);

    /* Rounds 24-27 */
    E0 = _mm_sha1nexte_epu32(E0, MSG2);
    E1 = ABCD;
    MSG3 = _mm_sha1msg2_epu32(MSG3, MSG2);
    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E0, 1);
    MSG1 = _mm_sha1msg1_epu32(MSG1, MSG2);
    MSG0 = _mm_xor_si128(MSG0, MSG2);

    /* Rounds 28-31 */
    E1 = _mm_sha1nexte_epu32(E1, MSG3);
    E0 = ABCD;
    MSG0 = _mm_sha1msg2_epu32(MSG0, MSG3);
    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E1, 1);
    MSG2 = _mm_sha1msg1_epu32(MSG2, MSG3);
    MSG1 = _mm_xor_si128(MSG1, MSG3);

    /* Rounds 32-35 */
    E0 = _mm_sha1nexte_epu32(E0, MSG0);
    E1 = ABCD;
    MSG1 = _mm_sha1msg2_epu32(MSG1, MSG0);
    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E0, 1);
    MSG3 = _mm_sha1msg1_epu32(MSG3, MSG0);
    MSG2 = _mm_xor_si128(MSG2, MSG0);

    /* Rounds 36-39 */
    E1 = _mm_sha1nexte_epu32(E1, MSG1);
    E0 = ABCD;
    MSG2 = _mm_sha1msg2_epu32(MSG2, MSG1);
    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E1, 1);
    MSG0 = _mm_sha1msg1_epu32(MSG0, MSG1);
    MSG3 = _mm_xor_si128(MSG3, MSG1);

    /* Rounds 40-43 */
    E0 = _mm_sha1nexte_epu32(E0, MSG2);
    E1 = ABCD;
    MSG3 = _mm_sha1msg2_epu32(MSG3, MSG2);
    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E0, 2);
    MSG1 = _mm_sha1msg1_epu32(MSG1, MSG2);
    MSG0 = _mm_xor_si128(MSG0, MSG2);

    /* Rounds 44-47 */
    E1 = _mm_sha1nexte_epu32(E1, MSG3);
    E0 = ABCD;
    MSG0 = _mm_sha1msg2_epu32(MSG0, MSG3);
    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E1, 2);
    MSG2 = _mm_sha1msg1_epu32(MSG2, MSG3);
    MSG1 = _mm_xor_si128(MSG1, MSG3);

    /* Rounds 48-51 */
    E0 = _mm_sha1nexte_epu32(E0, MSG0);
    E1 = ABCD;
    MSG1 = _mm_sha1msg2_epu32(MSG1, MSG0);
    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E0, 2);
    MSG3 = _mm_sha1msg1_epu32(MSG3, MSG0);
    MSG2 = _mm_xor_si128(MSG2, MSG0);

    /* Rounds 52-55 */
    E1 = _mm_sha1nexte_epu32(E1, MSG1);
    E0 = ABCD;
    MSG2 = _mm_sha1msg2_epu32(MSG2, MSG1);
    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E1, 2);
    MSG0 = _mm_sha1msg1_epu32(MSG0, MSG1);
    MSG3 = _mm_xor_si128(MSG3, MSG1);

    /* Rounds 56-59 */
    E0 = _mm_sha1nexte_epu32(E0, MSG2);
    E1 = ABCD;
    MSG3 = _mm_sha1msg2_epu32(MSG3, MSG2);
    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E0, 2);
    MSG1 = _mm_sha1msg1_epu32(MSG1, MSG2);
    MSG0 = _mm_xor_si128(MSG0, MSG2);

    /* Rounds 60-63 */
    E1 = _mm_sha1nexte_epu32(E1, MSG3);
    E0 = ABCD;
    MSG0 = _mm_sha1msg2_epu32(MSG0, MSG3);
    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E1, 3);
    MSG2 = _mm_sha1msg1_epu32(MSG2, MSG3);
    MSG1 = _mm_xor_si128(MSG1, MSG3);

    /* Rounds 64-67 */
    E0 = _mm_sha1nexte_epu32(E0, MSG0);
    E1 = ABCD;
    MSG1 = _mm_sha1msg2_epu32(MSG1, MSG0);
    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E0, 3);
    MSG3 = _mm_sha1msg1_epu32(MSG3, MSG0);
    MSG2 = _mm_xor_si128(MSG2, MSG0);

    /* Rounds 68-71 */
    E1 = _mm_sha1nexte_epu32(E1, MSG1);
    E0 = ABCD;
    MSG2 = _mm_sha1msg2_epu32(MSG2, MSG1);
    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E1, 3);
    MSG3 = _mm_xor_si128(MSG3, MSG1);

    /* Rounds 72-75 */
    E0 = _mm_sha1nexte_epu32(E0, MSG2);
    E1 = ABCD;
    MSG3 = _mm_sha1msg2_epu32(MSG3, MSG2);
    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E0, 3);

    /* Rounds 76-79 */
    E1 = _mm_sha1nexte_epu32(E1, MSG3);
    E0 = ABCD;
    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E1, 3);

    /* Combine state */
    E0 = _mm_sha1nexte_epu32(E0, E0_SAVE);
    // ABCD += ABCD_SAVE
    ABCD = _mm_add_epi32(ABCD, ABCD_SAVE);

    data += 64;
    length -= 64;
  }

  /* Save state */
  // Convert back to little-endian.
  ABCD = _mm_shuffle_epi32(ABCD, 0x1B);
  _mm_storeu_si128((__m128i *)(void *)state, ABCD);
  // Convert back to little-endian.
  state[4] = (uint32_t)_mm_extract_epi32(E0, 3);
}

</code></pre>
</details>
<p>Our <code>is_piece_valid</code> function is practically identical to the last section:</p>
<pre><code class="language-c">static bool is_piece_valid(uint8_t *piece, uint64_t piece_len,
                           uint8_t digest_expected[20]) {
  SHA1_CTX ctx = {0};
  SHA1Init(&amp;ctx);

  // Process as many SHA1 64 bytes chunks as possible.
  uint64_t len_rounded_down = (piece_len / 64) * 64;
  uint64_t rem = piece_len % 64;
  sha1_sha_ext(ctx.state, piece, (uint32_t)len_rounded_down);

  memcpy(ctx.buffer, piece + len_rounded_down, rem);

  ctx.count = piece_len * 8;

  uint8_t digest_actual[20] = {0};
  SHA1Final(digest_actual, &amp;ctx);

  return !memcmp(digest_actual, digest_expected, 20);
}
</code></pre>
<h3 id="4270897102-results">
  <a class="title" href="#4270897102-results">Results</a>
  <a class="hash-anchor" href="#4270897102-results" aria-hidden="true" onclick="navigator.clipboard.writeText(this.href);"></a>
</h3>
<p>How fast?</p>
<pre><code class="language-sh"> $ hyperfine --warmup 3 './a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent'
Benchmark 1: ./a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent
  Time (mean ¬± œÉ):     866.9 ms ¬±  17.4 ms    [User: 809.6 ms, System: 54.4 ms]
  Range (min ‚Ä¶ max):   839.7 ms ‚Ä¶ 901.4 ms    10 runs
</code></pre>
<p>Now that's what I'm talking about, ~ <strong>571 MiB/s</strong>. Around a 10x speed-up compared to the basic SSE implementation! And now we are running under a second. Also the variability is much reduced which is nice.</p>
<p>What about a release build (without Address Sanitizer), for comparison?</p>
<p>This is the SIMD-less version with <code>-O2 -march=native</code>, benefiting from some auto-vectorization:</p>
<pre><code class="language-sh">$ hyperfine --warmup 3 './a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent'
Benchmark 1: ./a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent
  Time (mean ¬± œÉ):     617.8 ms ¬±  20.9 ms    [User: 573.6 ms, System: 42.2 ms]
  Range (min ‚Ä¶ max):   598.7 ms ‚Ä¶ 669.1 ms    10 runs
</code></pre>
<p>That's ~ <strong>802 Mib/s</strong>.</p>
<p>And this is the code using the SHA extension, again with <code>-O2 -march=native</code>:</p>
<pre><code class="language-sh">$ hyperfine --warmup 3 './a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent'
Benchmark 1: ./a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent
  Time (mean ¬± œÉ):     281.2 ms ¬±   5.4 ms    [User: 240.6 ms, System: 39.6 ms]
  Range (min ‚Ä¶ max):   276.1 ms ‚Ä¶ 294.3 ms    10 runs
</code></pre>
<p>That's ~ <strong>1.8 Gib/s</strong>.</p>
<p>Unsurprisingly, when inspecting the generated assembly code for the SIMD-less version, the auto-vectorization is <em>very</em> limited and does not use the SHA extension (compilers are smart, but not <em>that</em> smart).</p>
<p>As such, it's still very impressive that it reaches such a high performance. My guess is that the compiler does a good job at analyzing data dependencies and reordering statements to maximize utilization. Also, SHA1 does a lot of bit rotation, and the compiler makes heavy use of the <code>ror</code>, <code>rol</code>, and <code>shr</code> instructions to do just that instead of doing multiple naive bit operations like in the unoptimized code.</p>
<p>The version using the SHA extension performs very well, be it in debug + Address Sanitizer mode, or release mode.</p>
<p>Also, in both versions, as the SHA1 code got much faster, we start to see on the CPU profile <code>mmap</code> show up, as confirmed by the <code>system time</code> part becoming a fifth of the whole runtime.</p>
<p>That means that we are starting to be limited by I/O. Which is good!  Using <code>hdparm</code> to measure my disk performance, I get:</p>
<pre><code class="language-text"> Timing buffered disk reads: 6518 MB in  3.00 seconds = 2171.34 MB/sec
</code></pre>
<p>Since our benchmark first warms up a few times, we know that the file data is in the cache, so <code>buffered disk reads</code> seems like a good metric to go by. Thus, our program performance is near the disk I/O limit for cached reads. Sounds pretty good to me!</p>
<p>I tried to give the OS some hints to improve a bit on that front with <code>madvise(file_download_data, file_download_size, MADV_SEQUENTIAL | MADV_WILLNEED)</code>, but it did not have any impact on the timings.</p>
<h2 id="121160688-openssl-hand-crafted-assembly-implementation">
  <a class="title" href="#121160688-openssl-hand-crafted-assembly-implementation">OpenSSL hand crafted assembly implementation</a>
  <a class="hash-anchor" href="#121160688-openssl-hand-crafted-assembly-implementation" aria-hidden="true" onclick="navigator.clipboard.writeText(this.href);"></a>
</h2>
<p>The whole point of this article is to do SHA1 computations from scratch and avoid dependencies. Let's see how OpenSSL (in this case, <a href="https://github.com/aws/aws-lc">aws-lc</a> but I don't believe they changed that part at all) fares out of curiosity.</p>
<pre><code class="language-sh"> $ hyperfine --warmup 3 './a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent'
Benchmark 1: ./a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent
  Time (mean ¬± œÉ):     281.5 ms ¬±   3.9 ms    [User: 245.7 ms, System: 35.1 ms]
  Range (min ‚Ä¶ max):   276.3 ms ‚Ä¶ 288.9 ms    10 runs
</code></pre>
<p>So, the performance is essentially identical to our version. Pretty good.</p>
<p>OpenSSL picks at runtime the best code path based on what features the CPU supports. Interestingly on my system, even when compiled with <code>-march=native</code>, it does not decide to use the SHA extension, and instead goes for hand-optimized SIMD. That's mind-blowing that this SIMD code performs as well that dedicated silicon, including the cycles spent on the runtime check. So props to the developers!</p>
<p>I can see really low-level tricks like <code>prefetcht0</code> to ask for the prefetcher to cache some data ahead of time to reduce latency. And they mention they had help from some folks at Intel.</p>
<h2 id="2977180874-additional-improvements">
  <a class="title" href="#2977180874-additional-improvements">Additional improvements</a>
  <a class="hash-anchor" href="#2977180874-additional-improvements" aria-hidden="true" onclick="navigator.clipboard.writeText(this.href);"></a>
</h2>
<p>I have not talked about AVX2, AVX512, etc. These could be fun to implement and benchmark. If you are interested in this kind of thing, the OpenSSL project (and the various clones and forks) has a <a href="https://github.com/aws/aws-lc/blob/7518c784f4d6a8933345d9192b82ecc19bea4403/crypto/fipsmodule/sha/asm/sha1-x86_64.pl">Perl script</a> to generate assembly code to do SHA1 with various variants of SIMD and SHA extension. I think the numbers are pretty dated but it's a goldmine of information.</p>
<p>Oh, and I almost forgot: we can compute SHA1 on the <a href="https://github.com/cr-marcstevens/sha1_gpu_nearcollisionattacks">GPU</a>!</p>
<h2 id="3796851539-conclusion">
  <a class="title" href="#3796851539-conclusion">Conclusion</a>
  <a class="hash-anchor" href="#3796851539-conclusion" aria-hidden="true" onclick="navigator.clipboard.writeText(this.href);"></a>
</h2>
<p>That was a fun deep dive about performance, SIMD, and a deprecated hash algorithm that is still in use in many applications (e.g. Git).</p>
<p>What I have learned is that Address Sanitizer really likes SIMD code because it reduces significantly the runtime checks it has to do, and thus the performance impact is greatly reduced. It is often recommended to do fuzzing with Address Sanitizer on, so performance matters here.</p>
<p>SIMD code is like a math puzzle, it's weird and fun. I'm happy that I finally had my first real contact with it. It has the useful property to have a very stable performance across runs. I hope I did not get anything wrong in this article.</p>
<p>And it's wild to see different implementations range from 30s to 300 ms (a factor of 100!) to do the same thing. Also, optimizers these days are god damn impressive.</p>
<p>If you want to learn more about SIMD, I recommend this <a href="https://www.youtube.com/watch?v=6BIfqfC1i7U">talk</a> from the Titanfall developers at GDC where they explain how they use a lot of SIMD in their game engine, but also their thought process to go from a standard procedural code to a SIMD version:</p>
<img style="height:30rem" src="simd_talk.png" alt="GDC talk">
<p><a href="/blog"> ‚è¥ Back to all articles</a></p>

<blockquote id="donate">
  <p>If you enjoy what you're reading, you want to support me, and can afford it: <a href="https://paypal.me/philigaultier?country.x=DE&locale.x=en_US">Support me</a>. That allows me to write more cool articles!</p>
</blockquote>

<blockquote>
  <p>
    This blog is <a href="https://github.com/gaultier/blog">open-source</a>!
    If you find a problem, please open a Github issue.
    The content of this blog as well as the code snippets are under the <a href="https://en.wikipedia.org/wiki/BSD_licenses#3-clause_license_(%22BSD_License_2.0%22,_%22Revised_BSD_License%22,_%22New_BSD_License%22,_or_%22Modified_BSD_License%22)">BSD-3 License</a> which I also usually use for all my personal projects. It's basically free for every use but you have to mention me as the original author.
  </p>
</blockquote>

</div>
</body>
</html>
