const raw_index={documents:[{
html_file_name:"advent_of_code_2018_5.html",
title:"Getting started with Scheme by solving an Advent of Code 2018 challenge",
text:"Discussions: /r/scheme I started learning Scheme very recently. Chicken Scheme is a wonderful small and\nperformant implementation of Scheme, a programming language in the family of\nLISPs.\nSince I learn by doing, let\'s solve the Advent of Code 2018 day 5 challenge with a tiny Scheme program.\nI encourage you to check out Advent of\nCode and try to solve the challenges yourself. Many people have the feeling that LISPs are slow and cryptic with all those\nparentheses. I hope to show that it is in fact very approachable, easy to work\nwith, and even fast to run! I will not go through installing Chicken Scheme and learning the basics, because\nit was already done better than I can . The problem We have a string looking like this: AabcdZZqQ which represents a chain of\nchemical units. Adjacent units of the same type (i.e letter) and opposite\npolarity (i.e casing) react together and disappear.\nIt means we want to remove adjacent characters which are the same letter and have opposite casing, e.g Aa and qQ disappear while bc and ZZ remain. Once we are finished, we have: bcdZZ . The final output is the number of characters in the final string, i.e, 5 . Working with the REPL to iteratively close in on a solution First, let\'s define our input, which is a string: (define input &quot;aAbxXBctTCz&quot;) Later, we will read our input string from a file, but for now it is simpler to\njust hard-code it. Most functions in Scheme are immutable, meaning they do not\nmodify their arguments, they instead return a new item which is slightly different. We could work with strings, but it turns out it is simpler to work with lists\ninstead in our case. We do not want to keep track of indices, risking doing off-by-one mistakes.\nAlso, LISPs are good at handling lists (LISP stands for LISt Processor), and\nwe\'ll that we can use pattern matching to make the code very concise. I am not\naware of pattern matching capabilities on string, so let\'s use lists: (string-&gt;list input) Here, the string-&gt;list function just returns a list of characters for a string (in other\nlanguages it is usually named split ). Now, we need to detect if two characters are the same latter, with opposite casing.\nLet\'s write a char-opposite-casing? function to do just that. It will take 2\narguments, the letters we are inspecting, and will return a boolean.\nFor now, let\'s just make it always return true: (define (char-opposite-casing? a b) #\\t) We only deal with ASCII, so it is safe to compare ASCII codes to detect casing. What is the ASCII code of A ? Let\'s try it by using the function char-&gt;integer : (char-&gt;integer #\\A)  What about a ? (char-&gt;integer #\\a) So there is a difference of 32 between the same ASCII letter in lowercase and\nuppercase. Peeking at man ascii in the terminal confirms this hunch for all\nletters of the alphabet. So, time to implement char-opposite-casing? : (define (char-case-opposite-casing? a b)\n  (let* ((a-code (char-&gt;integer a))\n         (b-code (char-&gt;integer b))\n         (diff (- a-code b-code)))\n    (= (* 32 32) (* diff diff)))) Let\'s try it with a and A : (char-case-opposite-casing? #\\a #\\A)  And flipped: (char-case-opposite-casing? #\\A #\\a) And A and b : (char-case-opposite-casing? #\\A #\\b) let* is used to define local bindings which are only visible in this function.\nIt evaluates each binding in order which means we can define diff in terms of a and b (contrary to let ). We could have done without it but it makes the function more readable. The only hurdle is not caring\nabout the sign of the difference: if the difference is 32 or -32 , it is the\nsame. We could compare the absolute value, but I (arbitrarily) chose to implement it without\nbranches, by comparing the squared values (which swallows the signs). Now let\'s work on the central problem: how to remove\ncharacters in a list, in a functional, immutable way? The idea is to write a recursive function taking two arguments: an accumulator\n(let\'s call it acc from now on),\nwhich will be eventually the end result, and the input list ( input ), from which we\ngradually remove items until it is empty. We can view the first list as the work\nwe have done, and the second list as the work to do. Let\'s first define the function. For now, it just returns the empty list: (define (chem-react acc input)\n  \'()) At first, the accumulator is the empty list, so we will always call our function like\nthis: (chem-react \'() (string-&gt;list input)) It is import to know that most list functions do not work on the empty list in\nChicken Scheme. For example, to get the first element of a list, we use the car function: (define my-list (list 1 2 3))\n\n;; Note that this doest **not** mutate `my-list`\n(car my-list) But it won\'t work on the empty list: (define my-list \'())\n\n(car my-list) So we need to treat the case of the empty list (both for the first and the\nsecond argument) explicitly. We could do that by using lots of if , but it is\nmore readable and concise to use pattern matching. A small detour: pattern matching Scheme has a minimalist core, so we do not get pattern matching out of\nthe box, but we can easily add it with the package matchable . Let\'s install\nit in the terminal: $ chicken-install matchable Now we can import it at the top of our code: (import matchable)\n\n;; At this point we can refer to any function in this module `matchable`.\n;; No need to prefix them either with `matchable`. Let\'s try to match the empty list in our function, and return (as an example) a\nnumber, e.g 42 . We also want to match the case of both lists containing one\nelement, and returning the sum of those 2 elements: (define (chem-react acc input)\n  (match (list acc input)\n    [(_ ()) 42]\n    [((a) (b)) (+ a b)]))\n\n(chem-react \'() \'()) ;; =&gt; 42\n\n(chem-react (list 2) (list 3)) ;; =&gt; 5 A few interesting things here: _ allows us to match anything, so the first\ncase is equivalent to checking if the second list is\nempty. Additionally, we can bind variables to our patterns: we do that in the\nsecond case, binding the first element of the first list to a , and the fist\nelement of the second list to b , and summing the two. Note that not all possible cases are covered here, and we will get a (runtime)\nerror if we trigger one of them, for example with a list containing several numbers: (chem-react (list 1 2) (list 3)) ;; =&gt; Error: (match) &quot;no matching pattern&quot;: () Let\'s go ahead and match the case of a list of one or more elements ( (a . arest) ) to avoid that: (define (chem-react acc input)\n  (match (list acc input)\n    [(_ ()) 42]\n    [((a) (b)) (+ a b)]\n    [((a . arest) (b . brest)) (* a b)]))\n\n(chem-react (list 2 3) (list 4)) ;; =&gt; 8 Here we choose to (arbitrarily) return the product of the first elements of both\nlist, to show that pattern matching is also a way to do destructuring. Using pattern matching to solve our problem If the second list (the input) is empty, it means we are\nfinished, so we return the first list ( acc ): (define (chem-react acc input)\n  (match (list acc input)\n    [(_ ()) acc])) Our recursion will work as follows: we look at the first element of the second\nlist ( input , which is the work to do), let\'s call it b , and the first element of the first\nlist ( acc , the work done), let\'s call it a . If a and b are the same letter of opposite casing, we \'drop\' the two. Otherwise, we\nadd b to the first list, and \'continue\'. \'drop\' and \'continue\' are put in\nquotes because that is vocabulary from imperative languages such as C; we\'ll see\nin a minute how we implement it in a functional way. If the first list is empty, this is our starting case: the only thing we can do\nis mark b as \'processed\', i.e add it to the first list, and call ourselves\nwith the remainder of input . Indeed, we can only work with two characters, so\nif we only have one, we cannot do much. It\'s time to learn about a new function: cons . cons just adds an item to a list, and\nreturns the new list with the added item: (define my-list (list 2 3))\n\n;; Note: `my-list` is **not** modified\n(cons 1 my-list)  We can now use cons to implement the new case: (define (chem-react acc input)\n  (match (list acc input)\n    [(_ ()) acc]\n    [(() (b . brest)) (chem-react (cons b acc) brest)]))\n\n\n(chem-react \'() \'(#\\A)) ;; =&gt; (#\\A) This new pattern is required for the recursion to\nwork, but it also covers the trivial case of an input string of only one character. Now, let\'s treat the main case: we have at least an element a in acc and at\nleast an element b in input . If they are the same letters of opposite casing, we\ncall ourselves with the remainder of acc and the remainder of input , which\nis equivalent to \'drop\' a and b . Otherwise, we add b to acc , and we call\nourselves with the remainder of input , which is the equivalent of \'continuing\': (define (chem-react acc input)\n  (match (list acc input)\n    [(_ ()) acc]\n    [(() (b . brest)) (chem-react (cons b acc) brest)]\n    [((a . arest) (b . brest)) (if (char-case-opposite-casing? a b)\n                                   (chem-react arest brest)\n                                   (chem-react (cons b acc) brest))]))\n\n\n(chem-react \'() (list #\\A #\\a #\\b)) ;; =&gt; (#\\b)\n(chem-react \'() (string-&gt;list &quot;aAbxXBctTCz&quot;)) ;; =&gt; (#\\z) But wait a minute...Doesn\'t it look familiar? Yes, what we are doing here is a\nfold (sometimes called reduce)! Let\'s replace our custom recursion by fold . chem-react becomes the reduction\nfunction. It becomes simpler because fold will not call it on the empty list,\nso we only need to patter match acc (which is the empty list at the beginning): (define (chem-react acc x)\n  (match acc\n    [() (cons x acc)]\n    [(a . arest) (if (char-case-opposite-casing? a x)\n                     arest\n                     (cons x acc))]))\n\n\n(foldl chem-react \'() input) ;; =&gt; (#\\z) My experience writing code in a LISP is that I usually find a solution that is\nrelatively big, and I start replacing parts of it with standard functions such\nas fold and it ends up very small. How do I read the input from a file? It\'s quite simple: we use the modules chicken.file.posix and chicken.io : (import chicken.file.posix\n        chicken.io)\n\n(read-line (open-input-file &quot;/Users/pgaultier/Downloads/aoc5.txt&quot;)) ;; =&gt; &quot;a big string...&quot; The final solution Here I use the package clojurian ( chicken-install clojurian ) to have access\nto the -&gt;&gt; macro which makes code more readable. It works like the pipe in the\nshell. Instead of writing: (foo (bar &quot;foo&quot; (baz 1 2))) We write: (-&gt;&gt; (baz 1 2)\n     (bar &quot;foo&quot;)\n     (foo)) The macro reorders the functions calls to make it flat and avoid nesting.\nIt is not strictly required, but I like that my code looks like a\npipeline of data transformations. The final code: (import matchable\n        clojurian.syntax\n        chicken.file.posix\n        chicken.io)\n\n(define (char-case-opposite-casing? a b)\n  (let* ((a-code (char-&gt;integer a))\n         (b-code (char-&gt;integer b))\n         (diff (- a-code b-code)))\n    (= (* 32 32) (* diff diff))))\n\n(define (chem-react acc x)\n  (match acc\n    [() (cons x acc)]\n    [(a . arest) (if (char-case-opposite-casing? a x)\n                     arest\n                     (cons x acc))]))\n\n(-&gt;&gt; (open-input-file &quot;/Users/pgaultier/Downloads/aoc5.txt&quot;)\n     (read-line)\n     (string-&gt;list)\n     (foldl chem-react \'())\n     (length)\n     (print)) But we will get a stack overflow on a big input! Scheme has a nice requirement for all implementations: they must implement\ntail-call optimization, which is to say that the compiler can transform our function into an\nequivalent for-loop. So we won\'t get a stack overflow, and it will be quite\nefficient in terms of memory and time. But we are making thousands of copies, it will be slow as hell! Let\'s benchmark it on the real input (50 000 characters), with -O3 to enable optimizations: Note 1: The real output of the program is not shown to avoid spoiling the final result Note 2: This is a simplistic way to do benchmarking. A more correct way would\nbe: warming up the file cache, making many runs, averaging the results, etc.\nI did exactly that and it did not change the results in a significant manner. $ csc aoc5.scm -o aoc5 -O3 &amp;&amp; time ./aoc5\n./aoc5  0.01s user 0.00s system 82% cpu 0.021 total It takes 21 milliseconds. Not too bad for a garbage collected, functional,\nimmutable program. Here is a hand-written C version which only does one allocation and removes\nletters in place: #include &lt;errno.h&gt;\n#include &lt;fcntl.h&gt;\n#include &lt;stdint.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;sys/stat.h&gt;\n#include &lt;unistd.h&gt;\n\nint main() {\n  int fd = open(&quot;/home/pg/Downloads/aoc2020_5.txt&quot;, O_RDONLY);\n  if (fd == -1)\n    return errno;\n\n  struct stat st = {0};\n  if (stat(&quot;/home/pg/Downloads/aoc2020_5.txt&quot;, &amp;st) == -1)\n    return errno;\n\n  int64_t input_len = st.st_size;\n  char *const input = calloc(input_len, 1);\n\n  if (read(fd, input, input_len) != input_len)\n    return errno;\n\n  while (input[input_len - 1] == \'\\n\' || input[input_len - 1] == \' \')\n    input_len--;\n\n  int64_t i = 0;\n  while (i &lt; input_len) {\n    if (abs(input[i] - input[i + 1]) == 32) {\n      memmove(input + i, input + i + 2, input_len - i - 2);\n      input_len -= 2;\n      i = i &gt; 0 ? i - 1 : 0;\n    } else\n      i++;\n  }\n\n  printf(&quot;`%zu`\\n&quot;, input_len);\n} Let\'s benchmark it on the same input: $ cc -std=c99 -O3 -Weverything aoc5.c -march=native &amp;&amp; time ./a.out\n./a.out  0.01s user 0.00s system 86% cpu 0.012 total It took 12 milliseconds. So the scheme version is very close, and takes an\nacceptable amount of time. Can\'t we use strings and not lists? Yes, of course. However we need to be careful about how strings are implemented\nand what we we do with those. Most runtimes (e.g the JVM) use immutable strings,\nmeaning we could end up allocating thousands of big strings, and being quite slow. Conclusion That\'s it, we solved the fifth Advent of Code challenge in Scheme. The solution\nis under 30 lines of code, is (hopefully) simple and readable, and has a\nperformance close to C, while having memory safety (I had several segfaults\nwhile doing the C version). But more than that, I think the real value in LISPs is\ninteractive programming, instead of the classical write-compile-execute-repeat,\nwhich is much more time consuming. It is really important to get feedback as\nearly as possible, and LISPs give us that. I hope it gave you a glance at what Scheme can do, and stay tuned for more blog\nposts about programming. I intend to post more solutions to other coding\nchallenges, solved with a variety of programming languages. ",
titles:[
{
title:"The problem",
slug:"the-problem",
offset:682,
},
{
title:"Working with the REPL to iteratively close in on a solution",
slug:"working-with-the-repl-to-iteratively-close-in-on-a-solution",
offset:1155,
},
{
title:"A small detour: pattern matching",
slug:"a-small-detour-pattern-matching",
offset:4976,
},
{
title:"Using pattern matching to solve our problem",
slug:"using-pattern-matching-to-solve-our-problem",
offset:6810,
},
{
title:"The final solution",
slug:"the-final-solution",
offset:10273,
},
{
title:"Conclusion",
slug:"conclusion",
offset:14017,
},
],
},
{
html_file_name:"compile_ziglang_from_source_on_alpine_2020_9.html",
title:"How to compile LLVM, Clang, LLD, and Ziglang from source on Alpine Linux",
text:"This article is now outdated but remains for historical reasons. Ziglang , or Zig for short, is an ambitious programming language addressing important flaws of mainstream languages such as failing to handle memory allocation failures or forgetting to handle an error condition in general. It is also fast moving so for most, the latest (HEAD) version will be needed, and most package managers will not have it, so we will compile it from source. Since the official Zig compiler is (currently) written in C++ and using the LLVM libraries at a specific version, we will need them as well, and once again, some package managers will not have the exact version you want (10.0.0). I find it more reliable to compile LLVM, Clang, LLD, and Zig from source and that is what we will do here. I have found that the official LLVM and Zig instructions differed somewhat, were presenting too many options, and I wanted to have one place to centralize them for my future self. Incidentally, if you are a lost C++ developer trying to compile LLVM from source, without having ever heard of Zig, well you have stumbled on the right page, you can simply skip the final block about Zig. Note that those instructions should work just the same on any Unix system. Feel free to pick the directories you want when cloning the git repositories. # The only Alpine specific bit. build-base mainly installs make and a C++ compiler. Python 3 is required by LLVM for some reason.\n$ apk add build-base cmake git python3\n\n$ git clone https://github.com/llvm/llvm-project.git --branch llvmorg-10.0.0  --depth 1\n$ cd llvm-project/\n$ mkdir build\n$ cd build/\n# The flag LLVM_ENABLE_PROJECTS is crucial, otherwise only llvm will be built, without clang or lld,\n# and we need all three with the exact same version since C++ does not have a stable ABI.\n$ cmake -DCMAKE_BUILD_TYPE=Release -DLLVM_EXPERIMENTAL_TARGETS_TO_BUILD=&quot;AVR&quot; -DLLVM_ENABLE_LIBXML2=OFF -DLLVM_ENABLE_TERMINFO=OFF -DLLVM_ENABLE_PROJECTS=&quot;clang;lld&quot; ../llvm\n\n# nproc is Linux only but you can set the number of threads manually\n$ make -j$(nproc)\n$ sudo make install\n\n$ cd ~\n$ git clone https://github.com/ziglang/zig.git --depth 1\n$ cd zig\n$ mkdir build\n$ cd build\n$ cmake .. -DCMAKE_BUILD_TYPE=Release -DZIG_STATIC=ON\n# nproc is Linux only but you can set the number of threads manually\n$ make -j$(nproc)\n$ sudo make install You will now have a zig executable in the PATH as well as the zig standard library. You can verify you have now the latest version by doing: $ zig version\n0.6.0+749417a ",
titles:[
],
},
{
html_file_name:"speed_up_your_ci.html",
title:"Adventures in CI land, or how to speed up your CI",
text:"Every project has a Continuous Integration (CI) pipeline and every one of them complains its CI is too slow. It is more important than you might think; this can be the root cause of many problems, including lackluster productivity, low morale, high barrier of entry for newcomers, and overall suboptimal quality. But this need not be. I have compiled here a lengthy list of various ways you can simplify your CI and make it faster, based on my experience on open-source projects and my work experience. I sure wish you will find something in here worth your time. And finally, I hope you will realize this endeavour is not unlike optimizing a program: it requires some time and dedication but you will get tremendous results. Also, almost incidentally, it will be more secure and easier to audit. Lastly, remember to measure and profile your changes. If a change has made no improvements, it should be reverted. This article assumes you are running a POSIX system. Windows developers, this is not the article you are looking for. Reduce the size of everything Almost certainly, your CI pipeline has to download \'something\', be it a base docker image, a virtual machine image, some packages, maybe a few company wide scripts. The thing is, you are downloading those every time it runs, 24/7, every day of the year. Even a small size reduction can yield big speed ups. Remember, the network is usually the bottleneck. In no particular order: Only fetch required git objects. That means running git clone my-repo.git --depth 1 --branch shiny-feature , instead of cloning the whole repository every time, along with every branch and that one class file that your coworker accidentally committed once. Axe duplicate tools. curl and wget are equivalent, given the right command line options. Settle on using only one and stick to it. All my pipelines use: curl --sSL --retry 5 . You can customize further, but that\'s the gist of it. Other examples: make and ninja , gcc and clang , etc. Use POSIX tools. They are already present on whatever system you are using. When purely checking that a tool or an API returned \'OK\', simply use grep and awk , no need for ripgrep . Prefer sh over bash for simple scripts, make over rake for builds, etc. It\'s most likely faster, more stable, and more documented, too. Pay attention to the base image you are using. Prefer a small image where you install only what you need. I have seen docker base images over 1 Gb big. You will spend more time downloading it, uncompressing it, and checksumming it, than running your pipeline. Alpine Linux is great. Debian and Ubuntu are fine. When in doubt, inspect the content of the image. Look for stuff that should not be here. E.g.: X11 , man pages, etc. Don\'t install documentation. It\'s obvious but most people do it. While you are at it, don\'t install man , apropos , info , etc. Alpine Linux gets it right by splitting almost all packages between the package itself and its documentation. E.g.: cmake and cmake-doc . On the same vein: don\'t install shell autocompletions. Same idea. Again, on Alpine they are not part of the main package. E.g.: cmake and cmake-bash-completion . Stay away from aggregate packages (or meta-packages)! Those are for convenience only when developing. E.g.: build-base on Alpine is a meta-package gathering make , file , gcc , etc. It will bring lots of things you do not need. Cherry-pick only what you really required and steer clear of those packages. Learn how Docker image layers work: avoid doing RUN rm archive.tar , since it simply creates a new layer without removing the file from the previous layer. Prefer: RUN curl -sSL --retry 5 foo.com/archive.tar &amp;&amp; tar -xf archive.tar &amp;&amp; rm archive.tar which will not add the tar archive to the Docker image. Use multi-stage Docker builds. It is old advice at this point but it bears repeating. When using multi-stage: Only copy files you need from a previous stage instead of globbing wildly, thus defeating the purpose of multi-stages. Tell apart the development and the release variant of a package. For example: on Ubuntu, when using the SDL2 library, it comes in two flavors: libsdl2-dev and libsdl2-2.0 . The former is the development variant which you only need when building code that needs the headers and the libraries of the SDL2, while the latter is only useful with software needing the dynamic libraries at runtime. The development packages are usually bigger in size. You can astutely use multi-stage Docker builds to have first a build stage using the development packages, and then a final stage which only has the non-development packages. In CI, you almost never need both variants installed at the same time. Opt-out of \'recommended\' packages. Aptitude on Debian/Ubuntu is the culprit here: apt-get install foo will install much more than foo . It will also install recommended packages that most of the time are completely unrelated. Always use apt-get install --no-install-recommends foo . Don\'t create unnecessary files: you can use heredoc and shell pipelines to avoid creating intermediary files. Be lazy: Don\'t do things you don\'t need to do Some features you are not using are enabled by default. Be explicit instead of relying on obscure, ever changing defaults. Example: CGO_ENABLED=0 go build ... because it is (at the time of writing) enabled by default. The Gradle build system also has the annoying habit to run stuff behind your back. Use gradle foo -x baz to run foo and not baz . Don\'t run tests from your dependencies. This can happen if you are using git submodules or vendoring dependencies in some way. You generally always want to build them, but not run their tests. Again, gradle is the culprit here. If you are storing your git submodules in a submodules/ directory for example, you can run only your project tests with: gradle test -x submodules:test . Disable the generation of reports files. They frequently come in the form of HTML or XML form, and once again, gradle gets out of his way to clutter your filesystem with those. Of debatable usefulness locally, they are downright wasteful in CI. And it takes some precious time, too! Disable it with: tasks.withType&lt;Test&gt; {\n     useJUnitPlatform()\n     reports.html.isEnabled = false\n     reports.junitXml.isEnabled = false\n } Check alternative repositories for a dependency instead of building it from source. It can happen that a certain dependency you need is not in the main repositories of the package manager of your system. You can however inspect other repositories before falling back to building it yourself. On Alpine, you can simply add the URL of the repository to /etc/apk/repositories . For example, in the main Alpine Docker image, the repository https://&lt;mirror-server&gt;/alpine/edge/testing is not enabled. More information here . Other example: on OpenBSD or FreeBSD, you can opt-in to use the current branch to get the newest and latest changes, and along them the newest dependencies. Don\'t build the static and dynamic variants of the same library (in C or C++). You probably only want one, preferably the static one. Otherwise, you are doing twice the work! Fetch statically built binaries instead of building them from source. Go, and sometimes Rust, are great for this. As long as the OS and the architecture are the same, of course. E.g.: you can simply fetch kubectl which is a Go static binary instead of installing lots of Kubernetes packages, if you simply need to talk to a Kubernetes cluster. Naturally, the same goes for single file, dependency-less script: shell, awk, python, lua, perl, and ruby, assuming the interpreter is the right one. But this case is rarer and you might as well vendor the script at this point. Groom your \'ignore\' files. .gitignore is the mainstream one, but were you aware Docker has the mechanism in the form of a .dockerignore file? My advice: whitelist the files you need, e.g.: **/*\n!**/*.js This can have a huge impact on performance since Docker will copy all the files inside the Docker context directory inside the container (or virtual machine on macOS) and it can be a lot. You don\'t want to copy build artifacts, images, and so on each time which your image does not need. Use an empty Docker context if possible: you sometimes want to build an image which does not need any local files. In that case you can completely bypass copying any files into the image with the command: docker build . -f - &lt; Dockerfile . Don\'t update the package manager cache: you typically need to start your Dockerfile by updating the package manager cache, otherwise it will complain the dependencies you want to install are not found. E.g.: RUN apk update &amp;&amp; apk add curl . But did you know it is not always required? You can simply do: RUN apk --no-cache add curl when you know the package exists and you can bypass the cache. Silence the tools: most command line applications accept the -q flag which reduces their verbosity. Most of their output is likely to be useless, some CI systems will struggle storing big pipeline logs, and you might be bottlenecked on stdout! Also, it will simplify troubleshooting your build if it is not swamped in thousands of unrelated logs. Miscellenaous tricks Use sed to quickly edit big files in place. E.g.: you want to insert a line at the top of a JavaScript file to skip linter warnings. Instead of doing: $ printf \'/* eslint-disable */\\n\\n\' | cat - foo.js &gt; foo_tmp &amp;&amp; mv foo_tmp foo.js which involves reading the whole file, copying it, and renaming it, we can do: $ sed -i \'1s#^#/* eslint-disable */ #\' foo.js which is simpler. Favor static linking and LTO. This will simplify much of your pipeline because you\'ll have to deal with fewer files, ideally one statically built executable. Use only one CI job. That is because the startup time of a job is very high, in the order of minutes. You can achieve task parallelism with other means such as parallel or make -j . Parallelize all the things! Some tools do not run tasks in parallel by default, e.g. make and gradle . Make sure you are always using a CI instance with multiple cores and are passing --parallel to Gradle and -j$(nproc) to make. In rare instances you might have to tweak the exact level of parallelism to your particular task for maximum performance. Also, parallel is great for parallelizing tasks. Avoid network accesses: you should minimize the amount of things you are downloading from external sources in your CI because it is both slow and a source of flakiness. Some tools will unfortunately always try to \'call home\' even if all of your dependencies are present. You should disable this behavior explicitly, e.g. with Gradle: gradle build --offline . In some rare cases, you will be bottlenecked on a slow running script. Consider using a faster interpreter: for shell scripts, there is ash and dash which are said to be much faster than bash . For awk there is gawk and mawk . For Lua there is LuaJIT . Avoid building inside Docker if you can. Building locally, and then copying the artifacts into the image, is always faster. It only works under certain constraints, of course: same OS and architecture, or a portable artifact format such as jar , and not using native dependencies, or your toolchain supports cross-compilation A note on security Always use https Checksum files you fetched from third-parties with shasum . Favor official package repositories, docker images, and third-parties over those of individuals. Never bypass certificate checks (such as curl -k ) I am a DevOps Engineer, what can I do? Most of the above rules can be automated with a script, assuming the definition of a CI pipeline is in a text format (e.g. Gitlab CI). I would suggest starting here, and teaching developers about these simple tips than really make a difference. I would also suggest considering adding strict firewall rules inside CI pipelines, and making sure the setup/teardown of CI runners is very fast. Additionally, I would do everything to avoid a situation where no CI runner is available, preventing developers from working and deploying. Finally, I would recommend leading by example with the pipelines for the tools made by DevOps Engineers in your organization. Closing words I wish you well on your journey towards a fast, reliable and simple CI pipeline. I noticed in my numerous projects with different tech stacks that some are friendlier than others towards CI pipelines than others (I am looking at you, Gradle!). If you have the luxury of choosing your technical stack, do consider how it will play out with your pipeline. I believe this is a much more important factor than discussing whether $LANG has semicolons or not because I am convinced it can completely decide the outcome of your project. ",
titles:[
{
title:"Reduce the size of everything",
slug:"reduce-the-size-of-everything",
offset:1030,
},
{
title:"Be lazy: Don\'t do things you don\'t need to do",
slug:"be-lazy-don-t-do-things-you-don-t-need-to-do",
offset:5094,
},
{
title:"Miscellenaous tricks",
slug:"miscellenaous-tricks",
offset:9216,
},
{
title:"A note on security",
slug:"a-note-on-security",
offset:11302,
},
{
title:"I am a DevOps Engineer, what can I do?",
slug:"i-am-a-devops-engineer-what-can-i-do",
offset:11546,
},
{
title:"Closing words",
slug:"closing-words",
offset:12242,
},
],
},
{
html_file_name:"x11_x64.html",
title:"Learn x86-64 assembly by writing a GUI from scratch",
text:"Discussions: Hacker News , r/programming , Lobsters . Most people think assembly is only to be used to write toy programs for learning purposes, or to write a highly optimized version of a specific function inside a codebase written in a high-level language. Well, what if we wrote a whole program in assembly that opens a GUI window? It will be the hello world of the GUI world, but that still counts. Here is what we are working towards: I wanted to expand my knowledge of assembly and by doing something fun and motivating. It all originated from the observation that so many program binaries today are very big, often over 30 MiB (!), and I asked myself: How small a binary can be for a (very simplistic) GUI? Well, it turns out, very little. Spoiler alert: around 1 KiB! I am by no means an expert in assembly or in X11. I just hope to provide an entertaining, approachable article, something a beginner can understand. Something I wished I had found when I was learning those topics. If you spot an error, please open a Github issue ! Note: Authentication is optional in the X11 protocol, but some X11 servers e.g. XWayland require it. Authentication is skipped here and is handled in a separate article . What do we need? I will be using the nasm assembler which is simple, cross-platform, fast, and has quite a readable syntax. For the GUI, I will be using X11 since I am based on Linux and it has some interesting properties that make it easy to do without external libraries. If you are running Wayland, it should work with XWayland out of the box ( EDIT: After testing it, I can confirm it does work ), and perhaps also on macOS with XQuartz, but I have not tested those (for macOS, remember to tell nasm to use the macho64 format, since macOS does not use the ELF format! Also, the stock linker on macOS does not support -static .). Note that the only difference between *nix operating systems in the context of this program is the system call values. Since I am based on Linux I will be using the Linux system call values, but \'porting\' this program to, say, FreeBSD, would only require to change those values, possibly using the nasm macros: %ifdef linux\n  %define SYSCALL_EXIT 60\n%elifdef freebsd\n  %define SYSCALL_EXIT 1\n%endif %define and its variants are part of the macro system in nasm , which is powerful but we will only use it here to define constants, just like in C: #define FOO 3 . No need for additional tooling to cross-compile, issues with dynamic libraries, libc differences, etc. Just compile on Linux by defining the right variable on the command line, send the binary to your friend on FreeBSD, and it just works(tm). That\'s refreshing. Some readers have rightfully pointed out that Linux is the only mainstream operating system that officially provides a stable userland ABI, other OSes often break their ABI from (major) version to version and recommend all programs to link to a library (e.g. libSystem in the case of macOS). That layer guarantees API stability, and acts as an insulation layer from breaking changes in the ABI. In practice, for common system calls such as the ones we use here, they very rarely break, but doing more exotic things may break in the future. That actually happened to the Go project in the past on macOS! The solution if that happens is to simply recompile the program on the new version of the OS. So let\'s dive in! X11 basics X11 is a server accessible over the network that handles windowing and rendering inside those windows. A client opens a socket, connects to the server, and sends commands in a specific format to open a window, draw shapes, text, etc. The server sends message about errors or events to the client. Most applications will want to use libX11 or libxcb which offer a C API, but we want to do that ourselves. Where the server lives is actually not relevant for a client, it might run on the same machine or in a data center far far away. Of course, in the context of a desktop computer in 2023, it will be running on the same machine, but that\'s a detail. The official documentation is pretty good, so when in doubt we can refer to it. Main in x64 assembly Let\'s start slow with minimal program that simply exits with 0, and build from there. First, we tell nasm we are writing a 64 bit program and that we target x86_64. Then, we need a main function, which we call _start and needs to be visible since this is the entry point of our program (hence the global keyword): ; Comments start with a semicolon!\nBITS 64 ; 64 bits.\nCPU X64 ; Target the x86_64 family of CPUs.\n\nsection .text\nglobal _start\n_start:\n  xor rax, rax ; Set rax to 0. Not actually needed, it\'s just to avoid having an empty body. section .text is telling nasm and the linker, that what follows is code that should be placed in the text section of the executable. We will soon have a section .data for our global variables. Note that those section usually get mapped by the OS to different pages in memory with different permissions (visible with readelf -l ) so that the text section is not writable and the data section is not executable, but that varies from OS to OS. The _start function has a body that does nothing for now, but not for long. The actual name of the main function is actually up to us, it\'s just that start or _start is usual. We build and run our little program like this: $ nasm -f elf64 -g main.nasm &amp;&amp; ld main.o -static -o main nasm actually only produces an object file, so to get an executable out of it, we need to invoke the linker ld . The flag -g is telling nasm to produce debugging information which is immensely useful when writing raw assembly, since firing the debugger is often our only recourse in face of a bug. To remove the debugging information, we can pass -s to the linker, for example when we are about to ship our program and want to save a few KiB. We finally have an executable: $ file ./main\nmain: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, with debug_info, not stripped We can see the different sections with readelf -a ./main , and it tells us that the .text section, which contains our code, is only 3 bytes long. Now, if we try to run our program, it will segfault. That\'s because we are expected by the operating system to exit (using the exit system call) ourselves (otherwise the CPU will keep executing whatever comes after our entry point until it hits an unmapped page, triggering a segfault). That\'s what libc does for us in C programs, so let\'s handle that: %define SYSCALL_EXIT 60\n\nglobal _start:\n_start:\n  mov rax, SYSCALL_EXIT\n  mov rdi, 0\n  syscall nasm uses the Intel syntax: &lt;instruction&gt; &lt;destination&gt;, &lt;source&gt; , so mov rdi, 0 puts 0 into the register rdi . Other assemblers use the AT&amp;T syntax which swaps the source and destination. My advice: pick one syntax and one assembler and stick to it, both syntaxes are fine and most tools have some support for both. Following the System V ABI, which is required on Linux and other Unices for system calls, invoking a system call requires us to put the system call code in the register rax , the parameters to the syscall (up to 6) in the registers rdi , rsi , rdx , rcx , r8 , r9 , and additional parameters, if any, on the stack (which will not happen in this program so we can forget about it).\nWe then use the instruction syscall and check rax for the return value, 0 usually meaning: no error. Note that Linux (and perhaps other Unices?) has a \'fun\' difference, which is that the fourth parameter of a system call is actually passed using the register r10 . Astute readers have pointed out that this is the case across all OSes and documented in the x86_64 architecture supplement of the System V ABI. The more you know! That\'s only for system calls, though, regular functions still use rcx for the fourth parameter. Note that the System V ABI is required when making system calls and when interfacing with C but we are free to use whatever conventions we want in our own assembly code. For a long time, Go was using a different calling convention than the System V ABI, for example, when calling functions (passing arguments on the stack). Most tools (debuggers, profilers) expect the System V ABI though, so I recommend sticking to it. Back to our program: when we run it, we see...nothing. That\'s because everything went well, true to the UNIX philosophy! We can check the exit code: $ ./main; echo $?\n0 Changing mov rdi, 0 to mov rdi, 8 will now result in: $ ./main; echo $?\n8 Another way to observe system calls made by a program is with strace , which will also prove very useful when troubleshooting. On some BSD, its equivalent is truss or dtruss . $ strace ./main\nexecve(&quot;./main&quot;, [&quot;./main&quot;], 0x7ffc60e6bf10 /* 60 vars */) = 0\nexit(8)                                 = ?\n+++ exited with 8 +++ Let\'s change it back to 0 and continue. A stack primer Before we can continue, we need to know the basics of how the stack works in assembly since we have no friendly compiler to do that for us. The three most important things about the stack are: It grows downwards: to reserve more space on the stack, we decrease the value of rsp A function must restore the stack pointer to its original value before the function returns, meaning, either remember the original value and set rsp to this, or, match every decrement by an increment of the same value. Before a function call, the stack pointer needs to be 16 bytes aligned, according to the System V ABI. Also, at the very beginning of a function, the stack pointer value is: 16*N + 8 . That\'s because before the function call, its value was 16 byte aligned, i.e. 16*N , and the call instruction pushes on the stack the current location (the register rip , which is 8 bytes long), to know where to jump when the called function returns. Not abiding by those rules will result in nasty crashes, so be warned. That\'s because the location of where to jump when the function returns will be likely overwritten and the program will jump to the wrong location. That, or the stack content will be overwritten and the program will operate on wrong values. Bad either way. A small stack example Let\'s write a function that prints hello to the standard out, using the stack, to learn the ropes. An easier way would be to store this static string in the .rodata section, but that would not teach us anything about the stack. We need to reserve (at least) 5 bytes on the stack, since that\'s the length in bytes of hello . The stack looks like this: ... rbp o l l e h And rsp points to the bottom of it. Here\'s how we access each element: Memory location (example) Assembly code Stack element 0x1016 ... 0x1015 rsp + 5 rbp 0x1014 rsp + 4 o 0x1013 rsp + 3 l 0x1012 rsp + 2 l 0x1011 rsp + 1 e 0x1010 rsp + 0 h We then pass the address on the stack of the beginning of the string to the write syscall, as well as its length: %define SYSCALL_WRITE 1\n%define STDOUT 1\n\nprint_hello:\n  push rbp ; Save rbp on the stack to be able to restore it at the end of the function.\n  mov rbp, rsp ; Set rbp to rsp\n\n  sub rsp, 5 ; Reserve 5 bytes of space on the stack.\n  mov BYTE [rsp + 0], \'h\' ; Set each byte on the stack to a string character.\n  mov BYTE [rsp + 1], \'e\'\n  mov BYTE [rsp + 2], \'l\'\n  mov BYTE [rsp + 3], \'l\'\n  mov BYTE [rsp + 4], \'o\'\n\n  ; Make the write syscall\n  mov rax, SYSCALL_WRITE\n  mov rdi, STDOUT ; Write to stdout.\n  lea rsi, [rsp] ; Address on the stack of the string.\n  mov rdx, 5 ; Pass the length of the string which is 5.\n  syscall\n\n  add rsp, 5 ; Restore the stack to its original value.\n\n  pop rbp ; Restore rbp\n  ret lea destination, source loads the effective address of the source into the destination, which is how C pointers are implemented. To dereference a memory location we use square brackets. So, assuming we just have loaded an address into rdi with lea , e.g. lea rdi, [hello_world] , and we want to store the value at the address into rax , we do: mov rax, [rdi] . We usually have to tell nasm how many bytes to dereference with BYTE , WORD , DWORD , QWORD so: mov rax, DWORD [rdi] , because nasm does not keep track of the sizes of each variable. That\'s also what the C compiler does when we dereference a int8_t , int16_t , int32_t , and int64_t pointer, respectively. There is a lot to unpack here. First, what is rbp ? That\'s a register like any other. But, you can choose to follow the convention of not using this register like the other registers, to store arbitrary values, and instead, use it to store a linked list of call frames. That\'s a lot of words. Basically, at the very beginning of a function, the value of rbp is stored on the stack (that\'s push rbp ). Since rbp stores an address (the address of the frame that\'s called us), we are storing on the stack the address of the caller in a known location. Immediately after that, we set rbp to rsp , that is, to the stack pointer at the beginning of the function. push rbp and mov rbp, rsp are thus usually referred to as the function prolog. For the rest of the function body, we treat rbp as a constant and only decrease rsp if we need to reserve space on the stack. So if function A calls function B which in turn calls function C, and each function stores on the stack the address of the caller frame, we know where to find on the stack the address of each. Thus, we can print a stack trace in any location of our program simply by inspecting the stack. Pretty nifty. That\'s already very useful to profilers and other similar tools. We must not forget of course, just before we exit the function, to restore rbp to its original value (which is still on the stack at that point): that\'s pop rbp . This is also known as the function epilog. Another way to look at it is that we remove the last element of the linked list of call frames, since we are exiting the leaf function. Don\'t worry if you have not fully understood everything, just remember to always have the function epilogs and prologs and you\'ll be fine: my_function:\n  push rbp\n  mov rbp, rsp\n\n  sub rsp, N\n\n  [...]\n\n\n  add rsp, N\n  pop rbp\n  ret Note : There is an optimization method that uses rbp as a standard register (with a C compiler, that\'s the flag -fomit-frame-pointer ), which means we lose the information about the call stack. My advice is: never do this, it is no worth it. Wait, but didn\'t you say the stack needs to be 16 byte aligned (that is, a multiple of 16)? Last time I checked, 5 is not really a multiple of 16! Good catch! The only reason why this program works, is that print_hello is a leaf function, meaning it does not call another function. Remember, the stack needs to be 16 bytes aligned when we do a call ! So the correct way would be: print_hello:\n  push rbp\n  mov rbp, rsp\n\n  sub rsp, 16\n  mov BYTE [rsp + 0], \'h\'\n  mov BYTE [rsp + 1], \'e\'\n  mov BYTE [rsp + 2], \'l\'\n  mov BYTE [rsp + 3], \'l\'\n  mov BYTE [rsp + 4], \'o\'\n\n  mov rax, SYSCALL_WRITE\n  mov rdi, STDOUT\n  lea rsi, [rsp]\n  mov rdx, 5\n  syscall\n\n  call print_world\n\n  add rsp, 16\n\n  pop rbp\n  ret Since when we enter the function, the value of rsp is 16*N+8 , and pushing rbp increases it by 8, the stack pointer is 16 bytes aligned at the point of sub rsp, 16 . Decrementing it by 16 (or a multiple of 16) keeps it 16 bytes aligned. We now can safely call another function from within print_hello : print_world:\n  push rbp\n  mov rbp, rsp\n\n  sub rsp, 16\n  mov BYTE [rsp + 0], \' \'\n  mov BYTE [rsp + 1], \'w\'\n  mov BYTE [rsp + 2], \'o\'\n  mov BYTE [rsp + 3], \'r\'\n  mov BYTE [rsp + 4], \'l\'\n  mov BYTE [rsp + 5], \'d\'\n\n  mov rax, SYSCALL_WRITE\n  mov rdi, STDOUT\n  lea rsi, [rsp]\n  mov rdx, 6\n  syscall\n\n  add rsp, 16\n\n  pop rbp\n  ret\n\nprint_hello:\n  push rbp\n  mov rbp, rsp\n\n  sub rsp, 16\n  mov BYTE [rsp + 0], \'h\'\n  mov BYTE [rsp + 1], \'e\'\n  mov BYTE [rsp + 2], \'l\'\n  mov BYTE [rsp + 3], \'l\'\n  mov BYTE [rsp + 4], \'o\'\n\n  mov rax, SYSCALL_WRITE\n  mov rdi, STDOUT\n  lea rsi, [rsp]\n  mov rdx, 5\n  syscall\n\n  call print_world\n\n  add rsp, 16\n\n  pop rbp\n  ret And we get hello world as an output. Now, try to do sub rsp, 5 in print_hello , and your program may crash. There is no guarantee, that\'s what makes it hard to track down. My advice is: Always use the standard function prologs and epilogs Always increment/decrement rsp by (a multiple of) 16 Address items on the stack relative to rsp , i.e. mov BYTE [rsp + 4], \'o\' If you have to decrement rsp by a value that\'s unknown at compile time (similar to how alloca() works in C), you can and rsp, -16 to 16 bytes align it. And you\'ll be safe. The last point is interesting, see for yourself: (gdb) p -100 &amp; -16\n$1 = -112\n(gdb) p -112 &amp; -16\n$2 = -112 Which translates in assembly to: sub rsp, 100\nand rsp, -16 Finally, following those conventions means that our assembly functions can be safely called from C or other languages following the System V ABI , without any modification, which is great. I have not talked about the red zone which is a 128 byte region at the bottom of the stack which our program is free to use as it pleases without having to change the stack pointer. In my opinion, it is not helpful and creates hard to track bugs, so I do not recommend to use it. To disable it entirely, run: nasm -f elf64 -g main.nasm &amp;&amp; cc main.o -static -o main -mno-red-zone -nostdlib . Opening a socket We now are ready to open a socket with the socket(2) syscall, so we add a few constants, taken from the libc headers ( note that those values might actually be different on a different Unix, I have not checked. Again, a few %ifdef can easily remedy this discrepancy ): %define AF_UNIX 1\n%define SOCK_STREAM 1\n\n%define SYSCALL_SOCKET 41 The AF_UNIX constant means we want a Unix domain socket, and SOCK_STREAM means stream-oriented . We use a domain socket since we now that our server is running on the same machine and it should be faster, but we could change it to AF_INET to connect to a remote IPv4 address for example. We then fill the relevant registers with those values and invoke the system call: mov rax, SYSCALL_SOCKET\n  mov rdi, AF_UNIX ; Unix socket.\n  mov rsi, SOCK_STREAM ; Stream oriented.\n  mov rdx, 0 ; Automatic protocol.\n  syscall The C equivalent would be: socket(AF_UNIX, SOCK_STREAM, 0); . So you see that if we fill the registers in the same order as the C function parameters, we stay close to what C code would do. The whole program now looks like this: BITS 64 ; 64 bits.\nCPU X64 ; Target the x86_64 family of CPUs.\n\nsection .text\n\n%define AF_UNIX 1\n%define SOCK_STREAM 1\n\n%define SYSCALL_SOCKET 41\n%define SYSCALL_EXIT 60\n\nglobal _start:\n_start:\n  ; open a unix socket.\n  mov rax, SYSCALL_SOCKET\n  mov rdi, AF_UNIX ; Unix socket.\n  mov rsi, SOCK_STREAM ; Stream oriented.\n  mov rdx, 0 ; automatic protocol.\n  syscall\n\n\n  ; The end.\n  mov rax, SYSCALL_EXIT\n  mov rdi, 0\n  syscall Building and running it under strace shows that it works and we get a socket with the file descriptor 3 (in this case, it might be different for you if you are following at home): $ nasm -f elf64 -g main.nasm &amp;&amp; ld main.o -static -o main \n$ strace ./main\nexecve(&quot;./main&quot;, [&quot;./main&quot;], 0x7ffe54dfe550 /* 60 vars */) = 0\nsocket(AF_UNIX, SOCK_STREAM, 0)         = 3\nexit(0)                                 = ?\n+++ exited with 0 +++ Connecting to the server Now that we have created a socket, we can connect to the server with the connect(2) system call. It\'s a good time to extract that logic in its own little function, just like in any other high-level language. x11_connect_to_server:\n  ; TODO In assembly, a function is simply a label we can jump to. But for clarity, both for readers of the code and tools, we can add a hint that this is a real function we can call, like this: call x11_connect_to_server . This will improve the call stack for example when using strace -k . This hint has the form (in nasm ): static &lt;name of the function&gt;:function . Of course, we also need to add our standard function prolog and epilog: x11_connect_to_server:\nstatic x11_connect_to_server:function\n  push rbp\n  mov rbp, rsp\n  \n  pop rbp\n  ret An additional help when reading functions in assembly code is adding comments describing what parameters they accept and what is the return value, if any. Since there is no language level feature for this, we resort to comments: ; Create a UNIX domain socket and connect to the X11 server.\n; @returns The socket file descriptor.\nx11_connect_to_server:\nstatic x11_connect_to_server:function\n  push rbp\n  mov rbp, rsp\n  \n  pop rbp\n  ret First, let\'s move the socket creation logic to our function and call it in the program: ; Create a UNIX domain socket and connect to the X11 server.\n; @returns The socket file descriptor.\nx11_connect_to_server:\nstatic x11_connect_to_server:function\n  push rbp\n  mov rbp, rsp\n  \n  ; Open a Unix socket: socket(2).\n  mov rax, SYSCALL_SOCKET\n  mov rdi, AF_UNIX ; Unix socket.\n  mov rsi, SOCK_STREAM ; Stream oriented.\n  mov rdx, 0 ; Automatic protocol.\n  syscall\n\n  cmp rax, 0\n  jle die\n\n  mov rdi, rax ; Store socket fd in `rdi` for the remainder of the function.\n\n  pop rbp\n  ret\n\ndie:\n  mov rax, SYSCALL_EXIT\n  mov rdi, 1\n  syscall\n\n_start:\nglobal _start:function\n  call x11_connect_to_server\n  \n  ; The end.\n  mov rax, SYSCALL_EXIT\n  mov rdi, 0\n  syscall The error checking is very simplistic: we only check that the return value of the system call (in rax ) is what we expect, otherwise we exit the program with a non-zero code by jumping to the die section. jle is a conditional jump, which inspects global flags, hopefully set just before with cmp or test , and jumps to a label if the condition is true. Here, we compare the returned value with 0, and if it is lower or equal to 0, we jump to the error label. That\'s how we implement conditionals and loops. Ok, we can finally connect to the server now. The connect(2) system call takes the address of a sockaddr_un structure as the second argument. This structure is too big to fit in a register. This is the first syscall we encounter that needs to be passed a pointer, in other words, the address of a region in memory. That region can be on the stack or on the heap, or even be our own executable mapped in memory. That\'s assembly, we get to do whatever we want. Since we want to keep things simple and fast, we will store everything in this program on the stack. And since we have 8 MiB of it (according to limit , on my machine, that is), it\'ll be plenty enough. Actually, the most space we will need on the stack in this program will be 32 KiB. The size of the sockaddr_un structure is 110 bytes, so we reserve 112 to align rsp to 16 bytes. Nasm does have structs, but they are rather a way to define offsets with a name, than structures like in C with a specific syntax to address a specific field. For the sake of simplicity, I\'ll use the manual way, without nasm structs. We set the first 2 bytes of this structure to AF_UNIX since this is a domain socket. Then comes the path of the Unix domain socket which X11 expects to be in a certain format. We want to display our window on the first monitor starting at 0, so the string is: /tmp/.X11-unix/X0 . In C, we would do: const sockaddr_un addr = {.sun_family = AF_UNIX,\n                            .sun_path = &quot;/tmp/.X11-unix/X0&quot;};\n  const int res =\n      connect(x11_socket_fd, (const struct sockaddr *)&amp;addr, sizeof(addr)); How do we translate that to assembly, especially the string part? We could set each byte to each character of the string in the structure, on the stack, manually, one by one. Another way to do it is to use the rep movsb idiom, which instructs the CPU to copy a character from a string A to another string B, N times. This is exactly what we need! The way it works is: We put the string in the .rodata section (same as the data section but read-only) We load its address in rsi (it\'s the source) We load the address of the string in the structure on the stack in rdi (it\'s the destination) We set rcx to the number of bytes to be copied We use cld to clear the DF flag to ensure the copy is done forwards (since it can also be done backwards) We call rep movsb and voila It\'s basically memcpy from C. This is an interesting case: we can see that some instructions expect some of their operands to be in certain registers and there is no way around it.  So, we have to plan ahead and expect those registers to be overwritten. If we need to keep their original values around, we have to store those values elsewhere, for example on the stack (that\'s called spilling) or in other registers. This is a broader topic of register allocation which is NP-hard! In small functions, it\'s manageable though. First, the .rodata section: section .rodata\n\nsun_path: db &quot;/tmp/.X11-unix/X0&quot;, 0\nstatic sun_path:data Then we copy the string: mov WORD [rsp], AF_UNIX ; Set sockaddr_un.sun_family to AF_UNIX\n  ; Fill sockaddr_un.sun_path with: &quot;/tmp/.X11-unix/X0&quot;.\n  lea rsi, sun_path\n  mov r12, rdi ; Save the socket file descriptor in `rdi` in `r12`.\n  lea rdi, [rsp + 2]\n  cld ; Move forward\n  mov ecx, 19 ; Length is 19 with the null terminator.\n  rep movsb ; Copy. ecx is the 32 bit form of the register rcx , meaning we only set here the lower 32 bits of the 64 bit register. This handy table lists all of the forms for all of the registers. But be cautious of the pitfall case of only setting a value in part of a register, and then using the whole register later. The rest of the bits that have not been set will contain some past value, which is hard to troubleshoot. The solution is to use movzx to zero extend, meaning setting the rest of the bits to 0. A good way to visualize this is to use info registers within gdb, and that will display for each register the value for each of its forms, e.g. for rcx , it will display the value for rcx , ecx , cx , ch , cl . Then, we do the syscall, check the returned value, exit the program if the value is not 0, and finally return the socket file descriptor, which will be used every time in the rest of the program when talking to the X11 server. Everything together, it looks like: ; Create a UNIX domain socket and connect to the X11 server.\n; @returns The socket file descriptor.\nx11_connect_to_server:\nstatic x11_connect_to_server:function\n  push rbp\n  mov rbp, rsp \n\n  ; Open a Unix socket: socket(2).\n  mov rax, SYSCALL_SOCKET\n  mov rdi, AF_UNIX ; Unix socket.\n  mov rsi, SOCK_STREAM ; Stream oriented.\n  mov rdx, 0 ; Automatic protocol.\n  syscall\n\n  cmp rax, 0\n  jle die\n\n  mov rdi, rax ; Store socket fd in `rdi` for the remainder of the function.\n\n  sub rsp, 112 ; Store struct sockaddr_un on the stack.\n\n  mov WORD [rsp], AF_UNIX ; Set sockaddr_un.sun_family to AF_UNIX\n  ; Fill sockaddr_un.sun_path with: &quot;/tmp/.X11-unix/X0&quot;.\n  lea rsi, sun_path\n  mov r12, rdi ; Save the socket file descriptor in `rdi` in `r12`.\n  lea rdi, [rsp + 2]\n  cld ; Move forward\n  mov ecx, 19 ; Length is 19 with the null terminator.\n  rep movsb ; Copy.\n\n  ; Connect to the server: connect(2).\n  mov rax, SYSCALL_CONNECT\n  mov rdi, r12\n  lea rsi, [rsp]\n  %define SIZEOF_SOCKADDR_UN 2+108\n  mov rdx, SIZEOF_SOCKADDR_UN\n  syscall\n\n  cmp rax, 0\n  jne die\n\n  mov rax, rdi ; Return the socket fd.\n\n  add rsp, 112\n  pop rbp\n  ret We are ready to talk to the X11 server! Sending data over the socket There is the send(2) syscall to do this, but we can keep it simple and use the generic write(2) syscall instead. Either way works. %define SYSCALL_WRITE 1 The C structure for the handshake in the case of success looks like this: typedef struct {\n  u8 order;\n  u8 pad1;\n  u16 major, minor;\n  u16 auth_proto_len, auth_data_len;\n  u16 pad2;\n  // Optionally, authorization information follows, if `auth_proto_len` and `auth_data_len` are  not 0.\n} x11_connection_req_t; pad* fields can be ignored since they are padding and their value is not read by the server. For our handshake, we need to set the order to be l , that is, little-endian, since X11 can be told to interpret message as big or little endian. Since x64 is little-endian, we do not want to have an endianness translation layer and so we stick to little-endian. We also need to set the major field, which is the version, to 11 . I\'ll leave it to the reader to guess why. In C, we would do: x11_connection_req_t req = {.order = \'l\', .major = 11}; This structure is only 12 bytes long, since we do not use authorization (we leave all subsequent fields after the minor_version as 0). But since we will have to read the response from the server which is quite big (around 14 KiB during my testing), we will right away reserve a lot of space on the stack, 32 KiB, to be safe: sub rsp, 1&lt;&lt;15\n  mov BYTE [rsp + 0], \'l\' ; Set order to \'l\'.\n  mov WORD [rsp + 2], 11 ; Set major version to 11. Then we send it to the server: ; Send the handshake to the server: write(2).\n  mov rax, SYSCALL_WRITE\n  mov rdi, rdi\n  lea rsi, [rsp]\n  mov rdx, 12\n  syscall\n\n  cmp rax, 12 ; Check that all bytes were written.\n  jnz die After that, we read the server response, which should be at first 8 bytes: ; Read the server response: read(2).\n  ; Use the stack for the read buffer.\n  ; The X11 server first replies with 8 bytes. Once these are read, it replies with a much bigger message.\n  mov rax, SYSCALL_READ\n  mov rdi, rdi\n  lea rsi, [rsp]\n  mov rdx, 8\n  syscall\n\n  cmp rax, 8 ; Check that the server replied with 8 bytes.\n  jnz die\n\n  cmp BYTE [rsp], 1 ; Check that the server sent \'success\' (first byte is 1).\n  jnz die The first byte in the server response is 0 for failure and 1 for success (and 2 for authentication but we will not need it here). The server sends a big message with a lot of general information, which we will need for later, so we store certain fields in global variables located in the data section. First we add those variables, each 4 bytes big: section .data\n\nid: dd 0\nstatic id:data\n\nid_base: dd 0\nstatic id_base:data\n\nid_mask: dd 0\nstatic id_mask:data\n\nroot_visual_id: dd 0\nstatic root_visual_id:data Then we read the server response, and skip over the parts we are not interested in. This boils down to incrementing a pointer by a dynamic value, a few times. Note that since we do not do any checks here, that would be a great attack vector to trigger a stack overflow or such in our program. ; Read the rest of the server response: read(2).\n  ; Use the stack for the read buffer.\n  mov rax, SYSCALL_READ\n  mov rdi, rdi\n  lea rsi, [rsp]\n  mov rdx, 1&lt;&lt;15\n  syscall\n\n  cmp rax, 0 ; Check that the server replied with something.\n  jle die\n\n  ; Set id_base globally.\n  mov edx, DWORD [rsp + 4]\n  mov DWORD [id_base], edx\n\n  ; Set id_mask globally.\n  mov edx, DWORD [rsp + 8]\n  mov DWORD [id_mask], edx\n\n  ; Read the information we need, skip over the rest.\n  lea rdi, [rsp] ; Pointer that will skip over some data.\n  \n  mov cx, WORD [rsp + 16] ; Vendor length (v).\n  movzx rcx, cx\n\n  mov al, BYTE [rsp + 21]; Number of formats (n).\n  movzx rax, al ; Fill the rest of the register with zeroes to avoid garbage values.\n  imul rax, 8 ; sizeof(format) == 8\n\n  add rdi, 32 ; Skip the connection setup\n  add rdi, rcx ; Skip over the vendor information (v).\n\n  ; Skip over padding.\n  add rdi, 3\n  and rdi, -4\n\n  add rdi, rax ; Skip over the format information (n*8).\n\n  mov eax, DWORD [rdi] ; Store (and return) the window root id.\n\n  ; Set the root_visual_id globally.\n  mov edx, DWORD [rdi + 32]\n  mov DWORD [root_visual_id], edx A small aside about padding, thanks to a perspicacious reader : How we skip padding is the only bit of smartness we allow ourselves: some fields in the X11 protocol have a variable length. But the X11 protocol counts everything in units of \'4 bytes\'. Meaning, if a field is only 5 bytes long, per the protocol, there will be 3 bytes of padding (which should be skipped over by the application), so that the field occupies 2 units of 4 bytes (it is 4 bytes-aligned). How do we do that then? The specification uses some division and modulo operations, but those are annoying to do in assembly. We can do better. libX11 uses this macro: #define ROUNDUP(nbytes, pad) (((nbytes) + ((pad)-1)) &amp; ~(long)((pad)-1)) And it should be used so: assert(ROUNDUP(0, 4) == 0);\nassert(ROUNDUP(1, 4) == 4);\nassert(ROUNDUP(2, 4) == 4);\nassert(ROUNDUP(3, 4) == 4);\nassert(ROUNDUP(4, 4) == 4);\nassert(ROUNDUP(5, 4) == 8);\n// etc This works, but is kind of complex. If we look at this output when compiling this code, we see that gcc smartly optimizes this macro down to: add     eax, 3\n  and     eax, -4 So we use this form. All together: ; Send the handshake to the X11 server and read the returned system information.\n; @param rdi The socket file descriptor\n; @returns The window root id (uint32_t) in rax.\nx11_send_handshake:\nstatic x11_send_handshake:function\n  push rbp\n  mov rbp, rsp\n\n  sub rsp, 1&lt;&lt;15\n  mov BYTE [rsp + 0], \'l\' ; Set order to \'l\'.\n  mov WORD [rsp + 2], 11 ; Set major version to 11.\n\n  ; Send the handshake to the server: write(2).\n  mov rax, SYSCALL_WRITE\n  mov rdi, rdi\n  lea rsi, [rsp]\n  mov rdx, 12\n  syscall\n\n  cmp rax, 12 ; Check that all bytes were written.\n  jnz die\n\n  ; Read the server response: read(2).\n  ; Use the stack for the read buffer.\n  ; The X11 server first replies with 8 bytes. Once these are read, it replies with a much bigger message.\n  mov rax, SYSCALL_READ\n  mov rdi, rdi\n  lea rsi, [rsp]\n  mov rdx, 8\n  syscall\n\n  cmp rax, 8 ; Check that the server replied with 8 bytes.\n  jnz die\n\n  cmp BYTE [rsp], 1 ; Check that the server sent \'success\' (first byte is 1).\n  jnz die\n\n  ; Read the rest of the server response: read(2).\n  ; Use the stack for the read buffer.\n  mov rax, SYSCALL_READ\n  mov rdi, rdi\n  lea rsi, [rsp]\n  mov rdx, 1&lt;&lt;15\n  syscall\n\n  cmp rax, 0 ; Check that the server replied with something.\n  jle die\n\n  ; Set id_base globally.\n  mov edx, DWORD [rsp + 4]\n  mov DWORD [id_base], edx\n\n  ; Set id_mask globally.\n  mov edx, DWORD [rsp + 8]\n  mov DWORD [id_mask], edx\n\n  ; Read the information we need, skip over the rest.\n  lea rdi, [rsp] ; Pointer that will skip over some data.\n  \n  mov cx, WORD [rsp + 16] ; Vendor length (v).\n  movzx rcx, cx\n\n  mov al, BYTE [rsp + 21]; Number of formats (n).\n  movzx rax, al ; Fill the rest of the register with zeroes to avoid garbage values.\n  imul rax, 8 ; sizeof(format) == 8\n\n  add rdi, 32 ; Skip the connection setup\n  add rdi, rcx ; Skip over the vendor information (v).\n\n  ; Skip over padding.\n  add rdi, 3\n  and rdi, -4\n\n  add rdi, rax ; Skip over the format information (n*8).\n\n  mov eax, DWORD [rdi] ; Store (and return) the window root id.\n\n  ; Set the root_visual_id globally.\n  mov edx, DWORD [rdi + 32]\n  mov DWORD [root_visual_id], edx\n\n  add rsp, 1&lt;&lt;15\n  pop rbp\n  ret From this point on, I will assume you are familiar with the basics of assembly and X11 and will not go as much into details. Generating ids When creating resources on the server-side, we usually first generate an id on the client side, and send that id to the server when creating the resource. We store the current id in a global variable and increment it each time a new id is generated. This is how we do it: ; Increment the global id.\n; @return The new id.\nx11_next_id:\nstatic x11_next_id:function\n  push rbp\n  mov rbp, rsp\n\n  mov eax, DWORD [id] ; Load global id.\n\n  mov edi, DWORD [id_base] ; Load global id_base.\n  mov edx, DWORD [id_mask] ; Load global id_mask.\n\n  ; Return: id_mask &amp; (id) | id_base\n  and eax, edx\n  or eax, edi\n\n  add DWORD [id], 1 ; Increment id.\n\n  pop rbp\n  ret Opening a font To open a font, which is a prerequisite to draw text, we send a message to the server specifying (part of) the name of the font we want, and the server will select a matching font. To play with another font, you can use xfontsel which displays all the font names that the X11 server knows about. First, we generate an id for the font locally, and then we send it alongside the font name. ; Open the font on the server side.\n; @param rdi The socket file descriptor.\n; @param esi The font id.\nx11_open_font:\nstatic x11_open_font:function\n  push rbp\n  mov rbp, rsp\n\n  %define OPEN_FONT_NAME_BYTE_COUNT 5\n  %define OPEN_FONT_PADDING ((4 - (OPEN_FONT_NAME_BYTE_COUNT % 4)) % 4)\n  %define OPEN_FONT_PACKET_U32_COUNT (3 + (OPEN_FONT_NAME_BYTE_COUNT + OPEN_FONT_PADDING) / 4)\n  %define X11_OP_REQ_OPEN_FONT 0x2d\n\n  sub rsp, 6*8\n  mov DWORD [rsp + 0*4], X11_OP_REQ_OPEN_FONT | (OPEN_FONT_NAME_BYTE_COUNT &lt;&lt; 16)\n  mov DWORD [rsp + 1*4], esi\n  mov DWORD [rsp + 2*4], OPEN_FONT_NAME_BYTE_COUNT\n  mov BYTE [rsp + 3*4 + 0], \'f\'\n  mov BYTE [rsp + 3*4 + 1], \'i\'\n  mov BYTE [rsp + 3*4 + 2], \'x\'\n  mov BYTE [rsp + 3*4 + 3], \'e\'\n  mov BYTE [rsp + 3*4 + 4], \'d\'\n\n\n  mov rax, SYSCALL_WRITE\n  mov rdi, rdi\n  lea rsi, [rsp]\n  mov rdx, OPEN_FONT_PACKET_U32_COUNT*4\n  syscall\n\n  cmp rax, OPEN_FONT_PACKET_U32_COUNT*4\n  jnz die\n\n  add rsp, 6*8\n\n  pop rbp\n  ret Creating a graphical context Since an application in X11 can have multiple windows, we first need to create a graphical context containing the general information. When we create a window, we refer to this graphical context by id. Again, we need to generate an id for the graphical context to be. X11 stores a hierarchy of windows, so when creating the graphical context, we also need to give it the root window id (i.e. the parent id). ; Create a X11 graphical context.\n; @param rdi The socket file descriptor.\n; @param esi The graphical context id.\n; @param edx The window root id.\n; @param ecx The font id.\nx11_create_gc:\nstatic x11_create_gc:function\n  push rbp\n  mov rbp, rsp\n\n  sub rsp, 8*8\n\n%define X11_OP_REQ_CREATE_GC 0x37\n%define X11_FLAG_GC_BG 0x00000004\n%define X11_FLAG_GC_FG 0x00000008\n%define X11_FLAG_GC_FONT 0x00004000\n%define X11_FLAG_GC_EXPOSE 0x00010000\n\n%define CREATE_GC_FLAGS X11_FLAG_GC_BG | X11_FLAG_GC_FG | X11_FLAG_GC_FONT\n%define CREATE_GC_PACKET_FLAG_COUNT 3\n%define CREATE_GC_PACKET_U32_COUNT (4 + CREATE_GC_PACKET_FLAG_COUNT)\n%define MY_COLOR_RGB 0x0000ffff\n\n  mov DWORD [rsp + 0*4], X11_OP_REQ_CREATE_GC | (CREATE_GC_PACKET_U32_COUNT&lt;&lt;16)\n  mov DWORD [rsp + 1*4], esi\n  mov DWORD [rsp + 2*4], edx\n  mov DWORD [rsp + 3*4], CREATE_GC_FLAGS\n  mov DWORD [rsp + 4*4], MY_COLOR_RGB\n  mov DWORD [rsp + 5*4], 0\n  mov DWORD [rsp + 6*4], ecx\n\n  mov rax, SYSCALL_WRITE\n  mov rdi, rdi\n  lea rsi, [rsp]\n  mov rdx, CREATE_GC_PACKET_U32_COUNT*4\n  syscall\n\n  cmp rax, CREATE_GC_PACKET_U32_COUNT*4\n  jnz die\n  \n  add rsp, 8*8\n\n  pop rbp\n  ret Creating the window We can now create the window, which refers to the freshly created graphical context.\nWe also provide the desired x and y coordinates of the window, as well as the desired dimensions (width and height). Note that those are simply hints and the resulting window may well have different coordinates and dimensions, for example when using a tiling window manager, or when resizing the window. ; Create the X11 window.\n; @param rdi The socket file descriptor.\n; @param esi The new window id.\n; @param edx The window root id.\n; @param ecx The root visual id.\n; @param r8d Packed x and y.\n; @param r9d Packed w and h.\nx11_create_window:\nstatic x11_create_window:function\n  push rbp\n  mov rbp, rsp\n\n  %define X11_OP_REQ_CREATE_WINDOW 0x01\n  %define X11_FLAG_WIN_BG_COLOR 0x00000002\n  %define X11_EVENT_FLAG_KEY_RELEASE 0x0002\n  %define X11_EVENT_FLAG_EXPOSURE 0x8000\n  %define X11_FLAG_WIN_EVENT 0x00000800\n  \n  %define CREATE_WINDOW_FLAG_COUNT 2\n  %define CREATE_WINDOW_PACKET_U32_COUNT (8 + CREATE_WINDOW_FLAG_COUNT)\n  %define CREATE_WINDOW_BORDER 1\n  %define CREATE_WINDOW_GROUP 1\n\n  sub rsp, 12*8\n\n  mov DWORD [rsp + 0*4], X11_OP_REQ_CREATE_WINDOW | (CREATE_WINDOW_PACKET_U32_COUNT &lt;&lt; 16)\n  mov DWORD [rsp + 1*4], esi\n  mov DWORD [rsp + 2*4], edx\n  mov DWORD [rsp + 3*4], r8d\n  mov DWORD [rsp + 4*4], r9d\n  mov DWORD [rsp + 5*4], CREATE_WINDOW_GROUP | (CREATE_WINDOW_BORDER &lt;&lt; 16)\n  mov DWORD [rsp + 6*4], ecx\n  mov DWORD [rsp + 7*4], X11_FLAG_WIN_BG_COLOR | X11_FLAG_WIN_EVENT\n  mov DWORD [rsp + 8*4], 0\n  mov DWORD [rsp + 9*4], X11_EVENT_FLAG_KEY_RELEASE | X11_EVENT_FLAG_EXPOSURE\n\n\n  mov rax, SYSCALL_WRITE\n  mov rdi, rdi\n  lea rsi, [rsp]\n  mov rdx, CREATE_WINDOW_PACKET_U32_COUNT*4\n  syscall\n\n  cmp rax, CREATE_WINDOW_PACKET_U32_COUNT*4\n  jnz die\n\n  add rsp, 12*8\n\n  pop rbp\n  ret Mapping the window If you are following along at home, and just ran the program, you have realized nothing is displayed. That is because X11 does not show the window until we have mapped it. This is a simple message to send: ; Map a X11 window.\n; @param rdi The socket file descriptor.\n; @param esi The window id.\nx11_map_window:\nstatic x11_map_window:function\n  push rbp\n  mov rbp, rsp\n\n  sub rsp, 16\n\n  %define X11_OP_REQ_MAP_WINDOW 0x08\n  mov DWORD [rsp + 0*4], X11_OP_REQ_MAP_WINDOW | (2&lt;&lt;16)\n  mov DWORD [rsp + 1*4], esi\n\n  mov rax, SYSCALL_WRITE\n  mov rdi, rdi\n  lea rsi, [rsp]\n  mov rdx, 2*4\n  syscall\n\n  cmp rax, 2*4\n  jnz die\n\n  add rsp, 16\n\n  pop rbp\n  ret We now have a black window: Yay! Polling for server messages We would like to draw text in the window now, but we have to wait for the Expose event to be sent to us, which means that the window is visible, to be able to start drawing on it. We want to listen for all server messages actually, be it errors or events, for example when the user presses a key on the keyboard. If we do a simple blocking read(2) , but the server sends nothing, the program will appear not responding. Not good.\nThe solution is to use the poll(2) system call to be awoken by the operating system whenever there is data to be read on the socket, a la NodeJS or Nginx. A shrewd reader has pointed out that we could simply read from the socket in a loop, since we only have one, possibly with a timeout. Linux, and perhaps others, support setting a read timeout on a socket with setsockopt(2) . But I will keep this version in this article since this is the original one. Feel free to experiment with the alternative at home! First, we need to mark the socket as \'non-blocking\' since it is by default in blocking mode: ; Set a file descriptor in non-blocking mode.\n; @param rdi The file descriptor.\nset_fd_non_blocking:\nstatic set_fd_non_blocking:function\n  push rbp\n  mov rbp, rsp\n\n  mov rax, SYSCALL_FCNTL\n  mov rdi, rdi \n  mov rsi, F_GETFL\n  mov rdx, 0\n  syscall\n\n  cmp rax, 0\n  jl die\n\n  ; `or` the current file status flag with O_NONBLOCK.\n  mov rdx, rax\n  or rdx, O_NONBLOCK\n\n  mov rax, SYSCALL_FCNTL\n  mov rdi, rdi \n  mov rsi, F_SETFL\n  mov rdx, rdx\n  syscall\n\n  cmp rax, 0\n  jl die\n\n  pop rbp\n  ret Then, we write a small function to read data on the socket. For simplicity, we only read 32 bytes of data, because most messages from X11 are of this size. We also return the first byte which contains the event type. ; Read the X11 server reply.\n; @return The message code in al.\nx11_read_reply:\nstatic x11_read_reply:function\n  push rbp\n  mov rbp, rsp\n\n  sub rsp, 32\n  \n  mov rax, SYSCALL_READ\n  mov rdi, rdi\n  lea rsi, [rsp]\n  mov rdx, 32\n  syscall\n\n  cmp rax, 1\n  jle die\n\n  mov al, BYTE [rsp]\n\n  add rsp, 32\n\n  pop rbp\n  ret We now can poll. If an error occurs or the other side has closed their end of the socket, we exit the program. ; Poll indefinitely messages from the X11 server with poll(2).\n; @param rdi The socket file descriptor.\n; @param esi The window id.\n; @param edx The gc id.\npoll_messages:\nstatic poll_messages:function\n  push rbp\n  mov rbp, rsp\n\n  sub rsp, 32\n\n  %define POLLIN 0x001\n  %define POLLPRI 0x002\n  %define POLLOUT 0x004\n  %define POLLERR  0x008\n  %define POLLHUP  0x010\n  %define POLLNVAL 0x020\n\n  mov DWORD [rsp + 0*4], edi\n  mov DWORD [rsp + 1*4], POLLIN\n\n  mov DWORD [rsp + 16], esi ; window id\n  mov DWORD [rsp + 20], edx ; gc id\n\n  .loop:\n    mov rax, SYSCALL_POLL\n    lea rdi, [rsp]\n    mov rsi, 1\n    mov rdx, -1\n    syscall\n\n    cmp rax, 0\n    jle die\n\n    cmp DWORD [rsp + 2*4], POLLERR  \n    je die\n\n    cmp DWORD [rsp + 2*4], POLLHUP  \n    je die\n\n    mov rdi, [rsp + 0*4]\n    call x11_read_reply\n\n    jmp .loop\n\n  add rsp, 32\n  pop rbp\n  ret Drawing text At last, we can draw text. The small difficulty here is that the text is of unknown length in the general case, so we have to compute the size of the X11 message, including the padding at the end. So far, we only had messages of fixed size. The official documentation has formulas to compute those values. ; Draw text in a X11 window with server-side text rendering.\n; @param rdi The socket file descriptor.\n; @param rsi The text string.\n; @param edx The text string length in bytes.\n; @param ecx The window id.\n; @param r8d The gc id.\n; @param r9d Packed x and y.\nx11_draw_text:\nstatic x11_draw_text:function\n  push rbp\n  mov rbp, rsp\n\n  sub rsp, 1024\n\n  mov DWORD [rsp + 1*4], ecx ; Store the window id directly in the packet data on the stack.\n  mov DWORD [rsp + 2*4], r8d ; Store the gc id directly in the packet data on the stack.\n  mov DWORD [rsp + 3*4], r9d ; Store x, y directly in the packet data on the stack.\n\n  mov r8d, edx ; Store the string length in r8 since edx will be overwritten next.\n  mov QWORD [rsp + 1024 - 8], rdi ; Store the socket file descriptor on the stack to free the register.\n\n  ; Compute padding and packet u32 count with division and modulo 4.\n  mov eax, edx ; Put dividend in eax.\n  mov ecx, 4 ; Put divisor in ecx.\n  cdq ; Sign extend.\n  idiv ecx ; Compute eax / ecx, and put the remainder (i.e. modulo) in edx.\n  ; LLVM optimizer magic: `(4-x)%4 == -x &amp; 3`, for some reason.\n  neg edx\n  and edx, 3\n  mov r9d, edx ; Store padding in r9.\n\n  mov eax, r8d \n  add eax, r9d\n  shr eax, 2 ; Compute: eax /= 4\n  add eax, 4 ; eax now contains the packet u32 count.\n\n\n  %define X11_OP_REQ_IMAGE_TEXT8 0x4c\n  mov DWORD [rsp + 0*4], r8d\n  shl DWORD [rsp + 0*4], 8\n  or DWORD [rsp + 0*4], X11_OP_REQ_IMAGE_TEXT8\n  mov ecx, eax\n  shl ecx, 16\n  or [rsp + 0*4], ecx\n\n  ; Copy the text string into the packet data on the stack.\n  mov rsi, rsi ; Source string in rsi.\n  lea rdi, [rsp + 4*4] ; Destination\n  cld ; Move forward\n  mov ecx, r8d ; String length.\n  rep movsb ; Copy.\n\n  mov rdx, rax ; packet u32 count\n  imul rdx, 4\n  mov rax, SYSCALL_WRITE\n  mov rdi, QWORD [rsp + 1024 - 8] ; fd\n  lea rsi, [rsp]\n  syscall\n\n  cmp rax, rdx\n  jnz die\n\n  add rsp, 1024\n\n  pop rbp\n  ret We then call this function inside the polling loop, and we store the \'exposed\' state in a boolean on the stack to know whether we should render the text or not: %define X11_EVENT_EXPOSURE 0xc\n    cmp eax, X11_EVENT_EXPOSURE\n    jnz .received_other_event\n\n    .received_exposed_event:\n    mov BYTE [rsp + 24], 1 ; Mark as exposed.\n\n    .received_other_event:\n\n    cmp BYTE [rsp + 24], 1 ; exposed?\n    jnz .loop\n\n    .draw_text:\n      mov rdi, [rsp + 0*4] ; socket fd\n      lea rsi, [hello_world] ; string\n      mov edx, 13 ; length\n      mov ecx, [rsp + 16] ; window id\n      mov r8d, [rsp + 20] ; gc id\n      mov r9d, 100 ; x\n      shl r9d, 16\n      or r9d, 100 ; y\n      call x11_draw_text Finally, we see our Hello, world! text displayed inside the window: The end Wow, that was a lot. But we did it! We wrote a (albeit simplistic) GUI program in pure assembly, no dependencies, and that\'s just 600 lines of code in the end. How did we fare on the executable size part? With debug information: 10744 bytes (10 KiB) Without debug information (stripped): 8592 bytes (8 KiB) Stripped and OMAGIC ( --omagic linker flag, from the man page: Set the text and data sections to be readable and writable.  Also, do not page-align the data segment ): 1776 bytes (1 KiB) Not too shabby, a GUI program in 1 KiB. Where to go from there? We could move text rendering client-side. Doing it server-side has lots of limitations. We could add shape rendering, such as quads and circles We could listen to keyboard and mouse events (the polling loop is easy to extend to do that) I hope that you had as much fun as I did! Addendum: the full code The full code ; Build with: nasm -f elf64 -g main.nasm &amp;&amp; ld main.o -static -o main \n\nBITS 64 ; 64 bits.\nCPU X64 ; Target the x86_64 family of CPUs.\n\nsection .rodata\n\nsun_path: db &quot;/tmp/.X11-unix/X0&quot;, 0\nstatic sun_path:data\n\nhello_world: db &quot;Hello, world!&quot;\nstatic hello_world:data\n\nsection .data\n\nid: dd 0\nstatic id:data\n\nid_base: dd 0\nstatic id_base:data\n\nid_mask: dd 0\nstatic id_mask:data\n\nroot_visual_id: dd 0\nstatic root_visual_id:data\n\n\nsection .text\n\n%define AF_UNIX 1\n%define SOCK_STREAM 1\n\n%define SYSCALL_READ 0\n%define SYSCALL_WRITE 1\n%define SYSCALL_POLL 7\n%define SYSCALL_SOCKET 41\n%define SYSCALL_CONNECT 42\n%define SYSCALL_EXIT 60\n%define SYSCALL_FCNTL 72\n\n; Create a UNIX domain socket and connect to the X11 server.\n; @returns The socket file descriptor.\nx11_connect_to_server:\nstatic x11_connect_to_server:function\n  push rbp\n  mov rbp, rsp \n\n  ; Open a Unix socket: socket(2).\n  mov rax, SYSCALL_SOCKET\n  mov rdi, AF_UNIX ; Unix socket.\n  mov rsi, SOCK_STREAM ; Stream oriented.\n  mov rdx, 0 ; Automatic protocol.\n  syscall\n\n  cmp rax, 0\n  jle die\n\n  mov rdi, rax ; Store socket fd in `rdi` for the remainder of the function.\n\n  sub rsp, 112 ; Store struct sockaddr_un on the stack.\n\n  mov WORD [rsp], AF_UNIX ; Set sockaddr_un.sun_family to AF_UNIX\n  ; Fill sockaddr_un.sun_path with: &quot;/tmp/.X11-unix/X0&quot;.\n  lea rsi, sun_path\n  mov r12, rdi ; Save the socket file descriptor in `rdi` in `r12`.\n  lea rdi, [rsp + 2]\n  cld ; Move forward\n  mov ecx, 19 ; Length is 19 with the null terminator.\n  rep movsb ; Copy.\n\n  ; Connect to the server: connect(2).\n  mov rax, SYSCALL_CONNECT\n  mov rdi, r12\n  lea rsi, [rsp]\n  %define SIZEOF_SOCKADDR_UN 2+108\n  mov rdx, SIZEOF_SOCKADDR_UN\n  syscall\n\n  cmp rax, 0\n  jne die\n\n  mov rax, rdi ; Return the socket fd.\n\n  add rsp, 112\n  pop rbp\n  ret\n\n; Send the handshake to the X11 server and read the returned system information.\n; @param rdi The socket file descriptor\n; @returns The window root id (uint32_t) in rax.\nx11_send_handshake:\nstatic x11_send_handshake:function\n  push rbp\n  mov rbp, rsp\n\n  sub rsp, 1&lt;&lt;15\n  mov BYTE [rsp + 0], \'l\' ; Set order to \'l\'.\n  mov WORD [rsp + 2], 11 ; Set major version to 11.\n\n  ; Send the handshake to the server: write(2).\n  mov rax, SYSCALL_WRITE\n  mov rdi, rdi\n  lea rsi, [rsp]\n  mov rdx, 12\n  syscall\n\n  cmp rax, 12 ; Check that all bytes were written.\n  jnz die\n\n  ; Read the server response: read(2).\n  ; Use the stack for the read buffer.\n  ; The X11 server first replies with 8 bytes. Once these are read, it replies with a much bigger message.\n  mov rax, SYSCALL_READ\n  mov rdi, rdi\n  lea rsi, [rsp]\n  mov rdx, 8\n  syscall\n\n  cmp rax, 8 ; Check that the server replied with 8 bytes.\n  jnz die\n\n  cmp BYTE [rsp], 1 ; Check that the server sent \'success\' (first byte is 1).\n  jnz die\n\n  ; Read the rest of the server response: read(2).\n  ; Use the stack for the read buffer.\n  mov rax, SYSCALL_READ\n  mov rdi, rdi\n  lea rsi, [rsp]\n  mov rdx, 1&lt;&lt;15\n  syscall\n\n  cmp rax, 0 ; Check that the server replied with something.\n  jle die\n\n  ; Set id_base globally.\n  mov edx, DWORD [rsp + 4]\n  mov DWORD [id_base], edx\n\n  ; Set id_mask globally.\n  mov edx, DWORD [rsp + 8]\n  mov DWORD [id_mask], edx\n\n  ; Read the information we need, skip over the rest.\n  lea rdi, [rsp] ; Pointer that will skip over some data.\n  \n  mov cx, WORD [rsp + 16] ; Vendor length (v).\n  movzx rcx, cx\n\n  mov al, BYTE [rsp + 21]; Number of formats (n).\n  movzx rax, al ; Fill the rest of the register with zeroes to avoid garbage values.\n  imul rax, 8 ; sizeof(format) == 8\n\n  add rdi, 32 ; Skip the connection setup\n\n  ; Skip over padding.\n  add rdi, 3\n  and rdi, -4\n\n  add rdi, rcx ; Skip over the vendor information (v).\n  add rdi, rax ; Skip over the format information (n*8).\n\n  mov eax, DWORD [rdi] ; Store (and return) the window root id.\n\n  ; Set the root_visual_id globally.\n  mov edx, DWORD [rdi + 32]\n  mov DWORD [root_visual_id], edx\n\n  add rsp, 1&lt;&lt;15\n  pop rbp\n  ret\n\n; Increment the global id.\n; @return The new id.\nx11_next_id:\nstatic x11_next_id:function\n  push rbp\n  mov rbp, rsp\n\n  mov eax, DWORD [id] ; Load global id.\n\n  mov edi, DWORD [id_base] ; Load global id_base.\n  mov edx, DWORD [id_mask] ; Load global id_mask.\n\n  ; Return: id_mask &amp; (id) | id_base\n  and eax, edx\n  or eax, edi\n\n  add DWORD [id], 1 ; Increment id.\n\n  pop rbp\n  ret\n\n; Open the font on the server side.\n; @param rdi The socket file descriptor.\n; @param esi The font id.\nx11_open_font:\nstatic x11_open_font:function\n  push rbp\n  mov rbp, rsp\n\n  %define OPEN_FONT_NAME_BYTE_COUNT 5\n  %define OPEN_FONT_PADDING ((4 - (OPEN_FONT_NAME_BYTE_COUNT % 4)) % 4)\n  %define OPEN_FONT_PACKET_U32_COUNT (3 + (OPEN_FONT_NAME_BYTE_COUNT + OPEN_FONT_PADDING) / 4)\n  %define X11_OP_REQ_OPEN_FONT 0x2d\n\n  sub rsp, 6*8\n  mov DWORD [rsp + 0*4], X11_OP_REQ_OPEN_FONT | (OPEN_FONT_NAME_BYTE_COUNT &lt;&lt; 16)\n  mov DWORD [rsp + 1*4], esi\n  mov DWORD [rsp + 2*4], OPEN_FONT_NAME_BYTE_COUNT\n  mov BYTE [rsp + 3*4 + 0], \'f\'\n  mov BYTE [rsp + 3*4 + 1], \'i\'\n  mov BYTE [rsp + 3*4 + 2], \'x\'\n  mov BYTE [rsp + 3*4 + 3], \'e\'\n  mov BYTE [rsp + 3*4 + 4], \'d\'\n\n\n  mov rax, SYSCALL_WRITE\n  mov rdi, rdi\n  lea rsi, [rsp]\n  mov rdx, OPEN_FONT_PACKET_U32_COUNT*4\n  syscall\n\n  cmp rax, OPEN_FONT_PACKET_U32_COUNT*4\n  jnz die\n\n  add rsp, 6*8\n\n  pop rbp\n  ret\n\n; Create a X11 graphical context.\n; @param rdi The socket file descriptor.\n; @param esi The graphical context id.\n; @param edx The window root id.\n; @param ecx The font id.\nx11_create_gc:\nstatic x11_create_gc:function\n  push rbp\n  mov rbp, rsp\n\n  sub rsp, 8*8\n\n%define X11_OP_REQ_CREATE_GC 0x37\n%define X11_FLAG_GC_BG 0x00000004\n%define X11_FLAG_GC_FG 0x00000008\n%define X11_FLAG_GC_FONT 0x00004000\n%define X11_FLAG_GC_EXPOSE 0x00010000\n\n%define CREATE_GC_FLAGS X11_FLAG_GC_BG | X11_FLAG_GC_FG | X11_FLAG_GC_FONT\n%define CREATE_GC_PACKET_FLAG_COUNT 3\n%define CREATE_GC_PACKET_U32_COUNT (4 + CREATE_GC_PACKET_FLAG_COUNT)\n%define MY_COLOR_RGB 0x0000ffff\n\n  mov DWORD [rsp + 0*4], X11_OP_REQ_CREATE_GC | (CREATE_GC_PACKET_U32_COUNT&lt;&lt;16)\n  mov DWORD [rsp + 1*4], esi\n  mov DWORD [rsp + 2*4], edx\n  mov DWORD [rsp + 3*4], CREATE_GC_FLAGS\n  mov DWORD [rsp + 4*4], MY_COLOR_RGB\n  mov DWORD [rsp + 5*4], 0\n  mov DWORD [rsp + 6*4], ecx\n\n  mov rax, SYSCALL_WRITE\n  mov rdi, rdi\n  lea rsi, [rsp]\n  mov rdx, CREATE_GC_PACKET_U32_COUNT*4\n  syscall\n\n  cmp rax, CREATE_GC_PACKET_U32_COUNT*4\n  jnz die\n  \n  add rsp, 8*8\n\n  pop rbp\n  ret\n\n; Create the X11 window.\n; @param rdi The socket file descriptor.\n; @param esi The new window id.\n; @param edx The window root id.\n; @param ecx The root visual id.\n; @param r8d Packed x and y.\n; @param r9d Packed w and h.\nx11_create_window:\nstatic x11_create_window:function\n  push rbp\n  mov rbp, rsp\n\n  %define X11_OP_REQ_CREATE_WINDOW 0x01\n  %define X11_FLAG_WIN_BG_COLOR 0x00000002\n  %define X11_EVENT_FLAG_KEY_RELEASE 0x0002\n  %define X11_EVENT_FLAG_EXPOSURE 0x8000\n  %define X11_FLAG_WIN_EVENT 0x00000800\n  \n  %define CREATE_WINDOW_FLAG_COUNT 2\n  %define CREATE_WINDOW_PACKET_U32_COUNT (8 + CREATE_WINDOW_FLAG_COUNT)\n  %define CREATE_WINDOW_BORDER 1\n  %define CREATE_WINDOW_GROUP 1\n\n  sub rsp, 12*8\n\n  mov DWORD [rsp + 0*4], X11_OP_REQ_CREATE_WINDOW | (CREATE_WINDOW_PACKET_U32_COUNT &lt;&lt; 16)\n  mov DWORD [rsp + 1*4], esi\n  mov DWORD [rsp + 2*4], edx\n  mov DWORD [rsp + 3*4], r8d\n  mov DWORD [rsp + 4*4], r9d\n  mov DWORD [rsp + 5*4], CREATE_WINDOW_GROUP | (CREATE_WINDOW_BORDER &lt;&lt; 16)\n  mov DWORD [rsp + 6*4], ecx\n  mov DWORD [rsp + 7*4], X11_FLAG_WIN_BG_COLOR | X11_FLAG_WIN_EVENT\n  mov DWORD [rsp + 8*4], 0\n  mov DWORD [rsp + 9*4], X11_EVENT_FLAG_KEY_RELEASE | X11_EVENT_FLAG_EXPOSURE\n\n\n  mov rax, SYSCALL_WRITE\n  mov rdi, rdi\n  lea rsi, [rsp]\n  mov rdx, CREATE_WINDOW_PACKET_U32_COUNT*4\n  syscall\n\n  cmp rax, CREATE_WINDOW_PACKET_U32_COUNT*4\n  jnz die\n\n  add rsp, 12*8\n\n  pop rbp\n  ret\n\n; Map a X11 window.\n; @param rdi The socket file descriptor.\n; @param esi The window id.\nx11_map_window:\nstatic x11_map_window:function\n  push rbp\n  mov rbp, rsp\n\n  sub rsp, 16\n\n  %define X11_OP_REQ_MAP_WINDOW 0x08\n  mov DWORD [rsp + 0*4], X11_OP_REQ_MAP_WINDOW | (2&lt;&lt;16)\n  mov DWORD [rsp + 1*4], esi\n\n  mov rax, SYSCALL_WRITE\n  mov rdi, rdi\n  lea rsi, [rsp]\n  mov rdx, 2*4\n  syscall\n\n  cmp rax, 2*4\n  jnz die\n\n  add rsp, 16\n\n  pop rbp\n  ret\n\n; Read the X11 server reply.\n; @return The message code in al.\nx11_read_reply:\nstatic x11_read_reply:function\n  push rbp\n  mov rbp, rsp\n\n  sub rsp, 32\n\n  mov rax, SYSCALL_READ\n  mov rdi, rdi\n  lea rsi, [rsp]\n  mov rdx, 32\n  syscall\n\n  cmp rax, 1\n  jle die\n\n  mov al, BYTE [rsp]\n\n  add rsp, 32\n\n  pop rbp\n  ret\n\ndie:\n  mov rax, SYSCALL_EXIT\n  mov rdi, 1\n  syscall\n\n\n; Set a file descriptor in non-blocking mode.\n; @param rdi The file descriptor.\nset_fd_non_blocking:\nstatic set_fd_non_blocking:function\n  push rbp\n  mov rbp, rsp\n\n  %define F_GETFL 3\n  %define F_SETFL 4\n\n  %define O_NONBLOCK 2048\n\n  mov rax, SYSCALL_FCNTL\n  mov rdi, rdi \n  mov rsi, F_GETFL\n  mov rdx, 0\n  syscall\n\n  cmp rax, 0\n  jl die\n\n  ; `or` the current file status flag with O_NONBLOCK.\n  mov rdx, rax\n  or rdx, O_NONBLOCK\n\n  mov rax, SYSCALL_FCNTL\n  mov rdi, rdi \n  mov rsi, F_SETFL\n  mov rdx, rdx\n  syscall\n\n  cmp rax, 0\n  jl die\n\n  pop rbp\n  ret\n\n; Poll indefinitely messages from the X11 server with poll(2).\n; @param rdi The socket file descriptor.\n; @param esi The window id.\n; @param edx The gc id.\npoll_messages:\nstatic poll_messages:function\n  push rbp\n  mov rbp, rsp\n\n  sub rsp, 32\n\n  %define POLLIN 0x001\n  %define POLLPRI 0x002\n  %define POLLOUT 0x004\n  %define POLLERR  0x008\n  %define POLLHUP  0x010\n  %define POLLNVAL 0x020\n\n  mov DWORD [rsp + 0*4], edi\n  mov DWORD [rsp + 1*4], POLLIN\n\n  mov DWORD [rsp + 16], esi ; window id\n  mov DWORD [rsp + 20], edx ; gc id\n  mov BYTE [rsp + 24], 0 ; exposed? (boolean)\n\n  .loop:\n    mov rax, SYSCALL_POLL\n    lea rdi, [rsp]\n    mov rsi, 1\n    mov rdx, -1\n    syscall\n\n    cmp rax, 0\n    jle die\n\n    cmp DWORD [rsp + 2*4], POLLERR  \n    je die\n\n    cmp DWORD [rsp + 2*4], POLLHUP  \n    je die\n\n    mov rdi, [rsp + 0*4]\n    call x11_read_reply\n\n    %define X11_EVENT_EXPOSURE 0xc\n    cmp eax, X11_EVENT_EXPOSURE\n    jnz .received_other_event\n\n    .received_exposed_event:\n    mov BYTE [rsp + 24], 1 ; Mark as exposed.\n\n    .received_other_event:\n\n    cmp BYTE [rsp + 24], 1 ; exposed?\n    jnz .loop\n\n    .draw_text:\n      mov rdi, [rsp + 0*4] ; socket fd\n      lea rsi, [hello_world] ; string\n      mov edx, 13 ; length\n      mov ecx, [rsp + 16] ; window id\n      mov r8d, [rsp + 20] ; gc id\n      mov r9d, 100 ; x\n      shl r9d, 16\n      or r9d, 100 ; y\n      call x11_draw_text\n\n\n    jmp .loop\n\n\n  add rsp, 32\n  pop rbp\n  ret\n\n; Draw text in a X11 window with server-side text rendering.\n; @param rdi The socket file descriptor.\n; @param rsi The text string.\n; @param edx The text string length in bytes.\n; @param ecx The window id.\n; @param r8d The gc id.\n; @param r9d Packed x and y.\nx11_draw_text:\nstatic x11_draw_text:function\n  push rbp\n  mov rbp, rsp\n\n  sub rsp, 1024\n\n  mov DWORD [rsp + 1*4], ecx ; Store the window id directly in the packet data on the stack.\n  mov DWORD [rsp + 2*4], r8d ; Store the gc id directly in the packet data on the stack.\n  mov DWORD [rsp + 3*4], r9d ; Store x, y directly in the packet data on the stack.\n\n  mov r8d, edx ; Store the string length in r8 since edx will be overwritten next.\n  mov QWORD [rsp + 1024 - 8], rdi ; Store the socket file descriptor on the stack to free the register.\n\n  ; Compute padding and packet u32 count with division and modulo 4.\n  mov eax, edx ; Put dividend in eax.\n  mov ecx, 4 ; Put divisor in ecx.\n  cdq ; Sign extend.\n  idiv ecx ; Compute eax / ecx, and put the remainder (i.e. modulo) in edx.\n  ; LLVM optimizer magic: `(4-x)%4 == -x &amp; 3`, for some reason.\n  neg edx\n  and edx, 3\n  mov r9d, edx ; Store padding in r9.\n\n  mov eax, r8d \n  add eax, r9d\n  shr eax, 2 ; Compute: eax /= 4\n  add eax, 4 ; eax now contains the packet u32 count.\n\n\n  %define X11_OP_REQ_IMAGE_TEXT8 0x4c\n  mov DWORD [rsp + 0*4], r8d\n  shl DWORD [rsp + 0*4], 8\n  or DWORD [rsp + 0*4], X11_OP_REQ_IMAGE_TEXT8\n  mov ecx, eax\n  shl ecx, 16\n  or [rsp + 0*4], ecx\n\n  ; Copy the text string into the packet data on the stack.\n  mov rsi, rsi ; Source string in rsi.\n  lea rdi, [rsp + 4*4] ; Destination\n  cld ; Move forward\n  mov ecx, r8d ; String length.\n  rep movsb ; Copy.\n\n  mov rdx, rax ; packet u32 count\n  imul rdx, 4\n  mov rax, SYSCALL_WRITE\n  mov rdi, QWORD [rsp + 1024 - 8] ; fd\n  lea rsi, [rsp]\n  syscall\n\n  cmp rax, rdx\n  jnz die\n\n  add rsp, 1024\n\n  pop rbp\n  ret\n\n_start:\nglobal _start:function\n  call x11_connect_to_server\n  mov r15, rax ; Store the socket file descriptor in r15.\n\n  mov rdi, rax\n  call x11_send_handshake\n\n  mov r12d, eax ; Store the window root id in r12.\n\n  call x11_next_id\n  mov r13d, eax ; Store the gc_id in r13.\n\n  call x11_next_id\n  mov r14d, eax ; Store the font_id in r14.\n\n  mov rdi, r15\n  mov esi, r14d\n  call x11_open_font\n\n\n  mov rdi, r15\n  mov esi, r13d\n  mov edx, r12d\n  mov ecx, r14d\n  call x11_create_gc\n\n  call x11_next_id\n  \n  mov ebx, eax ; Store the window id in ebx.\n\n  mov rdi, r15 ; socket fd\n  mov esi, eax\n  mov edx, r12d\n  mov ecx, [root_visual_id]\n  mov r8d, 200 | (200 &lt;&lt; 16) ; x and y are 200\n  %define WINDOW_W 800\n  %define WINDOW_H 600\n  mov r9d, WINDOW_W | (WINDOW_H &lt;&lt; 16)\n  call x11_create_window\n\n  mov rdi, r15 ; socket fd\n  mov esi, ebx\n  call x11_map_window\n\n  mov rdi, r15 ; socket fd\n  call set_fd_non_blocking\n\n  mov rdi, r15 ; socket fd\n  mov esi, ebx ; window id\n  mov edx, r13d ; gc id\n  call poll_messages\n\n  ; The end.\n  mov rax, SYSCALL_EXIT\n  mov rdi, 0\n  syscall ",
titles:[
{
title:"What do we need?",
slug:"what-do-we-need",
offset:1212,
},
{
title:"X11 basics",
slug:"x11-basics",
offset:3385,
},
{
title:"Main in x64 assembly",
slug:"main-in-x64-assembly",
offset:4127,
},
{
title:"A stack primer",
slug:"a-stack-primer",
offset:8900,
},
{
title:"A small stack example",
slug:"a-small-stack-example",
offset:10174,
},
{
title:"Opening a socket",
slug:"opening-a-socket",
offset:17294,
},
{
title:"Connecting to the server",
slug:"connecting-to-the-server",
offset:19274,
},
{
title:"Sending data over the socket",
slug:"sending-data-over-the-socket",
offset:27289,
},
{
title:"Generating ids",
slug:"generating-ids",
offset:34832,
},
{
title:"Opening a font",
slug:"opening-a-font",
offset:35502,
},
{
title:"Creating a graphical context",
slug:"creating-a-graphical-context",
offset:36858,
},
{
title:"Creating the window",
slug:"creating-the-window",
offset:38422,
},
{
title:"Mapping the window",
slug:"mapping-the-window",
offset:40235,
},
{
title:"Polling for server messages",
slug:"polling-for-server-messages",
offset:40941,
},
{
title:"Drawing text",
slug:"drawing-text",
offset:43979,
},
{
title:"The end",
slug:"the-end",
offset:46952,
},
{
title:"Addendum: the full code",
slug:"addendum-the-full-code",
offset:47797,
},
],
},
{
html_file_name:"kahns_algorithm.html",
title:"Cycle detection in graphs does not have to be hard: A lesser known, simple way with Kahn\'s algorithm",
text:"Introduction Graphs are everywhere in Software Engineering, or so we are told by Computer Science teachers and interviewers. But sometimes, they do show up in real problems. Not too long ago, I was tasked to create a Web API to create and update a company\'s hierarchy of employee, and display that on a web page. Basically, who reports to whom. In the simple case, it\'s a tree, when an employee reports to exactly one manager. Here\'s the tree of employees in an organization. An employee reports to a manager, and this forms a tree. The root is the CEO since they report to no one and so they have no outgoing edge. An arrow (or \'edge\') between two nodes means &lt;source&gt; reports to &lt;destination&gt; , for example: Jane the CFO reports to Ellen the CEO . But here is the twist: our API receives a list of employee -&gt; manager links, in any order: Jane -&gt; Ellen\nAngela -&gt; Ellen\nZoe -&gt; Jane\nZoe -&gt; Angela\nBella -&gt; Angela\nMiranda -&gt; Angela It opens the door to various invalid inputs: links that form a graph (an employee has multiple managers), multiple roots (e.g. multiple CEOs) or cycles. We have to detect those and reject them, such as this one: The database So how do we store all of those people in the database? CREATE TABLE IF NOT EXISTS people(name TEXT NOT NULL UNIQUE, manager BIGINT REFERENCES people) Each employee has an optional reference to a manager. This is not a novel idea, actually this is one of the examples in the official SQLite documentation . For example, to save Ellen, CEO inside the database, we do: INSERT INTO people VALUES(\'Ellen, CEO\', NULL) And to save Jane, CFO in the database: INSERT INTO people VALUES(\'Jane, CFO\', 1) Where Ellen, CEO , Jane\'s boss, which we just inserted before, has the id 1 . Immediately, we notice that to insert an employee, their manager needs to already by in the database, by virtue of the self-referential foreign key manager BIGINT REFERENCES people . So we need a way to sort the big list of employee -&gt; manager links (or \'edges\' in graph parlance), to insert them in the right order. First we insert the CEO, who reports to no one. Then we insert the employees directly reporting to the CEO. Then the employees reporting to those. Etc. And that\'s called a topological sort. A big benefit is that we hit three birds with one stone: We detect cycles We have the nodes in a convenient order to insert them in the database Since the algorithm for the topological sort takes as input an adjacency matrix (more on this later), we can easily detect the invalid case of a node having more than one outgoing edge (i.e. more than one manager, i.e. multiple roots). From now one, I will use the graph of employees (where Zoe has two managers) as example since that\'s a possible input to our API and we need to detect this case. Topological sort From Wikipedia: A topological sort or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertex v, u comes before v in the ordering. For instance, the vertices of the graph may represent tasks to be performed, and the edges may represent constraints that one task must be performed before another; in this application, a topological ordering is just a valid sequence for the tasks That\'s a mouthful but it\'s not too hard. A useful command line utility that\'s already on your (Unix) machine is tsort , which takes a list of edges as input, and outputs a topological sort. Here is the input in a text file ( people.txt ): Jane Ellen\nAngela Ellen\nZoe Jane\nZoe Angela\nBella Angela\nMiranda Angela tsort uses a simple way of defining each edge A -&gt; B on its own line with the syntax: A B . The order of the lines does not matter. And here\'s the tsort output: $ tsort &lt; people.txt\nBella\nMiranda\nZoe\nAngela\nJane\nEllen The first 3 elements are the ones with no incoming edge, the Software Engineers, since no one reports to them. Then come their respective managers, Angela and Jane. Finally comes their manager, Ellen . So to insert all those people in our people SQL table, we go through that list in reverse order: We can first insert Ellen , then Jane , etc, until we finally insert Bella . Also, tsort detects cycles, for example if we add the line: Ellen Zoe at the end of people.txt , we get: $ tsort &lt; people.txt\nBella\nMiranda\ntsort: -: input contains a loop:\ntsort: Jane\ntsort: Ellen\ntsort: Zoe\nJane\ntsort: -: input contains a loop:\ntsort: Angela\ntsort: Ellen\ntsort: Zoe\nAngela\nEllen\nZoe So, how can we implement something like tsort for our problem at hand? That\'s where Kahn\'s algorithm comes in to do exactly that: find cycles in the graph and output a topological sort. Note that that\'s not the only solution and there are ways to detect cycles without creating a topological sort, but this algorithm seems relatively unknown and does not come up often on the Internet, so let\'s discover how it works and implement it. I promise, it\'s not complex. How to store the graph in memory There are many ways to do so, and Kahn\'s algorithm does not dictate which one to use. We\'ll use an adjacency matrix , because it\'s simple conceptually, maps well to Kahn\'s algorithm, and can be optimized if needed. It\'s just a 2D square table of size n x n (where n is the number of nodes), where the cell at row i and column j is 1 if there is an edge from the node i to the node j , and otherwise, 0 . The order of the nodes is arbitrary, I\'ll use the alphabetical order because again, it\'s simple to do: Angela\nBella\nEllen\nJane\nMiranda\nZoe Here, Angela is the node 0 and Zoe is the node 5 . Since there is an edge from Zoe to Angela , i.e. from the node 5 to the node 0 , the cell at the position (5, 0) is set to 1 . The full adjacency matrix for the employee graph in the example above looks like: Angela Bella Ellen Jane Miranda Zoe Angela 0 0 1 0 0 0 Bella 1 0 0 0 0 0 Ellen 0 0 0 0 0 0 Jane 0 0 1 0 0 0 Miranda 1 0 0 0 0 0 Zoe 1 0 0 1 0 0 The way to read this table is: For a given row, all the 1 \'s indicate outgoing edges For a given column, all the 1 \'s indicate incoming edges If there is a 1 on the diagonal, it means there is an edge going out of a node and going to the same node. There are a lot of zeroes in this table. Some may think this is horribly inefficient, which it is, but it really depends on number of nodes, i.e. the number of employees in the organization.\nBut note that this adjacency matrix is a concept, it shows what information is present, but not how it is stored. For this article, we will store it the naive way, in a 2D array. Here are two optimization ideas I considered but have not had time to experiment with: Make this a bitarray. We are already only storing zeroes and ones, so it maps perfectly to this format. Since there are a ton of zeroes (in the valid case, a regular employee\'s row only has one 1 and the CEO\'s row is only zeroes), it is very compressible. An easy way would be to use run-length encoding, meaning, instead of 0 0 0 0 , we just store the number of times the number occurs: 4 0 . Easy to implement, easy to understand. A row compresses to just a few bytes. And this size would be constant, whatever the size of the organization (i.e. number of employees) is. Wikipedia lists others if you are interested, it\'s a well-known problem. Alright, now that we know how our graph is represented, on to the algorithm. Kahn\'s algorithm Kahn\'s algorithm keeps track of nodes with no incoming edge, and mutates the graph (in our case the adjacency matrix), by removing one edge at a time, until there are no more edges, and builds a list of nodes in the right order, which is the output. Here\'s the pseudo-code: L \u{2190} Empty list that will contain the sorted elements\nS \u{2190} Set of all nodes with no incoming edge\n\nwhile S is not empty do\n    remove a node n from S\n    add n to L\n    for each node m with an edge e from n to m do\n        remove edge e from the graph\n        if m has no other incoming edges then\n            insert m into S\n\nif graph has edges then\n    return error   (graph has at least one cycle)\nelse \n    return L   (a topologically sorted order) And in plain English: Line 1: The result of this algorithm is the list of nodes in the desired order (topological). It starts empty, and we add nodes one-by one during the algorithm. We can simply use an array in our implementation. Line 2: We first collect all nodes with no incoming edge. In terms of adjacency matrix, it means picking columns with only zeroes. The algorithm calls it a set, but we are free in our implementation to use whatever data structure we see fit. It just means a given node appears at most once in it. In our example, this set is: [Zoe, Bella, Miranda] . During the algorithm course, we will add further nodes to this set. Note that this is a working set, not the final result. Also, the order does not matter. Line 4: Self-explanatory, we continue until the working set is empty and there is no more work to do. Line 5: We first pick a node with no incoming edge (it does not matter which one). For example, Zoe , and remove it from S . S is now: [Bella, Miranda] . Line 6: We add this node to the list of topologically sorted nodes, L . It now is: [Zoe] . Line 7: We then inspect each node that Zoe has an edge to. That means Jane and Angela . In terms of adjacency matrix, we simply read Zoe\'s row, and inspect cells with a 1 in it. Line 8: We remove such an edge, for example, Zoe -&gt; Jane . In terms of adjacency matrix, it means setting the cell on the row Zoe and column Jane to 0 . At this point, the graph looks like this: Line 9: If Jane does not have another incoming edge, we add it to the set of all nodes with no incoming edge. That\'s the case here, so S now looks like: [Bella, Miranda, Jane] . We now loop to line 7 and handle the node Angela since Jane is taken care of. Line 7-10: We are now handling the node Angela . We remove the edge Zoe -&gt; Angela . We check whether the node Angela has incoming edges. It does, so we do not add it to S . The graph is now: We are now done with the line 7 for loop, so go back to line 5 and pick this time Bella . And so on. The graph would now, to the algorithm, look like: And here are the next steps in images: Line 12-15: Once the loop at line 4 is finished, we inspect our graph. If there are no more edges, we are done. If there is still an edge, it means there was a cycle in the graph, and we return an error.\nNote that this algorithm is not capable by itself to point out which cycle there was exactly, only that there was one. That\'s because we mutated the graph by removing edges. If this information was important, we could keep track of which edges we removed in order, and re-add them back, or perhaps apply the algorithm to a copy of the graph (the adjacency matrix is trivial to clone). This algorithm is loose concerning the order of some operations, for example, picking a node with no incoming edge, or in which order the nodes in S are stored. That gives room for an implementation to use certain data structures or orders that are faster, but in some cases we want the order to be always the same to solve ties in the stable way and to be reproducible. In order to do that, we simply use the alphabetical order. So in our example above, at line 5, we picked Zoe out of [Zoe, Bella, Miranda] . Using this method, we would keep the working set S sorted alphabetically and pick Bella out of [Bella, Miranda, Zoe] . Implementation I implemented this at the time in Go, but I will use for this article the lingua franca of the 2010s, JavaScript. I don\'t write JavaScript these days, I stopped many years ago, so apologies in advance if I am not using all the bells and whistles of \'Modern JavaScript\', or if the code is not quite idiomatic. First, we define our adjacency matrix and the list of nodes. This is the naive format. We would get the nodes and edges in some format, for example JSON, in the API, and build the adjacency matrix, which is trivial. Let\'s take the very first example, the (valid) tree  of employees: const adjacencyMatrix = [\n  [0, 0, 1, 0, 0, 0],\n  [1, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0],\n  [1, 0, 0, 0, 0, 0],\n  [0, 0, 0, 1, 0, 0],\n];\n\nconst nodes = [&quot;Angela&quot;, &quot;Bella&quot;, &quot;Ellen&quot;, &quot;Jane&quot;, &quot;Miranda&quot;, &quot;Zoe&quot;]; Helpers We need a helper function to check if a node has no incoming edge (line 9 in the algorithm): function hasNodeNoIncomingEdge(adjacencyMatrix, nodeIndex) {\n  const column = nodeIndex;\n\n  for (let row = 0; row &lt; adjacencyMatrix.length; row += 1) {\n    const cell = adjacencyMatrix[row][column];\n    if (cell != 0) {\n      return false;\n    }\n  }\n\n  return true;\n} Then, using this helper, we can define a second helper to initially collect all the nodes with no incoming edge (line 2 in the algorithm): function getNodesWithNoIncomingEdge(adjacencyMatrix, nodes) {\n  return nodes.filter((_, i) =&gt; hasNodeNoIncomingEdge(adjacencyMatrix, i));\n} We can try it: console.log(getNodesWithNoIncomingEdge(adjacencyMatrix, nodes)); And it outputs: [ \'Bella\', \'Miranda\', \'Zoe\' ] We need one final helper, to determine if the graph has edges (line 12), which is straightforward: function graphHasEdges(adjacencyMatrix) {\n  for (let row = 0; row &lt; adjacencyMatrix.length; row += 1) {\n    for (let column = 0; column &lt; adjacencyMatrix.length; column += 1) {\n      if (adjacencyMatrix[row][column] == 1) return true;\n    }\n  }\n\n  return false;\n} The algorithm We are finally ready to implement the algorithm. It\'s a straightforward, line by line, translation of the pseudo-code: function topologicalSort(adjacencyMatrix) {\n  const L = [];\n  const S = getNodesWithNoIncomingEdge(adjacencyMatrix, nodes);\n\n  while (S.length &gt; 0) {\n    const node = S.pop();\n    L.push(node);\n    const nodeIndex = nodes.indexOf(node);\n\n    for (let mIndex = 0; mIndex &lt; nodes.length; mIndex += 1) {\n      const hasEdgeFromNtoM = adjacencyMatrix[nodeIndex][mIndex];\n      if (!hasEdgeFromNtoM) continue;\n\n      adjacencyMatrix[nodeIndex][mIndex] = 0;\n\n      if (hasNodeNoIncomingEdge(adjacencyMatrix, mIndex)) {\n        const m = nodes[mIndex];\n        S.push(m);\n      }\n    }\n  }\n\n  if (graphHasEdges(adjacencyMatrix)) {\n    throw new Error(&quot;Graph has at least one cycle&quot;);\n  }\n\n  return L;\n} Let\'s try it: console.log(topologicalSort(adjacencyMatrix, nodes)); We get: [ \'Zoe\', \'Jane\', \'Miranda\', \'Bella\', \'Angela\', \'Ellen\' ] Interestingly, it is not the same order as tsort , but it is indeed a valid topological ordering. That\'s because there are ties between some nodes and we do not resolve those ties the exact same way tsort does. But in our specific case, we just want a valid insertion order in the database, and so this is enough. Inserting entries in the database Now, we can produce the SQL code to insert our entries. We operate on a clone of the adjacency matrix for convenience because we later need to know what is the outgoing edge for a given node. We handle the special case of the root first, which is the last element, and then we go through the topologically sorted list of employees in reverse order, and insert each one. We use a one liner to get the manager id by name when inserting to avoid many round trips to the database: const employeesTopologicallySorted = topologicalSort(structuredClone(adjacencyMatrix), nodes)\n\nconst root = employeesTopologicallySorted[employeesTopologicallySorted.length - 1];\nconsole.log(`INSERT INTO people VALUES(&quot;${root}&quot;, NULL)`);\n\nfor (let i = employeesTopologicallySorted.length - 2; i &gt;= 0; i -= 1) {\n  const employee = employeesTopologicallySorted[i];\n  const employeeIndex = nodes.indexOf(employee);\n\n  const managerIndex = adjacencyMatrix[employeeIndex].indexOf(1);\n  const manager = nodes[managerIndex];\n  console.log(\n    `INSERT INTO people SELECT &quot;${employee}&quot;, rowid FROM people WHERE name = &quot;${manager}&quot; LIMIT 1;`,\n  );\n} Which outputs: INSERT INTO people VALUES(&quot;Ellen&quot;, NULL);\nINSERT INTO people SELECT &quot;Angela&quot;, rowid FROM people WHERE name = &quot;Ellen&quot; LIMIT 1;\nINSERT INTO people SELECT &quot;Bella&quot;, rowid FROM people WHERE name = &quot;Angela&quot; LIMIT 1;\nINSERT INTO people SELECT &quot;Miranda&quot;, rowid FROM people WHERE name = &quot;Angela&quot; LIMIT 1;\nINSERT INTO people SELECT &quot;Jane&quot;, rowid FROM people WHERE name = &quot;Ellen&quot; LIMIT 1;\nINSERT INTO people SELECT &quot;Zoe&quot;, rowid FROM people WHERE name = &quot;Jane&quot; LIMIT 1; Detecting cycles As we said earlier, we get that for free, so let\'s check our implementation against this invalid example: We add the edge Ellen -&gt; Zoe to create a cycle: const adjacencyMatrix = [\n  [0, 0, 1, 0, 0, 0],\n  [1, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 1], // =&gt; We change the last element of this row (Ellen\'s row, Zoe\'s column) from 0 to 1.\n  [0, 0, 1, 0, 0, 0],\n  [1, 0, 0, 0, 0, 0],\n  [0, 0, 0, 1, 0, 0],\n];\n\nconst nodes = [&quot;Angela&quot;, &quot;Bella&quot;, &quot;Ellen&quot;, &quot;Jane&quot;, &quot;Miranda&quot;, &quot;Zoe&quot;];\n\nconst employeesTopologicallySorted = topologicalSort(structuredClone(adjacencyMatrix), nodes); And we get an error as expected: /home/pg/my-code/blog/kahns_algorithm.js:63\n    throw new Error(&quot;Graph has at least one cycle&quot;);\n    ^\n\nError: Graph has at least one cycle Detecting multiple roots One thing that topological sorting does not do for us is to detect the case of multiple roots in the graph, for example: To do this, we simply scan the adjacency matrix and verify that there is only one row with only zeroes, that is, only one node that has no outgoing edges: function hasMultipleRoots(adjacencyMatrix) {\n  let countOfRowsWithOnlyZeroes = 0;\n\n  for (let row = 0; row &lt; adjacencyMatrix.length; row += 1) {\n    let rowHasOnlyZeroes = true;\n    for (let column = 0; column &lt; adjacencyMatrix.length; column += 1) {\n      if (adjacencyMatrix[row][column] != 0) {\n        rowHasOnlyZeroes = false;\n        break;\n      }\n    }\n    if (rowHasOnlyZeroes) countOfRowsWithOnlyZeroes += 1;\n  }\n\n  return countOfRowsWithOnlyZeroes &gt; 1;\n} Let\'s try it with our invalid example from above: const adjacencyMatrix = [\n  [0, 0, 1, 0, 0, 0, 0],\n  [1, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0],\n  [1, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 1, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0],\n];\n\nconst nodes = [&quot;Angela&quot;, &quot;Bella&quot;, &quot;Ellen&quot;, &quot;Jane&quot;, &quot;Miranda&quot;, &quot;Zoe&quot;, &quot;Kelly&quot;];\n\n\nconsole.log(hasMultipleRoots(adjacencyMatrix)); And we get: true . With our previous (valid) example, we get: false . Playing with the database We can query each employee along with their manager name so: SELECT a.name as employee_name, COALESCE(b.name, \'\') as manager_name FROM people a LEFT JOIN people b ON a.manager = b.rowid; To query the manager (N+1) and the manager\'s manager (N+2) of an employee: SELECT COALESCE(n_plus_1.name, \'\'), COALESCE(n_plus_2.name, \'\')\nFROM people employee\nLEFT JOIN people n_plus_1 ON employee.manager = n_plus_1.rowid\nLEFT JOIN people n_plus_2 ON n_plus_1.manager = n_plus_2.rowid\nWHERE employee.name = ? We can also do this with hairy recursive Common Table Expression (CTE) but I\'ll leave that to the reader. Closing thoughts Graphs and algorithms operating on them do not have to be complicated. Using an adjacency matrix and Kahn\'s algorithm, we can achieve a lot with little and it remains simple. There are many ways to optimize the code in this article; the point was not to write the most efficient code, but to showcase in the clearest, simplest way possible to detect cycles and store a graph/tree in memory and in a database. If you want to play with the code here and try to make it faster, go at it! Addendum: the full code The full code const adjacencyMatrix = [\n  [0, 0, 1, 0, 0, 0],\n  [1, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0],\n  [1, 0, 0, 0, 0, 0],\n  [0, 0, 0, 1, 0, 0],\n];\n\nconst nodes = [&quot;Angela&quot;, &quot;Bella&quot;, &quot;Ellen&quot;, &quot;Jane&quot;, &quot;Miranda&quot;, &quot;Zoe&quot;];\n\nfunction hasNodeNoIncomingEdge(adjacencyMatrix, nodeIndex) {\n  const column = nodeIndex;\n\n  for (let row = 0; row &lt; adjacencyMatrix.length; row += 1) {\n    const cell = adjacencyMatrix[row][column];\n\n    if (cell != 0) {\n      return false;\n    }\n  }\n\n  return true;\n}\n\nfunction getNodesWithNoIncomingEdge(adjacencyMatrix, nodes) {\n  return nodes.filter((_, i) =&gt; hasNodeNoIncomingEdge(adjacencyMatrix, i));\n}\n\nfunction graphHasEdges(adjacencyMatrix) {\n  for (let row = 0; row &lt; adjacencyMatrix.length; row += 1) {\n    for (let column = 0; column &lt; adjacencyMatrix.length; column += 1) {\n      if (adjacencyMatrix[row][column] == 1) return true;\n    }\n  }\n\n  return false;\n}\n\nfunction topologicalSort(adjacencyMatrix) {\n  const L = [];\n  const S = getNodesWithNoIncomingEdge(adjacencyMatrix, nodes);\n\n  while (S.length &gt; 0) {\n    const node = S.pop();\n    L.push(node);\n    const nodeIndex = nodes.indexOf(node);\n\n    for (let mIndex = 0; mIndex &lt; nodes.length; mIndex += 1) {\n      const hasEdgeFromNtoM = adjacencyMatrix[nodeIndex][mIndex];\n      if (!hasEdgeFromNtoM) continue;\n\n      adjacencyMatrix[nodeIndex][mIndex] = 0;\n\n      if (hasNodeNoIncomingEdge(adjacencyMatrix, mIndex)) {\n        const m = nodes[mIndex];\n        S.push(m);\n      }\n    }\n  }\n\n  if (graphHasEdges(adjacencyMatrix)) {\n    throw new Error(&quot;Graph has at least one cycle&quot;);\n  }\n\n  return L;\n}\n\nfunction hasMultipleRoots(adjacencyMatrix) {\n  let countOfRowsWithOnlyZeroes = 0;\n\n  for (let row = 0; row &lt; adjacencyMatrix.length; row += 1) {\n    let rowHasOnlyZeroes = true;\n    for (let column = 0; column &lt; adjacencyMatrix.length; column += 1) {\n      if (adjacencyMatrix[row][column] != 0) {\n        rowHasOnlyZeroes = false;\n        break;\n      }\n    }\n    if (rowHasOnlyZeroes) countOfRowsWithOnlyZeroes += 1;\n  }\n\n  return countOfRowsWithOnlyZeroes &gt; 1;\n}\n\nconsole.log(hasMultipleRoots(adjacencyMatrix));\nconst employeesTopologicallySorted = topologicalSort(structuredClone(adjacencyMatrix), nodes);\nconsole.log(employeesTopologicallySorted);\n\nconst root = employeesTopologicallySorted[employeesTopologicallySorted.length - 1];\nconsole.log(`INSERT INTO people VALUES(&quot;${root}&quot;, NULL)`);\n\nfor (let i = employeesTopologicallySorted.length - 2; i &gt;= 0; i -= 1) {\n  const employee = employeesTopologicallySorted[i];\n  const employeeIndex = nodes.indexOf(employee);\n\n  const managerIndex = adjacencyMatrix[employeeIndex].indexOf(1);\n  const manager = nodes[managerIndex];\n  console.log(\n    `INSERT INTO people SELECT &quot;${employee}&quot;, rowid FROM people WHERE name = &quot;${manager}&quot; LIMIT 1;`,\n  );\n} ",
titles:[
{
title:"Introduction",
slug:"introduction",
offset:0,
},
{
title:"The database",
slug:"the-database",
offset:1176,
},
{
title:"Topological sort",
slug:"topological-sort",
offset:2814,
},
{
title:"How to store the graph in memory",
slug:"how-to-store-the-graph-in-memory",
offset:4973,
},
{
title:"Kahn\'s algorithm",
slug:"kahn-s-algorithm",
offset:7382,
},
{
title:"Implementation",
slug:"implementation",
offset:11449,
},
{
title:"Helpers",
slug:"helpers",
offset:12348,
},
{
title:"The algorithm",
slug:"the-algorithm",
offset:13497,
},
{
title:"Inserting entries in the database",
slug:"inserting-entries-in-the-database",
offset:14789,
},
{
title:"Detecting cycles",
slug:"detecting-cycles",
offset:16557,
},
{
title:"Detecting multiple roots",
slug:"detecting-multiple-roots",
offset:17391,
},
{
title:"Playing with the database",
slug:"playing-with-the-database",
offset:18691,
},
{
title:"Closing thoughts",
slug:"closing-thoughts",
offset:19320,
},
{
title:"Addendum: the full code",
slug:"addendum-the-full-code",
offset:19822,
},
],
},
{
html_file_name:"advent_of_code_2018_5_revisited.html",
title:"Optimizing an Advent of Code solution in assembly",
text:"A few days ago I was tweaking the appearance of this blog and I stumbled upon my first article which is about solving a simple problem from Advent of Code. Here it is again: We have a string looking like this: AabcdZZqQ which represents a chain of\nchemical units. Adjacent units of the same type (i.e letter) and opposite\npolarity (i.e casing) react together and disappear.\nIt means we want to remove adjacent characters which are the same letter and have opposite casing, e.g Aa and qQ disappear while bc and ZZ remain. Once we are finished, we have: bcdZZ . The final output is the number of characters in the final string, i.e, 5 . Immediately, I thought I could do better than my past self: In the Lisp solution, there are lots of allocations and the code is not straightforward. In the Lisp solution, we use multiple external dependencies, which usually turn out to be problematic in the long run. In the C solution, there is no allocation apart from the input but we do a lot of unnecessary work. In the C solution, we use abs(x) == 32 which we could avoid by doing x*x == 32*32 . This coincided with me listening to an interview from the VLC developers saying they wrote hundred of thousand of lines of (multi platform!) Assembly code by hand in their new AV1 decoder. I thought that was intriguing, who still writes assembly by hand in 2023? Well these guys are no idiots so I should try it as well. The new solution I came up with a new algorithm, which on paper does less work. It\'s one linear pass on the input, and does not allocate. Since the result we care about is the number of remaining characters, we simply keep track of the count as we sift through the input. We maintain two pointers, current and next , which we compare to decide whether we should merge the characters they point to. \'Merging\' means setting the two characters to 0 (it\'s basically a tombstone) and lower the count. next is always incremented by one in each loop iteration, that\'s the easy one. current is always pointing to a character before current , but not always directly adjacent, because there may be tombstones, i.e. zeroes, in-between. In pseudo-code: remaining_count = len(input)\nend = input + len(input)\ncurrent = &amp;input[0]\nnext = &amp;input[1]\n\nwhile next != end:\n    diff = *next - *current\n\n    if diff*diff == 32*32:\n      *current = 0\n      *next = 0\n      remaining_count -= 2\n\n      current -= 1\n      while current == 0:\n        current -= 1\n      endwhile\n    else:\n      current = next\n    endif\n\n next += 1\n    \nendwhile\n\nprint(remaining_count) The easy case is when there is no need to merge: current simply becomes next (and next is incremented at the end of the loop iteration). The \'hard\' case is merging: we set the two tombstones, lower the count, and now we are in a pickle: current needs to go backwards, but we do not know to where. There might be an arbitrary number of zeroes preceding the character current points to: the data on the left of next is sparse, the data on the right of next is not. [...] 0 0 A 0 0 0 0 B 0 0 0 0 C D E F [...]\n          ^         ^         ^\n          |         |         |\n          target    |         |\n                    current   |\n                              next So we have to do a backwards search to find the first non zero character.\nWe could memoize this location, but that would basically come down to the Scheme solution, having an output array of the same size as the input. Astute readers might have noticed a potential issue with the backwards search: We may underflow the input and go out of bounds! To avoid that, we could clamp current , but the branch misprediction is costly (an earlier implementation of mine did this and that was almost twice as slow!), and we can simplify the code as well as improve the performance by simply prefixing the input with a non-zero value that has no chance of being merged with the rest of the input, say, 1 . Let\'s implement it in x86_64 assembly! The x86_64 implementation For a gentle introduction to x64 assembly, go read an earlier article of mine. BITS 64\nCPU X64\n\n%define SYSCALL_EXIT 60\n%define SYSCALL_WRITE 1\n\nsection .data\n\n\nprefix: db 1\ninput: db &quot;xPGgpXlvVLLP...&quot; ; Truncated for readability\nstatic input:data\n\n%define input_len 50000\n\nsection .text\n\nexit:\nstatic exit:function\n  mov rax, SYSCALL_EXIT\n  mov rdi, 0\n  syscall\n\nsolve:\nstatic solve:function\n  push rbp\n  mov rbp, rsp\n\n  ; TODO\n\n  pop rbp\n  ret\n\nglobal _start\n_start:\n  call solve\n  call exit We circumvent reading the input from a file and embed it directly in our code, something many people having their hand at Advent of Code challenges do. It is in the data section and not in the .rodata section because we are going to mutate it in place with the tombstones. We also have to exit the program by ourselves since there is no libc, and we create a solve function which will have our logic. We compile and run it so (on Linux, other OSes will be similar but slightly different): $ nasm -f elf64 -g aoc2018_5.asm &amp;&amp; ld.lld aoc2018_5.o -static -g -o aoc2018_5\n$ ./aoc2020_5 Which outputs nothing for now, of course. We will need to print the remaining_count to stdout at the end so we add a function to do so: write_int_to_stdout:\nstatic write_int:function\n  push rbp\n  mov rbp, rsp\n\n  sub rsp, 32\n\n  %define ARG0 rdi\n  %define N rax\n  %define BUF rsi\n  %define BUF_LEN r10\n  %define BUF_END r9\n\n  lea BUF, [rsp+32]\n  mov BUF_LEN, 0\n  lea BUF_END, [rsp]\n  mov N, ARG0\n\n  .loop:\n    mov rcx, 10 ; Divisor.\n    mov rdx, 0 ; Reset rem.\n    div rcx ; rax /= rcx\n\n    add rdx, \'0\' ; Convert to ascii.\n\n    ; *(end--) = rem\n    dec BUF_END\n    mov [BUF_END], dl\n    \n    inc BUF_LEN\n\n    cmp N, 0\n    jnz .loop\n\n  mov rax, SYSCALL_WRITE\n  mov rdi, 1\n  mov rsi, BUF_END\n  mov rdx, BUF_LEN\n  syscall\n\n\n  %undef ARG0\n  %undef N\n  %undef BUF\n  %undef BUF_LEN\n  %undef BUF_END\n\n  add rsp, 32\n  pop rbp\n  ret I am trying a new style of writing assembly which I saw notably the Go developers use: Since the biggest problem is that we have no named variables, we leverage the macro system from nasm to name the registers we work with in a human readable fashion. Our solve function can now return a dummy number and we can print it out by passing the return value (in rax ) of solve as the first argument (in rdi ) of write_int_to_stdout : solve:\nstatic solve:function\n  push rbp\n  mov rbp, rsp\n\n  mov rax, 123\n\n  pop rbp\n  ret\n\nglobal _start\n_start:\n  call solve\n\n  mov rdi, rax\n  call write_int_to_stdout\n\n  call exit We now can focus on implementing solve . It\'s a one to one translation of the pseudo-code. We just have to judiciously choose which registers to use based on the x64 System V ABI to avoid bookkeeping work of saving and restoring registers. For example, we use rax to store remaining_count since this will be the return value, so that we do not have to do anything special at the end of the function. Another pitfall to be aware of is that since we are dealing with ASCII characters, we could use the 8 bit form of the registers. However, some opcode such as imul are not usable with these. We have to use the 16, 32, or 64 bit form. This does not compile: mov dl, 2\n  imul dl, dl But this does: mov dx, 2\n  imul dx, dx And so we need to zero extend the 16 bit registers in some locations with movzx to fill the remainder of the register with zeroes. Forgetting to do so will lead to very nasty, obscure bugs. Finally, we always write loops in the form of do { ... } while(condition) . This is easier in our case; we assume (and know) the input is not empty, for example. Here we go. Note that this function does not need any stack space, since we modify the input in place, and the standard registers are enough to store the few values we keep track of: solve:\nstatic solve:function\n  push rbp\n  mov rbp, rsp\n\n  %define INPUT_LEN r10\n  %define CURRENT r9\n  %define NEXT r11\n  %define REMAINING_COUNT rax\n  %define END r8\n\n  lea CURRENT, [input] \n  lea NEXT, [input + 1] \n  mov INPUT_LEN, input_len\n  mov REMAINING_COUNT, INPUT_LEN\n  lea END, [input]\n  add END, INPUT_LEN\n  \n\n.loop:\n  movzx dx, BYTE [CURRENT]\n  movzx cx, BYTE [NEXT]\n  sub dx, cx\n  imul dx, dx\n\n  mov rcx, 32*32\n\n  cmp rdx, rcx\n  jnz .else\n  .then:\n    mov BYTE [CURRENT], 0\n    mov BYTE [NEXT], 0\n\n    sub REMAINING_COUNT, 2\n\n    .reverse_search:\n    dec CURRENT\n    mov dl, [CURRENT]\n    cmp dl, 0\n    jz .reverse_search\n\n\n    jmp .endif\n  .else:\n    mov CURRENT, NEXT\n  .endif:\n\n  inc NEXT\n  cmp NEXT, END\n  jl .loop\n\n  %undef INPUT_LEN\n  %undef CURRENT\n  %undef NEXT\n  %undef REMAINING_COUNT\n  %undef END\n\n\n  pop rbp\n  ret Benchmarking So, did it work? Is it fast? Let\'s compare the old C solution (also embedding the input data for a fair comparison) with our new Assembly one: $ clang -Ofast -g3 -march=native aoc2018_5.c -o aoc2018_5-c\n$ hyperfine --warmup 3 --shell=none ./aoc2018_5 ./aoc2018_5-c\n\nBenchmark 1: ./aoc2018_5\n  Time (mean \u{b1} \u{3c3}):       2.7 ms \u{b1}   1.0 ms    [User: 2.4 ms, System: 0.1 ms]\n  Range (min \u{2026} max):     1.2 ms \u{2026}   4.6 ms    1174 runs\n \nBenchmark 2: ./aoc2018_5-c\n  Time (mean \u{b1} \u{3c3}):       5.3 ms \u{b1}   1.4 ms    [User: 5.0 ms, System: 0.2 ms]\n  Range (min \u{2026} max):     4.1 ms \u{2026}  10.4 ms    442 runs\n \nSummary\n  \'./aoc2018_5\' ran\n    1.95 \u{b1} 0.86 times faster than \'./aoc2018_5-c\' Yes, indeed, almost twice as fast! Learnings Assembly can absolutely be written by hand, although with (much) more effort and a harder time troubleshooting what goes wrong. Some tools (profilers, reverse-engineering tools) expect that the assembly they consume came from a C source file and will be confused when it\'s not the case. There is no need to reach for esoteric, vendor specific instructions (such as SIMD or Intel string opcodes) to go fast and even beat a C implementation that does a bit more work than necessary. Doing less work is the most important thing when optimizing. At the same time, doing a bit of work instead of memoizing can be fast(er), even though it seems counter-intuitive, due to memory latency and cache misses. Having a clear idea in pseudo-code of the solution will simplify the implementation in any language. Naive assembly is very fast (your CPU is crazy fast when you do not abuse it with badly written interpreted languages, seriously!), but a good compiler will emit better assembly than you if you write C or similar. Unless you are an expert like the VLC guys. Appendix: The full code The old C implementation The full code #include &lt;stdint.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n\nstatic char input[] =\n    &quot;xPGgpXlvVLLPplNSiIsWwaAEeMmJjyYfFWfFwpPqcCYvVySsAUuaCcDdHlLSshxKkMmXQnNKkr&quot;\n    &quot;RptBbTqQEevKkkKVsSmMmMvqFfGFSsfZzgQTtFLlfsSFBTtbfbBiIAHhzZaVNbBOonsfFSBYGg&quot;\n    &quot;RryvaAVTtbFfqaAEetqQUubBTyYAWwzZeENRrgGaAfFnNnpPJjulLEeUaQqnNJjQtTPTaqQAoO&quot;\n    &quot;EerRtnVaALlhDdPpHvvVrRsFVvfsMcCvkPZzpKbBOofZzyYFzZsAjJavGgkKRrVwWSYygFfGLr&quot;\n    &quot;RLlgGlVaAXZzHuULDpPdoOlhgGVoOKkVvaAhOoHIiBNnxsSXbBbvVFfvtTUvVlaALPptTGguPp&quot;\n    &quot;jJFmRrMqjJOOqQZzooQRVvrKkpVvPOopPKKbBkYykYQqPpPpoKkOihHIECcJjeyvsSVpbBPGKk&quot;\n    &quot;gfVvHhiIvGgViXxlLdDIgGKMCncCNcmfFDDdSsXxqQtTaAdenNdDVvPuUpVvtTZzEQqyoOvVTx&quot;\n    &quot;XtbBnDdcCZWwrRzNaeEAlDZzeEdJlelLEzZhHhOoHYyKkLjWwNnLAjJaiHhIvhHHhEhHeSsSKk&quot;\n    &quot;sXxVvzZEevVzpliIRPprLPcOHhsSoRSsTtrlLCiISsJjMmZvnNoOVclLCVzZMmKhHkqWufFmMS&quot;\n    &quot;mMkKsiFfoRrOIUozZyYNnSNnqQsTrRtSHhNniIbGgCcnNBxXSFYyWwfgFRrfaROorAsxXSGsSK&quot;\n    &quot;BbkspPIeoOHhEiXxVvpPUfFuyYypPynNYpPSskdDKjJCeErRtxXTHIzjJZihoOZvoOGfFWwgVz&quot;\n    &quot;CUuaXxAeEnBVvbNiIAaRdDxXrmMcgCyYceyYyYEEMmgWwGOoKkqQkeEVvGgrRJjrYyEeRKYyOP&quot;\n    &quot;ZzJjpooOcCrgGtTOoRNneHiIXxnNIQqzZiHhZgGZzyYeEuUOmMbZzUjaAJuPpBozZzlfFLiIph&quot;\n    &quot;HsvVSvVUTtuDdPhPVDdPpvYyvDgGCcdTSstaHhcCHhAJgGLOOoaARVCclgGLvrsSolAPpUuaXx&quot;\n    &quot;aAkoOPpeEzGgZcCjJHhKkKlYUuyMQIiqSiqQBbIsGLLGgvWwTtVlpyTtYgAaGPSeEfFsDdgGkV&quot;\n    &quot;bBNnvQrLlRqYWwyEqQLlsSSVvstTghHGeKkKQqhHHhmMoYyhsSAaHdDuUKkOMmDdMmgGgQqRnN&quot;\n    &quot;TtVvryYlDdrnNLlRUcCurRgGZuUWwMmbBTtjJzjJbBdDaAwWLjJKDpPpcCDdHhPRrhHRzjeEJZ&quot;\n    &quot;mMcNfVvlLvtTVSsxXFnVvTxtTKkjkAaKhzZxXfFHuBbkKTeEyrRcCYTtGgtuZDdjJzUUcSsCqQ&quot;\n    &quot;gGuUqUulLRrwWQeHOohSXxNnisSIWHhwiuUIfFVMxUaAuGdDgXmdHhDUuTFWwftbBvVKAakvrq&quot;\n    &quot;QAayYWwRCcIvVijJhHrSNnsDaAdBbHLlhRPpAqZzQSsaQmOpPozQLlqhHZIDdisdDSyFpPfEJj&quot;\n    &quot;eYowWhHrnNRRUuhHLeEdDlkKyYkKqQCcEexXifFIYfFqQyFzDdZcCorRgGzqQZDdzIiMmwWrRI&quot;\n    &quot;IjJihlLHoOyYDzrRuUKbBvMmeEKkFffFsSVcYJuUjQqMHEeeEhbBYyucCfFGDdqQRmMHhhHqWw&quot;\n    &quot;oOQTtrBhHxXCcbggGGpgGPgEecCYyEeEesSisSeOovVcwWCEIPJjnlLTtyYNLtfFHzZQCcqLly&quot;\n    &quot;xXYSqvGgVQzZsBvVDoOdlLbfFGxphHqQPHhmMdDGgHszjJZShXxXYysbBifrRFiIxcChIiHgGQ&quot;\n    &quot;qqvVbBEewWQxjJZmPpUuMzXlYyHhLXDvVBbdEsSexsSvtTrReEVnNmMTtXuKkjxXJMyYmZFfzU&quot;\n    &quot;MmyVvTtFfYdDExEeXvVooOOjJTtCcUrRSwWsbBbhHtTBGVvgNDdnQqukKGgJOYHFfhKfFkSnNk&quot;\n    &quot;KOoOohHsSnNsmmMMvVFGglHxXhLfFPsKkSpfyBbqQqaxXfpPZzbBCggGdDPlBbLaAZzKeEkYJi&quot;\n    &quot;IemMtYyTlLjJEjXxsSyLlXxCckKeEuUZzrqQlLxXafFeEVvAiRrIRpmiRrFfbBzZvHhVICcoOM&quot;\n    &quot;XxiWwNIinkkKBbgQqGPpLlKWhHwszZSQuUjKkfFJcZzCVvWeEYywtXxTjJzZkSsKfFeEcCrDhX&quot;\n    &quot;xMmrRjUuOoYyJQDdqHwWMmLlOodqQMmqGCcgwWpJjqMmQKkPMmpPBbxnNzZyYySscTcCFftwWO&quot;\n    &quot;eEoOobSsBSsOoaAGoOgRWwcQUWOokKwuTdUuDtYyYiJjYyIyquqaFfwWAQUZtKkTBbzndDXxjJ&quot;\n    &quot;LlJjyxWjJDdLlsSMmGgtiGgIqQpgGTtDdPClLAasyYFlLKoOkfMmFUuBEerRYyGflLFDdnNFfg&quot;\n    &quot;FLlXJjxWHhwDOoOodOokKPpMmfoOzZupPUvVjJSBRrbYLlyuUNTXxnNtUuvVVvpPQqQqUuaqQA&quot;\n    &quot;lLMBbHhxOfFozZJjXzZhHgqQEPpeVvSsEgGAaeUucCItTiLuUlGSsyYEeRrBbZiIAPpvRrhHzq&quot;\n    &quot;EeQqvVBbGgIhHRXxrishHSQNnyYZzxXZpdDPVWXxwMvVmmMByYcCaAEebxXVvKfFlLcCcCkZzz&quot;\n    &quot;QqGgFIifPpZNnjJMwsSWwRrWmvHosSOhVIibMmyYQqTtBMoOYOoSsZFKuUkKZzZzxXPpbqAanN&quot;\n    &quot;NnQyYsZzShHUuGgSiIsYgGyeeECcsAadGgDOojqQGgSstTPpJSQRrGgUunNqpkKdDZzPhHkSsr&quot;\n    &quot;RQwtTXyYxsSZzAaRcUgGvVufNSfFsnFCrXeogGWwQqlLOKbBuUkvWwzZUuEoOKkNneseECcwWw&quot;\n    &quot;zbBWiIwDQqdAawWiIwWNnLiQDdqIAatTEOdDoeVkOopPKvQmbBKkvVbyhHYBWwbzZrQqRZDdhp&quot;\n    &quot;CcOoXxnayYizZIFfANPKkeELKDdHhvyYVCckkvVKTAadDsSHhAjBbeQqEJpPaJjXxaAAFfXxRr&quot;\n    &quot;VpuFWwWwfFoOZRrzPpxHJjhIqQiXfrRkKkKkNnLlKqhhHLNWWKkwwZznvOIioMwWXxeEmYyzZX&quot;\n    &quot;cCxVZzmMUuPKYykKyYkpiIlSAaYcCyBCwDwWdWPpcbsghHnNbBsSMlLbBTpPtEeVuUZzfciICE&quot;\n    &quot;mMMfFmlLeuwWoOnNUgGJjiybBCJZziZzAaIjQqhHCcaAcnGgjUuPpQqpPhXxHUjJuuxNnXUYyJ&quot;\n    &quot;CcLOoImMilBbLlkKGkKPwWEeuUzZHoOqQjJFfGgCckBbKzlwKpQqsSsSViIIirRvbBiZzIZipl&quot;\n    &quot;xXLPTjnNJVpPyYvtDyYwqQWdYyarRxXDdALlIDdTtTcClZyYzvVRvVrLtmMkKvTDdxXtVqQlVv&quot;\n    &quot;LPNnpoaAdDeELUwWuwxXWlOxXGgiaAITtgGyYuUzZdDynNYGwzZPkqQOoKklLxBbdQqDGyYgXK&quot;\n    &quot;zHhRDhHdWzZwbBhvYyVHLHXxhlcaACIidDVbjJxeMmEXJBbtTjBVDdpPsNnSvrRbbBMmyoBbON&quot;\n    &quot;nwWJTtOoQqIiVhsSHLlQqHhXxAoOHhTjCcJwWLltFfetTEFWwtTIyYiYyzZvVzZTOobchHCBQS&quot;\n    &quot;iIsBbRDAamMJZzjoOJjyYIidHhQqOuUWwOovIiuUCcVoWPhHpwdDJjIvTufFUtWQqvVuUXxwtT&quot;\n    &quot;iIVisqyYQiISHLjJGkKcCBAabhHiIKkWCcwpPcChfFHCnNlLcZSBbfFbBgwWzWwZaAZIizGcCm&quot;\n    &quot;SsGxpNnPXQhHgGgGFfuJRraAjOouaUuTtAbRBbrjJlUtuUEeTCcwxXrROioZreOlnncKXxkCNK&quot;\n    &quot;ZzaAkpKkRcCrRVvrPNuULfCcxdZzDZvVCfFsUuSreFdDafFAfqQWPpTeINniEBArbBRaeJjEbZ&quot;\n    &quot;nNsSFflLzZztsPpEeBbwQqwWGgQqoOFfZzWnNSyYsXKkxWwKkkcCONnoNVvGgXxFuQqUkKfjJY&quot;\n    &quot;yFeGgEcCnNFfLldyikbiIBhHKjOoJaAIYpPIMmHLlhibBIaJjyrRYbBjJAaASsVvFuUfTMNTtn&quot;\n    &quot;nNmFfqQBbJOoCcXIhHgGCciTtSBMiImBYybxLgGlwWXNljuUJjJbBdDyYhHLQqxXmtTMwWqQnx&quot;\n    &quot;XWwzSsmMkBbJjeRrsSEDdTtGAagXxKeEJjwuUDdpPdDWTCcKktjmMJjlLXybBYnNMmwoOfTtix&quot;\n    &quot;XjJDduMmdzZJjclLqiIQciqQfjJvVFOJHhnNjUuoOoFfDdpkKPHbBhwWLYGvVgSXHsSFfxNnXh&quot;\n    &quot;pvVAaSsFZtTFfzXoOxfUJjuYyRrYyqQvVSslLPMbYyBmpIiCdkTtKDcDmiIoOQxXBfSsFHhVtT&quot;\n    &quot;vKcwWvVdDlOoHhuUQaTtAqLtTCkQXxEFfeoUunNHhtkKOobBEisSIetTWwTzrlLGgdDRiIelLm&quot;\n    &quot;cOoSsYyzOoZPpdgGZzDenNEnyYXbBoUuOwHOYyplLPSsoUuSECdvPpcCVmvVLdDlDdoOUuRpPU&quot;\n    &quot;YyurxXleKkeEEAaWwHhOoDrReXxEuULEelCNVvnjJIivVPpmwWeEAabBCcwZSwWsXxyxUuXYuU&quot;\n    &quot;dXxoOhHhHggGIPpkKiNQqnSsrReRrEGXxrRfFgQqYyAZLllLzaAxbBNnXRrtTNcyPUumMIipSs&quot;\n    &quot;TtYzOnNMmxXwVQsSvVqvJjVBbTtvoOQcCcCVvqWbmMBSkKsoOvVUCTtcuDxiTtIXxCcdlLDxXB&quot;\n    &quot;FbBfbpPNfFnfFHhdDBkKowWObUUuuxOrRoDUuQqQHhHhBbqrRdiNnUuFfoOINEenNOAzZakoOh&quot;\n    &quot;HJMhHmOoCcaAQIMmiqhHmDYwWdDjJXTtnNTnNpPtCcQqiInNnNXiYtWOYyNoOmRtVvlIiLIpiI&quot;\n    &quot;PigetuqQUPqaAUrRvVugGuUsCcYySQzToOwgDdGaiIAWdeAaFfuUtuUequUQZzBbEpbBhHvVFf&quot;\n    &quot;WpPXryoOYUcCASsauofnNFiHhInNKfFgGWwWVBSsbBbvMmWNnwQqDdnNDywWWNnwrveEVRgvVF&quot;\n    &quot;fGgGfJKkjRgEeYyXxWRrwfFWhHwPIKCcsytTYyYAazXxTtEHheCjJQqoODWwgGdVahHAvcZgdD&quot;\n    &quot;EhHuUSsYxXKkyXuGgUsGuUgLjJYRrsqJmMHhjQSfwWFrRaJKkRxXGglLrRDdHhIizZBYyJjbFf&quot;\n    &quot;eEmvRPprXxYDdFfyrRMdDQqaAnNKsLpPjJleEctfTtFonNOVENnpnNZAaPjJhrREeVvbBCctEe&quot;\n    &quot;IioOuKdDkXxYyJHfPpvVHlLAayaAYhVvZzFfFlWwBLlwWQqyYFPnNpPOopPoyYiIOpJpNiIJjk&quot;\n    &quot;KnTcCEuVvtTtSsStjVjpPTCcfFteEtTxJjnxlLgGlLdDwWtLlrRTyIiYMmMmXmdDMDDddHhNuU&quot;\n    &quot;vUuJjjJlLWAauUwmUppqQPAaHhtTUuwWJjYydXeELVNneuiqQkkaAaOoKtTkaQEyYeqAdDaTIi&quot;\n    &quot;KqQkwWcCGaAQqUuIigGiIRrTLDVvCcdbBJIijhnNHsyYcChHmMSqQcCiIiDkgcCGOomMDkGIiw&quot;\n    &quot;WSoOwdjJDLlsZYyzlLIkKivXxrRnNKkMmXmMPpkBrRoOIiHaAxbBXZrRoOzQqwWhXxbQHhqAoP&quot;\n    &quot;BbZzQqCcpzFfJcCaAjuUzZZUuzqiIQwGgWjJWwFdDfQaAtTHlLFgGfBbBMmycqQHhSyYdMmyYm&quot;\n    &quot;MgUuzZujJhvcnNCPpVPpHgiIGFHhOMmNnfFoGmMvIiHXzZDdnNZzxoiIBbIihUuDmMBbdXxXcC&quot;\n    &quot;KkatTAzZFfmCUucTtUuhHCNnjeEoOVIbBjJbBXxYDdXxJjyzZNnVvApPpxXPfFtxXYBbKRrkyT&quot;\n    &quot;jiQzOoEoOeSsLtTlZqyhxgGOqQTtOSaAskiWwqGlonNTtfVvvVFOLEegaXxAQRrJztTrROoQqZ&quot;\n    &quot;jZzZhHBHhbeXxYQqyxEEexXYYZzyFfDcKkWaAkKwwWOozZIiBuUTtxXoOAHhsSWLCcLlrSssPp&quot;\n    &quot;SIbBlkYyXOFCcfxXlfFLOoMmuUohHOmMoGCcgSsJWwjDdLlcpCcPidDwcOoCVvLbpPBEebmMSD&quot;\n    &quot;yYeEdnlcCHhPzZtOozZdDhkKHTWwZzwhHBbtDdinyYNjpPJccZzVvYyChHRrDGkKUuxXUuaDdw&quot;\n    &quot;QqKuUkLleSsERrRxXnNTxXggGGnNmMSsmpPBGgyYFyiYymMnNeTkKcIiCkKELleLmMgKkcClnF&quot;\n    &quot;fEeSsdKLmMlkHDdjJuUiPdoOLlBbXsSxbBojcCJmMOCcUxXnaikwMmMhcZYysiIHhRrCcZzxXn&quot;\n    &quot;NyYbQqAajJUurRQuUutTkKUnaAJsSjXRrpPeHJjoOhYyEoOeEUeOFCLAaRrloOFyoOYfWwwSrR&quot;\n    &quot;sWwWMiwWQqWwIlLmAaHhRriIOVvQvqQIiotTcCVChKcCsSbBkKkeEFfEePdUwWuDRSiIsCcJAb&quot;\n    &quot;HhBeFLlfjkKJqVZtiNnPpiNJjnvVxWwSsXpPINnKEyYpPpPjbnNTtaAWfFwBtTvVJrReOoEcOm&quot;\n    &quot;rNnRuUoUuLlnNcYdDMmcCDdLlAiBOoNnbcCZzbENneIiwWsSDgmMGkKwGgEeHxXhyMzZrMmRmn&quot;\n    &quot;epoOdDPENIMmiZzYfFVvAnLloONhHaFPKkpHhdjzZJNKkcmMCsSKkLWwKkRrdDlQqHobddDlUu&quot;\n    &quot;oajWwmhHMXxLLlnTpSsTtPtTtNeSNnsalLAEjBbGgatTAtTJjJDAnNkinmMNrRSsIkRrtZkKzB&quot;\n    &quot;bMmVQDdqvmMuBWkEeEexXKwwPpzZYydxXyYxmoONcCKkRWNnwZzrljJLCVPpvaAtlLwkKWTcbF&quot;\n    &quot;fBdrRguUVXdVMLlmvwWVMyeEYmHRuUOcAaCoGoOgyxiBbIXFfuUroAOaAcClLoaeEODdRVvrHC&quot;\n    &quot;ZUuzcDWdoOaAUuDOQquLOolUojJSsxXoGgOxMmiIDpPdGguUIidDIixXlLLlmVvpPHhQPaYyAk&quot;\n    &quot;KptTXxXjJwLlrRPpCcWwZzqodDPtToOXxPporRMmHdDsSmMsHjJlCxXdyYDzvbBPkKpVvLEekK&quot;\n    &quot;eEkSPpZzLlOVvHhCcosbBSJmsOoSNnNnPHhKkaVvZzoOoOaAeoOoAIiaUutTgVvGtTOhHGcCxO&quot;\n    &quot;oexXKjJLlkiJqQjDdtTeqdDzQqAaGgZoOMmDyYdIisnyYnuUNWrTvVgLSsllLGgkCcTtoUugGO&quot;\n    &quot;KGtCcZzKlLbAXUuWwrRxaexXEUuVqQvBYyYyrRzxXZvNNyYrsSRPpHhChpdDJjPIiKkHhrRHiI&quot;\n    &quot;SswCcWwXgKkcCGxUukaAKkKszfBbDdiICSsbBZzwicCjJIfqQbBFZzBbRrJVDoOdEevjuUyYTt&quot;\n    &quot;wGBoqQSjJsgGObtDdEeVrfkgGKFUuMmNJjWwHnNtFfTsDdtSsTZxXzWwSYmCcMIiyhnkPpsSOo&quot;\n    &quot;KNVwWGgvFUuOofSAHhlLaIiuUAVsSvcBbCVQficoOjJVvCBbUKkdDugGjJPkacCAgkMdDmEeLl&quot;\n    &quot;SsKBbKkXevVEZFfbHheTvpPRjJdGasrRSJjAgTJjoObBUurRcWwNnCMmFQquUvbqQBymMYVsOE&quot;\n    &quot;eozIiLlZkDSsZzEYyhDuCcxXUOUDduKjJgGzZhHkoddDsSjGzZggJcCSsbrqQReEBPKrRHhkeE&quot;\n    &quot;pjIioOowWOAaoOkKhHqGiIkKjJdMkjhalLAHaAlLxvaluLljJLlVvrQqrPwWwWzQqXxkwWXxKZ&quot;\n    &quot;tSsTfFuxQqdDfHhFXpPFEembBbEqQeERCcDdxXaAPpsStuBAabGHhgwzZPpxipvVkKeEfoOjtC&quot;\n    &quot;cFgGFfdnNoOKVvefFEBbHhkNTtKkrAaZzdDrbBRwwWyYMmdBbjbBfMInvVNEeGnNgiIdDDdRrU&quot;\n    &quot;ugxXTtScAaCsuJRrjHhpPUGKZPpzXjXxuWwUcdDCyfFCrRcoORrDQqDdBfSsscCaLliCgGPrai&quot;\n    &quot;vxXFfVIAJjRuUEDdepgGZyYzQNVzZvUkKZPxpPXGgRrpzwKsSlLkMmaalLmMDEelLcCTAuUfcC&quot;\n    &quot;FoNvVnCcNcbrRSsRTtIiqeEeENnTRrdDkLlJMmjoqhHqQqmMQWBbwEqFaJjIiWwAbzZzCtTgGc&quot;\n    &quot;qFtTfGgSswWAnhHNhaAHMKbByYKaAJEzmMdEbBeKkcjoOUTgGtuMmJNZznCvvTtQcCqVitFffF&quot;\n    &quot;yhTtTtpZAdDaXqQxWwCvJzaAQqAauUZwGgWjjJnNDMmDdgEUaAsSaeEMmAubBFRrgGHqlGgLyY&quot;\n    &quot;NnEiIrBbHhiIKktRrTRNgsIMmMZzmTtnWkKQOohHGgDdbBLlxXqwemjFvgGJjhDiINnHyeKeEm&quot;\n    &quot;MaAxXhqQQAaqrRHtTMkKLbBlFfFMmfXhLQqzZMmtKSskDiIbBcClLdpPBbWECUuqQceLRrgqVv&quot;\n    &quot;gGQezDdinJDdNPpRrAVvOoaZefsSsdDMmHGFCcfPsaoOAbBVePIgGYVaAOoAdDDTtdIiavAeeG&quot;\n    &quot;gGpSscwWCYlLeEyexXIiEPKkeEdDAxRrXaGgsJvxhHHxXllLLhdDPiIUQlLSsqVvflLFVEXsSo&quot;\n    &quot;JKkUWwukKjwWsNnrRsQqSStjJdDdDFfdcCAsvSsiMmIlLDfFuUIQIicCqtTRCnNcriVHKVvkhZ&quot;\n    &quot;edDERryYyYnNzSqQsTYiHhIXxiqiIgYyoIinQxOoXxiIXVvdDIwWmUuyyYAaJjYGgdDrRSsAZT&quot;\n    &quot;tzDdaNYyDXxdxgGgcqyYmMlVsSwnNWQTtTtqvnNqQbBqJZziHyYNnCyYIJjBbDdBDdTzZtuldD&quot;\n    &quot;LjbfrRQjJqFBUNnujmHhcIocVnNdDaAtTvCRrgaAGHhOBbsZoOPiIprRksSKLWwXxIioOdHiIG&quot;\n    &quot;gLExyYXmOoInNiZzMeGgfFyYIiEgGaAPpVvIYJjbncCNBTtyErRAkKasSeoZzOMUuAmNWwWFAa&quot;\n    &quot;YyiIfRxBboOQqeqQluUwzZTMmDHhNbBnzZcCHMmhPIsSiAapDNnqQcfXxxNnrRZzXTtCGmRrtK&quot;\n    &quot;kTMZOKknWyeNdJtlLyYgGEDdsSsYyCciTIgbBgGGstefzZZzZzhIijcrRCTtboDdOQqWwWmMSs&quot;\n    &quot;uuNkCcDdXxiIHoRMmrENneDdIiVvxUuohHUuOaOolGgYUuyJqQVKkVUGBbgMlLmTtwWWwYyTEd&quot;\n    &quot;DeaANnNnRrBAavRrVbHhhbBiFZzfIHtMQWwVXqvTtRrVksSjJDRrPMmhnNaApPtAaXxnNqsSKM&quot;\n    &quot;ZcCzYyEjJedDBdDIKtEepPKkQqXxNdDnocGSsgVsmMVveESfUUuuFSvVgGrVvxXvVaFfiaAIku&quot;\n    &quot;TtUqQjMmkwRrQqkbBKxZztPbmJSsXpGgHhPnNcJjpdYyDyqTJjeeEdrRJjDXYdDxXAaFxglLfF&quot;\n    &quot;mRreEvpPuUtBbUusSPfHhydDYfRrXxqVvQKkYsSyMSSssZbBMUuwBRrrRbiuURrRIxQqwpPKvV&quot;\n    &quot;kWfFEFgGpPlGqyEebUuBqZzWEuUYEQdDqLUhKkHRrXCoOwypPJjOoFfzGgaAAaZuFoYygGGgWD&quot;\n    &quot;IskKvDdwScGgXwPTwWthjhVAaAakKvnbrRXxysSiUXMmxiIxoOmMweoWyhrBbRVvHYvrcCiIkK&quot;\n    &quot;ZiIwVkKvZDdDCiutTUIoOIicdzkKzZhrRMmAaeQqkKBbSsOFfoVvEoOHGDmMKkdiJjuUUeTtim&quot;\n    &quot;wWTJoOfSsFjWTtxXwuUuqfFiIQUhmMGgtTHllaAZzpLVOzZHxvhHtTdNnNdPwWLMNnRruUwPpW&quot;\n    &quot;OogGVvDeEdnNzZkKkMmDdKnqQhFNYwWgGNqcYyZzRLlrpPkKsSLlLaAAaQsSVuVHhvjJUdVvDq&quot;\n    &quot;HiUpsPpSKkNnyZDdAJjaYyYnZzKYyCcvNneEVwWTtdDkNrRXxaInNuxBOUUuuZSPpsSVHhPpjN&quot;\n    &quot;nKjqQJuUCtTCcKuFfUNaAxMmgPpDdSEeszZGaAMrqQRvmONdDnpWwNnjJEePvGEewFwZzWjGgc&quot;\n    &quot;CIiDkKXxmMGgcCZzkKdCcdDGGSobBvNnVKvVVqQUznlLZzZqQCzZvVmMaLlGgCcJjFMmfvVFso&quot;\n    &quot;OSuCcyIzvVqQrCcRNQGDRFfYGJkoDdZzjJToNQqSNnsnDpPdqQTNMmvVfFcwWqTtCDdqaAQGgW&quot;\n    &quot;CctTxXCqeEFfVsSaAULrBblnNmMpxXZOozxXPRrSJfFqZzIiwWnYycClYfFrRSsjJyLRrnNFtC&quot;\n    &quot;czakYyrkKgGCkKtTcTtQqhwjJjJftYyqQJvVjTTtFOVwWAWwaPbVvyYdDWwrRBoOJjgGRKkUvM&quot;\n    &quot;mVurhHyYHuUBfySsYyYqrRwWQTtepmMQikZfFzrRIVwWjoOJhHPpuUawzZWbjqQJdiIDBfFAhS&quot;\n    &quot;gfFYlLLhoHhOZzmMoOgGDyYdpPFKkpPgGuUfkKEJrgGUuCEecRJbCcBEuUevmMSsVJwWCchdpP&quot;\n    &quot;DYyYtZPpNKIikSslLcCfGqQrRvVQYuUxYpPUUuzHhUuTfFxquRSsdEtlduUDtTMrjJRhHPvJjI&quot;\n    &quot;aAQqilbwKrRAaFnuUVvNfYhjcCckKcCcbBCaACmEehHgGQqHBbDdhpPHhnNqQHhcCIizZEespP&quot;\n    &quot;SyYtDIeBOorxBXxbJjXKkYyYegYyyCsHhEeSGkKgXxGdCOOOuUobBbnNBVvxXiIFjJxXjqnNkK&quot;\n    &quot;OojRrSsJQXxDdrRBRWJjFTyYtfZzTMGqQeEFfNnRrRDPpRrITWwWFdgGDfnBbyYfFBbReEhHfD&quot;\n    &quot;dUAaJjkKQvVzZcCquyYvzZRrMgsKWNnwkkKvVPpCcAaFcgRYyrGqfZzzZLlGqMPqQmbFxXaJkK&quot;\n    &quot;jFgDpuUbBXxPdGTyYqWwQBAaUtTucFfCACcSsakLMDdykdDfFUuRkKtTvQsKPRuUcFfkOoPpLl&quot;\n    &quot;WwmTtiIQqMbIiwWBPCmlLMqQVagGAZzaJsSUQqbXrRTtGOogxrRlJjnNxXBcCaAsSsvgxXGPpV&quot;\n    &quot;YyqtTxdixzZXGpNmrRMvHhEeVFzgBbGZBbnOgkIQqiKFfgAadDGBcoOEeCbWLlKkwXoRrcFfHh&quot;\n    &quot;YyfyYFGLIKkBeELNNnbnNBbBbztTZtTnNUuBSslcyYnTtEeNHhRrpmMPxWwXGwWokFYyomWwmx&quot;\n    &quot;XzZOxdmiTpemCcZzHZzyYetTleELEhjJHsSLlvMneElLBZFaejJxXEKRKkMmAatTrciXMvVUum&quot;\n    &quot;xByWwYVaArFiIsSSNSSIiPAgAaRrRXKZdApPaNnDkBbIiCZqBwcCWvVcbnrCcHIigMmgGAabRr&quot;\n    &quot;BLlZdDZzhHzOPmMCccamLTtYyTJjWMGgCcCCDdZkKlLtTvVNNnnzBvYyGgPpPAasdHhcCfFsSX&quot;\n    &quot;xqQYyOoxrIizZPxXtTdUuDUKkLuRrBbUluUISIisifhHqhHGgWwSDdsDVvwWTrRsStpEcCuxwR&quot;\n    &quot;wSsYJjndDCuUcCoMmOIeEisSUuiuUfuBbYdDyHhLvJjxXvVVlUoOZOGBhBbHbLsSlBYyLaAlYy&quot;\n    &quot;izjJRrsLxhHIiXcCxvdDjJVGgSLVCcvAalsxXPsSpBbXeiwxNnXsmMhVvHZpcjjJDPpXEcCexg&quot;\n    &quot;ERkZzKCNFfSAatNnTqQuOoCcUEeEeCcYPwWVvtdrRHhrRDkKVvNnQqoOIaAUuNeDCKGgkcQqAa&quot;\n    &quot;JjmMKuxXzLGgRrvVvVPHhvVwWMwQrnsFYfFaAbDlDvVtXbBpPFfXxxaATnNuUfFhzbSVFfDCgq&quot;\n    &quot;fFQpjJLlPynNoMmNNmMwWNQqsSDdnBbnYGSsjJgKOoPQDdsGgSqmMpBbkdDsShTDdtrROJvWwW&quot;\n    &quot;pSsuUPwZMvVmzGgLlqQduhUuHYWmrfFnNyqQiiIIYkKRaBXTkKNubUWnNycaACCNSjkflLAdnR&quot;\n    &quot;HhaANnqAxUuXMmaSTtsCcIiKkeEAjJaFBLlQPpgOlLsSrzYyEeeEzfFRRXyVoJjOvYtTPpBYzB&quot;\n    &quot;DYyhAtGBbMmtTcfFZGfKkWwFWwgzIhZzFBGgxXxUaAiqQRzNLPgGUupBbGglTtcxXxvPGsIaKk&quot;\n    &quot;mAaiBEBbnNhxBbZLlwLyYllspwWTEmMmMZmjJMKmXDdxvVHhDddkirRfxFfXrDdkkAalLvVUmn&quot;\n    &quot;NHhCcrMjGnEeNSsgYybfSHhpqQUunNMEiRrIeEeqaAeEpqQGMmgVtTvPzZXdDxpaAPVVZzcCRi&quot;\n    &quot;IIiryYQqfRcUUIiuohBMmRrfTZgtFvsScqSsgGQQOvBWmzZffFFtiIlEeoOUpPlvVLZjJHhyYx&quot;\n    &quot;ZtiIOotTHaAcPXxpCdcCDzZicCXVvGbhGgZOoHhWoOJjPfFBSoOsfIsSiEeFVWnNKTcCeExliI&quot;\n    &quot;aAqNTtnPnOvZpAaHyYsSUuCcAVvYBbyaWsSSBbQqshBbgGTNntHiIHJoOjicfFYymMCIhkKuHG&quot;\n    &quot;gQoOUzvVKbBKAacuUCAANjJvVnzZwMOkKhHnYiSsqsSuUUujHfFhsgGXxGWwOUapcNmMnTtYTt&quot;\n    &quot;XYwglAagwYWwNBZzblEeLGTtgOrljJLOoxsNnQEslfFSsgRrExaAWwaAMxXPbBHdaPpmMjJeBb&quot;\n    &quot;ZzExXEezHRrNqOKkExrpZzPNoOnSsfFwMUFfCgPpGtEbVIiIkKoOiaAiZzBHnEuiIUxXQdDCbB&quot;\n    &quot;cezsSZQqfodLlQoOkKdDDdiIyqQIaAkxtiXxaaAATnfFUuNVCctQqzZTnkKyesSOlLFfBbkVVe&quot;\n    &quot;EXqYyYpkiILEEyYecCAhrRHHhYuQqUyxXFfaNnxXaAHlLBhHbBEqQRrMmetTWwbWwCEtLloKkO&quot;\n    &quot;txcCXTpPTecSuUppeEnOoNAaEUueCWDdwckOaysSMuUlskAGgaKXazZIBbHivVIHhtlMqQlGgL&quot;\n    &quot;FqBbQVCcOhHMmOcLlnslLaASYyNCxtSsUuWCciCcSsmwWnhWkZzYcCWGgwQAaEePcnNaAtTpPN&quot;\n    &quot;QqRrnUuIOuULlTtJaBJjNRsPBcCIGwNnWCsSGgxoOlDuBbqkKKkHdDgGeNnlLROLoOlXFSIisY&quot;\n    &quot;ynNSsNOjzoSTzZbBQfVaAvFxXiMuUbBOBIiLyfFIqQiiWwAaEFfuUCVLDdlvOOEpPgxXZzcwWe&quot;\n    &quot;EnNhHCUcCUuAatnKkWwRriINUwlRrLWuwzRrZToOtWZggIiGmJjVvMSaAXiKeEkIxivVQqSLls&quot;\n    &quot;lLSsQtTqXxtjLNnkKxNnzqQvVOoNxIivlLpdeFfExXDGHdwwWJjWnXgrRGqQlLxTtIiLlhRrMm&quot;\n    &quot;VNNEeAQqaXBMcCfrWtFfNDdNSscIwDmMsYtDdTACOfFolUDdszHSseEOJevJxXjbGSsgTtSWwH&quot;\n    &quot;RGglLXFfTtcCaAZKKNnVvEeQsRuvPplLfFTtHhyJjHhEzuUiIZYhSGgYVTaATTuUttfFLluUbZ&quot;\n    &quot;zBCngGUxwxTRrHmfFeEfWwVvrvDEMmuUuLHCwWYyNncJjHhOoRXxvsShHVrQqiICbBPpIpZjOI&quot;\n    &quot;AaibFpPZtTKGMHhwAiIKsSsQzZliPuGgwjAWyYoOwFIAOoFgxbTHbBhtHhuUIiGTkcXNpPQqnL&quot;\n    &quot;SsGDagGABbdSsKkNMmnzMNRrzFjJhHfIeEeEGgHhFfZzexLlXNZJrRkQWkiILlFfPCcJQqgFiD&quot;\n    &quot;SWICaAObSMmsAatHhTaMjJkGbBvDdVgkeEhkLZfzxXnNaAlNnhMVvdFfbBHhgBwWeEbBbUucCW&quot;\n    &quot;wbBeqQEyPpYFMmFFfhHLVvzZAGzbgGBSQqQvPqJjKAanZzwPIxXUHrRtJjaGgEesSgGIPsztpe&quot;\n    &quot;sSMmgGwWezlLnnNSsogGOTBCqFkKaHhSgGqlLGgNrVxXpbBbbaALgQqOoVCSslLPprgaZzHhgw&quot;\n    &quot;UEeuDUKeEEekcuUAacwKdDHqQhVvsHmMtThIUWFfwEMdDGXxhZGcFuyAaYdDXVfFnnNmMnNXxp&quot;\n    &quot;PpPVvAgGfFaNSsWtAapPTAaNqQZznyDPcyYJtUZzhDJIiKkjIifyYFtveCXxuUcCiYyxUuXcBo&quot;\n    &quot;ellNnWPpPpFfnLlNIUWXjikKRrCYycfFjqQkKWaZzQqbHhUalLAbCcBGhKwWkWlLcmMClUuLEe&quot;\n    &quot;TtVVzBdKwsSWmjlLosSOKkBOogXbzRIYNnNNNHhnWwJHhfunVjJjnNDLlUeEilLSmJAZvVVvza&quot;\n    &quot;YzZLlyDdzIFfyVvVCchHgGQqevVEpPnNhrgGWhHwlLQqsDdSYxVvvXxTEbiIBePptlVvajJFfG&quot;\n    &quot;gskKmlLeDHcCfFhtuknEomHhMmZbdDBtTLsdGqQXWVvwrRxggxXxqQQqXGxXkKInNBazDvVbBt&quot;\n    &quot;aRrUmNnLiIgGrdjECiyGWLsSmeOuUoluvcCVcthHDMmIgGeRrMmXjYyQMJYyYDwwXxWhjCdFJj&quot;\n    &quot;rzZpPhHpVvSsfeHhWGgMZzmbBlLHhUeEuHiIRDdqpPQkKliIKkpPQjlLyCmpPcIiUuQqjKwKYy&quot;\n    &quot;kDduUiEgGZpPtpeEPNnTlokRrKmMOyYQBYMZzyAaboOLZKfyYFXSaAmMYHhxXgZzGySgqbBgGo&quot;\n    &quot;BJjbOkKYyaKfPpOoxXyUuVRrvYQqFmMeEYyWwmrRhivoCcwWOQaHhArRBEKkelLOodDRrgzZiI&quot;\n    &quot;uBbUNnLfVooOVXxvOvAaRrBbFBbkXqQrKBEOoebkgyYKnNkuUWwGRoOsSTtKmZzAaoGguBNnBb&quot;\n    &quot;mMVwvVPlgHFZIiSTtwWsxXXxUuWrRvLlUtTfhHUuSsIYReEUurCUPIgIzlLJjZiPwWeETthrUi&quot;\n    &quot;cWwJjFvLGPpDdcCZtTzhHcrcCRCfHhFlLMbtaAgiBDdlGJoOOoEeeVUbBKkMHjYywgQqGWveEp&quot;\n    &quot;PCGgHWNnATtitTPQkkKKaAmxXdDMgGqRrmMKkprhHXxRCWwBqQdjJMBbiIOoSsCcSgvGgXSstd&quot;\n    &quot;DTZzqfNnFQeETiuPpPpkyYnElDaPpyYAdsKpfOPpoUuJLludDCyYlxuUXeIhHXTZViIEecCviV&quot;\n    &quot;vIzZzfZzsUuSNnqLXjJZUuNmMbBiInzxhwWpmXYynSrRxXgyjJYOopPGsaAxMmGQGNOSgeEMOE&quot;\n    &quot;eEfoTtOlEeWNWwLlaRfMdDwIlLTxyYXBbWcyWAaXuUxszZSaoOAZjJBSkKsgbUuDDNKumYxXmN&quot;\n    &quot;nLiNnFfIhRvzQZzjBKkbJmOVBbDpPdtTWYKdeEjJuePmMpOoVvDdbJjLlzZFwzlWuUwLnhHpPO&quot;\n    &quot;omMcAxGLlOLdDsSlkKiZzwiZzTBEeqQbrRPplnSsNLUuBZxOBiIDdoOjJLlugyQgfjiSoBbOmM&quot;\n    &quot;YeJEejrREZnNCczMJxSsbeEvVaVahHynNuNEeqQWtsSTbIAaYyMXiJjIRryYOoWsOpPPIdRAus&quot;\n    &quot;EbpEcCrRrjJpPBYPpLUeEtfBbFrRZHoxKsSfFvFBbfVaAmMZzaAbByMtTGgQqZzTiItqMmxnqN&quot;\n    &quot;XxknjkcCFDdJjVEeAmMbBSsavhvVHUCOoOJZpHlLvVhHHhPpQlLeKQLlqkQOouUkvVEeeESsHQ&quot;\n    &quot;VvZwWEezUVQAQqNlEHlLhewBYSDCUuNUuLlAQqKkBmQAVZWUdDyXxmTtYyRGgTGwWBKkGgHhhK&quot;\n    &quot;kbBHGgCMkKmHMEemcxXoJxedDEXsSBqQRLlYyMGgmeERkXxzZUvDMnNJjmWwhlphsHAigNnSsR&quot;\n    &quot;rNUQLcCbBkuHhgGtTUNpVWtKkTwivVIGdrFpWwPnLlMmFgGvVDMmqQdpleEaLlAfFKkwWLxXxX&quot;\n    &quot;iIzsTYymCyernsBOAaobBRFfayYwwoOvSsSsVWNLYylnNHhFhvksHicpOcaegGHhevxyjjJaaA&quot;\n    &quot;fPmOcivaqQfhAagGHEiKrRFuuedDfWwHhKVgdDvVUJjlLTtxgJhdDVhHhHyDdYVHhxAVvVDdva&quot;\n    &quot;fFfFKUuTGRLqQRrMmrzQquUMmzZZkgGvNrRZWxHhXWmGoRraAxXOmMXqQKCfLVvzvVNnpPZlUu&quot;\n    &quot;hkXukKUZzXbNnnAcdIGioOaAjHhpXxdDRruquUOlLoVTtvTRGRNgGLkKhHJzcUrqQRNcCnDrTs&quot;\n    &quot;UzLloKmJjThnNPEeptTHUurVedcCMpPHhCcoOAaNMiILlyYOsSjJqQqQfbBMwtpPKjJkWwvVTC&quot;\n    &quot;cDiIjEwfkWwLiIiNnBSkrkIiEejUqQiKkicTeEdDPPpBbhHpBbGZzgJaAMmUxCcPPlKkLKkjJK&quot;\n    &quot;kzZUsSCXxhHcbkKmzYpPyCIiWwcZzSsUAkQYqQEfrWcVLlvyVutTUoJpPibSZzQqYdUukGgKKM&quot;\n    &quot;QoOqaAwRrlLiIGHhkKxjJXXKQNnuUxhyYopUZzZzKIiYyFXxWwfKxXwWqsSuVvUseEMmGqFfGJ&quot;\n    &quot;AaPgxyhHYhquUQWwAAaXfFxEecJyZKkKqQdwxXWPppYCcyPqzbiIunxXfRiJjIebiMmYyABFIi&quot;\n    &quot;zBbEeDdZMoOmfgxTdoBdwcXxgNnxXOhHVynwWVjOBbVIiMmYggOiHhIbntCosSOsSGSsgGgUuI&quot;\n    &quot;qoCaUuwaAPpWAchWaAgGwHOqsXKyPdHhkKcCDdCiIvxxXEgfmMFaDwWghHMmvrbtJjTINnlIAg&quot;\n    &quot;GuRrLlUiIMmairxLaQQuUqQZvVzKhtTkVuUuUvKbBUulKXAKuUkaMGgmonZzOoUuXxyYGELlZm&quot;\n    &quot;MlLWmPUuMmuUAaSiwGzgEwvcLlEeTtCYybZznSsNZiImMwXxWgxNnqQJDnUIbqsWDdqdJffEzZ&quot;\n    &quot;eQqEfCcFpPtKkTkiIUkqrRXywWwxmyYqQCjJBlJtuUoZjuQdlLdzkKEGgqQsmgGgKkZzxXkxlJ&quot;\n    &quot;jKkLXKdsSUuDEqDnUsSfEmtYyfQqmMhfFHCQpEvVeqQMDjhODtaBvVAJjadaQCcqVWwpPsNkKU&quot;\n    &quot;qQJoOvzZSswWPpOozUijzZyzRQqsAxXQpPhDdDKkfiIFsoUSoCgWwwltreEOoRcCgGweEuUrVf&quot;\n    &quot;soVvaoCcONaMmaAFMhepXVnQqiIlLNtlLCvSUuIiskKZUuLqQrgGzuUVvspSalLDPpfFKDdkdr&quot;\n    &quot;erXWwTMaAmvWtTdDhHwnUuNXJFfjyYIRrDNniIdZzoOXcQqczZxfFXERrheJjAPGgQqmMXLOol&quot;\n    &quot;UulZzdqQalfGtduUPAUuAWzwWwHULIiluAbBavqQuMmguuNnyYUysqfUGgCUvZRwMmpjvVvKki&quot;\n    &quot;AaNmkOVPFQqiUhtTVVvvBbjJGbDwWdwkWIxhyaFIifAGTnwQxXWDfjFVvqFgXxdhHDGrRJjHvk&quot;\n    &quot;nNKkLVvlCpPcBjJrjJRrbBRhHZCkQJjqKtoDdPnCSnNsCcEjzhnYyEeKkNdDIUyFLosSkKOlBs&quot;\n    &quot;SSKFKkKkVgQqRrMcClCVvhHcozlLXxXOoWwqQuOWwWmtTIiyYyaAPpYuOiepPEIoquUgGuRvVG&quot;\n    &quot;TFfvSsVxPIirVvVKFMmVCIyZGJlLjwHmMctTldDJDMmFfuMMIHdLemRhHrSsrfYNEjcjHwWWHS&quot;\n    &quot;GghZznNSsHDvVdWwBIiTuwWDaLlkKyYbBTERgANnaEelLzGghCaZAabBhTtLUGguDFRrGTtgfG&quot;\n    &quot;zYyZzxgGXZgFfKQnGOoRrqQxXiItMhSIzZNniObvVvVRroRrqaAQBUvVdCtTcjJkKFFpCcCcbB&quot;\n    &quot;lLPsSHhLnNzNTStPXIWwitCWlLNnweEcrkHhYPqQtTpXcKwFWqQxcCVulOmvDawWAzNcDvVdEW&quot;\n    &quot;wQPpqWwjnuUIdDiCupUBbRoOrpPPvVpFLRcnLJjlqQTVhHAHfAncCNhHIiaoMmpuUApPaPADrp&quot;\n    &quot;gwIBbMGgvzZVstTSmAtTivnmuEeZzUbBnDQfFSsqoOHhHHLvVlrOoRbBLSsUuUtThnNLUulMmH&quot;\n    &quot;kKuGMmPqSwWwqEKkegeEODqYkMmKbjvRrfFYyVfFeFqQfWoOpPjzjPihXiHhPpHhWqdLlDrRKN&quot;\n    &quot;nXxlvvVCuUcZaADdzUUaAuKkGiwxXYyCcWIuvQiXiEELEekbBzZKaEPZfdrKiIWrxXovVZZAZz&quot;\n    &quot;xQquULeMjPHRXdqplLPqQQdkgejJxeEyYXYyIJJladTDoOpapEeslTtkmMYatwQqeEbgGgGsSP&quot;\n    &quot;pbuBgFLOolSwEpRrENEWwteZCczPNnHRrjEeAafFSkKZzESOokUDAfFVvfwAaWIiUPpzgGuAan&quot;\n    &quot;tlHKiLDdnNXFXxfxbBoKdDkOSsHhSTMLURrTtucePRrmMPpoORrhHWwUTkzKRGYqhHQzcChOHk&quot;\n    &quot;cyoOEjiQqnEegdDNnGOoRjdDpMTwaArSsWWwnzXZzAMmmMaDdxFqbBiIgGlEPpXZzpPDLkKVvl&quot;\n    &quot;mPpobeaAfNkKDbnISDdYSsnNRrvOoToIZoVvGjxRrwVPpPpmPCAvjJZzAlCcMmYtXZWrVvWzMU&quot;\n    &quot;IiVVfZzsSFQjJhHqQNRrkKKUOouIijMCcXxtCcrbTtCQqXeIiEDEedstDdHSsfvVTtZyYbHhHd&quot;\n    &quot;DMTtovNWwPaXoGHyYFlLNjJJjnLlRrJssvVSniINScCEoeRrAWTtoOYKzVvdxRPZSDdsKqQacC&quot;\n    &quot;cuHhCGgcHHsqiZzIKkJvnNjgUuGFUVvufoiCIBIkKRvVmtlBbeExEqoFUumMJHxcZhHJcdjbBJ&quot;\n    &quot;DwWdDZqsSXLJjuLzMZwGglVqSIimMkOgmQOCRrcCcnLwELltWUuYRuDtLlTsSeakipPYsSvVyI&quot;\n    &quot;PpKGdVEenNvxhHXXgeECpiIqdDFxLGRraZzEfFeEeTzxQkwaAWsgGhHeCHhpvwYWDdPpuUgGWS&quot;\n    &quot;sbBwtTwuNnNZzuEJjJNdDVvyVvYneEcaNnAZOBbmMpzXxWkKwOdizMTACuOiIoUQqXoOwyyZzJ&quot;\n    &quot;jUvkZhHpPkVtTnqmAaMyxSffJMmZbBrUuRuUCJjpPPcRXiISnEAaDdednqLlLOMmJnNWIuqqQY&quot;\n    &quot;uUyZCclLzZfjJoXxORdpPaADQqrUnYxXjkKrAaRJNoOnJdDjyUuPAapXxNuBvzmfMpTfuUiNFN&quot;\n    &quot;nmxoFBbfOcCDtmMTnmaQRqbrxOgGgGHiIUCcXzqJWvcCBhHfzZpPGqQbqQFLNHhIinNwWbtTxq&quot;\n    &quot;QuUuUIdrpPRHHhRrRxlXtfFHhlzZxcgGxXGgxKkYqQdDyQxjJjuNnhIodwVfjJfFmYyBPpPoOw&quot;\n    &quot;VIeEiezpbkBbaALBbWwllCvfSsdZOoJjWuDdUrRvVwlLzyYvYpPjJsSUiIfukuUfFmtTnfKFNZ&quot;\n    &quot;gHnPpvEezcbSHXxhsBihZzByYeHhUeuUVvJTtmyrVkEkbgDdwWfPnNfFppvVluTVLlhDdiIbBD&quot;\n    &quot;bBqHCcCcKAWwXnCMkKoyjTKsSAtFfZoOziITtNmJQqTnNigkWuUTtwKYeDdECcVXqQdBbhHQsB&quot;\n    &quot;WXxmMaDdtTSnyYOAQuJzUCweRrmKFrjxrNNfFnndYeEKkrpPlfFfaAxmMXxPCthHwbcCBEKLev&quot;\n    &quot;GLNTDihkKIvQdnXcaAxXCVtaFleMDoOTtQqxXRpbBPRcQdyYZxFSsnNckwWQTQWwdDAafHnFNy&quot;\n    &quot;CVvdRrDPWwwZeujJVvwBbWHhkRBVpPVoeEOPpPkKgGrLnLltlLTcCeYyQdgGjqSfBGgVkKTWQE&quot;\n    &quot;nNuBbHLVvCcVzZJjNZzAkKVvcJjCcvifwIDdivtTAtNndDFwlOolLjJzAbvjNPxXuFYDdyDdrh&quot;\n    &quot;tojCfFcuUUuoOTIjBbxxXXSbBCciZuxkJkKxNDdDNnbBeEdnRHuUsSbBTthHcPWhHsAaSZzZxA&quot;\n    &quot;aiqzZfFjJeFxXrnWTtaAIiiScqQuQGgqQDrWwRlyPpYLtTEoBbOpPlLNrDsSdzoWwfEeQDdqsC&quot;\n    &quot;cSEkeEBjIhHFssapPtTcxXlMhHmRrMmgRrOohHYKFfvlLmEeBnzJVRxTGHkkKKeEqRrSZPzGgG&quot;\n    &quot;aAgZplIyqAamMVvWdddDDKrpdGCeEeVZWwyYzKrLLrWMLNRzAoOagiIAaGNPdLlKLENsRDGwVV&quot;\n    &quot;gxXGdAuLlOOnDdLlaUSSssItTmwJVRfcKkCIimMxNnoUhGgSpPjwuceEeaKkrRSscCuUUcZPpS&quot;\n    &quot;jjQBvFfSLlHtrRrpffZzLoSNACcnNTtagsrRSJPJTVqQvYyCqjJQRAaBblTVvpEetbBwWLHGZz&quot;\n    &quot;LPvVtThHgGCqLzuJjUOCBbDHhHCaXmDsSMJvVAbqQUHhuQqdDGClLwDEedWOoIYwQbQcuJkmxX&quot;\n    &quot;MKPUuchHyYhTxTtSsldRrkKDXqQEuMBTpPtJedStTjtTAwWvVLOQZElsSoSbTToJjtTkFYyAaZ&quot;\n    &quot;zOyVvVkdDKJKSCcIYtTzZhDPYTUuBboOcmvzzZmdhHGajteEQWwMmMmpPLJjudlLFfHnTxXzZS&quot;\n    &quot;bSERrehbKkbdDgGElRKDcxQqXhnBJjQkbvVvqQuFDdfCctHwPSspsSMLxnQboInUAMNnLxPXxb&quot;\n    &quot;BXTrgTqLluwZzqQWzbDdsSxXMguTjJstTioOdfFDIRAvhWwnGOzioOIJQqJGeEihTtVNnvfFtB&quot;\n    &quot;bwWLxfFhjJHLFyuHKmjaHozLlZNnaFlLWwrtQybBkKerBbXiIgxXAQYxrRxkOqpPbBCcgedDxW&quot;\n    &quot;vVhNpBbjJgGfpOoPjUqYjJxhHXtCdvBaAuTzZKpPJkKLFflIatJgGoOrmMRYgGPAJxRlLYrRUu&quot;\n    &quot;iILpwQqcCgXxLqQvlYEQBTttRroOCOGPpJaAgDWxAmPhnQqNHfFRjEeGBbCQTtqlUuqQhHAaVq&quot;\n    &quot;QnSsNvxXSsuJjURrMmxJjPWYywlLpFfuUwpLlHeJbBjVyYOokQBjzaABvVGIiSdDTtvVhCqSwr&quot;\n    &quot;RRRrZzZzonuUNolZymhZqbTtTbBjJVvYyXxoGgzZDdXOhHfFoPwWponlLAUNlJbBjLYXAUuvsS&quot;\n    &quot;OYypwsLFfNktkKwBbIioKtBtYyhiIkKHKkThLkjJFaAEzZjJlKMmkKXBJjmjJqGzfyWwnAkDaA&quot;\n    &quot;seEUuTtwWsRcCFiINnjJIrsSLlrakfgGPpnTtMfcnaQygGRIiRWwWcKkflkKLBbYyFQUuqJjgG&quot;\n    &quot;pPvLlBmFWwpPgGxRKQzuFBcwWRUfePbCcBAquUngGgBVcEiQUuPpDMmyxXYyqpwWiJOoNnwdNV&quot;\n    &quot;vSLOaEiIqQezeEZHcQoPClLLuUlcuuqQOoVvsUVvRIVvWdDwwgGaAbXxIKyMmYeDOrRDrOyGgY&quot;\n    &quot;aAuzLlNZzAMmpkasSQqNnAxCcXmycCgkZzKvVAYmwRUuKyqytAaDWCqoNiIKkSsnJoOTXIitrd&quot;\n    &quot;DRhboDxaAaUuhzfJjQqDDdxhkrbhAaHJjuUGUlVzcMmKIJjiQRrqkdRYQTuZeBMqPvzZOrRMjd&quot;\n    &quot;SCDEeVCcEEeULeEEwXxGgXhWeiMmIvVbBJjnsSEXpjJeEOocCXpMaAtTmPaQrOkKoRosXbBXxx&quot;\n    &quot;CjJcmMNWwnlgeLQHcCCeETtcRrPzWwsXHjKkqRGouUOoFSTBbbTtBBjvOormMRDdOoVwSZasqs&quot;\n    &quot;SJqQNVnMmepXFcCXMdDTHgfFrBJjEQNRrRrhHoOoQCDAaCcdygdRrsCUNwWnXwWxrRoBbiAaIk&quot;\n    &quot;aYyTyAaZZhHDVmLuUtPpTmVxXLyYlLohmJQqXkziDpLSYAaynNuDdTtVvEeMmDcCNfFJLljZHh&quot;\n    &quot;AaqQWwLlxLlWneErxgnNoNnsIzdtTjJUEOtFduUwWDyoSshLrPbVvoOEeHhvVyMmYSpzsSZPKi&quot;\n    &quot;IbBkNnZzLItMzZhUuSsQqHfnLIilYLsmMmYyHhazcmUuswtsKkBbvuUJzKOofFaApEjJgGUBbX&quot;\n    &quot;xeEuvSdDsxhHqQXVeeELubBjAFTtfaeEgGtEGzQQryWlLwQYoOcyvzZRXxratmdgGDdmMDMFfm&quot;\n    &quot;AFiXxrRwTtzoshjJHSuuiodDOIyYWAYZudzrqujtTmFlNXxABbzZCcdNnQqgriIGkLlPpzdtjl&quot;\n    &quot;LPWwkpJjBbawLlUYLzbRTrtTxiPoVGgWrRlndDNEMDRrEpSauLwYcCyWyYtjJTiKkIPpAOokSs&quot;\n    &quot;KHMmwtkKoMvWUulLMmeEnNevMGCeYsSJjPqNDdnKkGwWakJPdhbzIZzFIiMXLlhhNmkCVvZMBE&quot;\n    &quot;cCvaQXhHKBzpDJmcCMmMaAHhZLsShMmrkKRuHhRChNnHsxMJcwWDdFUuPpJWwHyYGqjzZnNcWP&quot;\n    &quot;lgtXxjNSjfNcCjJnPklMXGgkEsSeKSdvVmMDrPiItTpDqQAAagPHxXqCcALlZTtkEGnOaqzHxX&quot;\n    &quot;sEeSUMmRrzZgGMmQqdbrsIzcbBClaAHggpEsdDqQwWSHSRraADXHhhHsZpPndDdDllvHEhHehV&quot;\n    &quot;MzcCkKZcMJiIAFPpxAaeUlBbbBzVcatHhpgihRwDhHrjJcmTvlMNCQqwmXxMTPpEdCqQcQrXxP&quot;\n    &quot;QoOqpDlGgLkKIiXkvasSAVKhNxNnjzWwnNZVvLgGHhlzTULoQqJbedDZZyHhAPVvVnYxYSOeCP&quot;\n    &quot;JdDdDpEWwenrRQGxXMOoZziCcIsaAIfBHhkrRSsYyhHCMzeAaeRvVqRGNnuJjUgrQFlLOWTHhj&quot;\n    &quot;JuvVGSpDdYZIiXntTNcCAtTrRwikKiwWIzdDZqLZQqvVvIimRCQqnNqaZzgEzAhHcCkGIiaDdN&quot;\n    &quot;NnmMSsmMrkpPcCKdfWZYZRrsFfwSFpAuvWRYNbCMgGmKIXFfkEXJTXFPUNzkaZzAKeLEdDPpVU&quot;\n    &quot;KrKmMgpVuUnNpaeEgGzQYLloOpPpghFftaZFfVKapWPpwijYyJdZztXywWYxjSspPCAaJWSxcP&quot;\n    &quot;pCXlLUhHuJVcsSCBXUuWUlLhQLlUTtOZODdLdwLhxnDCcdVXwlLoaoCtNHhKsSDdkKRykqvQqW&quot;\n    &quot;UuUuyclfFLlFACzLtTlWKZpSHbaInbUurSsUuxCcXRJFJlRrLhigFmJYhHdvXlVliIvbHvaAde&quot;\n    &quot;XFfGhKWNpPnpPNttTttITtwWijeYysxXLGPvImmtnNDdTLimyLuSptqfFkIYKPMEWUApPaLRoY&quot;\n    &quot;THfFIMGgMyjPprJPpiyvVtFfTXxrRjhloOrkPpUdyqQsETLlqTrJjLQqvosSyYHhWYFCiaPfWw&quot;\n    &quot;gGZzFpAIwPdOPhgUuXKkxbOpPuQAaqFQbBzZqUgMmBUrRxsSwlLUuTttHhTHOGgFGgfnRaAeEn&quot;\n    &quot;qgGlsSPKCckYyVxXXcdDCxQqvpqQZftWhnSsWDtAlLaTgOoxnxVlurzZCcsSvhHVaApPmuzZuu&quot;\n    &quot;UrHwXHPFlOzoJSufFPHAaiZZUWoOvMpcYwWOCUmMuqzbBZWJYyBbjaAzZpreEuUoeyKkcCSeEc&quot;\n    &quot;mMFslLYykjJKWQyYBnpMOvlLFOrznEYPyrkwJuUjQhEhqSspPNnKfdksLaAVJfFWbUSDJJdNJo&quot;\n    &quot;OTNxdTtEvVBXhTtwbeXkKPpuUYVNNnDYxUuoFyYMGVvQqOSOFWtORAaYqQfRXxrHhoOYYyOhrM&quot;\n    &quot;KkSmMjJFdFfnBhynqmMiXYQqyxAouRnLxvVNnPSEOiZiIcBILKBbkLyYBdZzDDSkKHhHygvUlt&quot;\n    &quot;uUxTtCCcKXnOdIwWONnJyMOKPNlxZzpwdDwfSAHuUhCXxtTJsZrkbBjJxlRrUjJPlIjBbJxXMm&quot;\n    &quot;jJizZRrETtUaAfFJjAJLvVFfVAuUbNnSAOENGToFfuhHgDVYyYmMaKkeQbdzZGgPIrbsmeEpPM&quot;\n    &quot;MmLoOomxuWJNnFfcoYsSgGyxHhCjJttTxwowRmMzZrTJbBDlNVsiZOoAXJjKOOonIyYiwWRyYI&quot;\n    &quot;aupGOPTWfjqQBJbBjKiICFfdHGzFNnssSCdthKSoyIUSUurhebMDdTtINnAsSsmaAQaGQqgAiC&quot;\n    &quot;qdXZGsSUcHhItTkKKqpLszXgGxYlcCGEesvUHVyRaArPCAwwWKfgMnsiIXxQqmMUubtSOPVSZG&quot;\n    &quot;XxgwPpWMzhMmqQbZzuUbTtYmWwMydtzFvVxXflTJvttTTPpDNThHIhJjHTzpcfojHAvbCBqCFI&quot;\n    &quot;jJipPiGKAKkaSskgYoOuYeQhHCcSWTmnIZyhHainyGgZqQqQeVXfAzChHcZyYwDdZiIWwzOkIH&quot;\n    &quot;yYnkkYdmhGPxVQXxqJjMmJjlSvHhJSFtNnWKEejfyYFAjJaWJMdgGDHpUWOocBbhPRYyZDafMT&quot;\n    &quot;tAHuBWyNMlLfFepPsSbQqdDLADQPBrJjsxcVvlLOeEoxkWeEVJjvVMxXEVOhqQDdFamhHFXbPm&quot;\n    &quot;RrsSSVYZFEeONnoQZeZgSsOodzKTtTmMpwWApMIurMHheKOEoOefFokEHHhPpAeEVvOuPRoYLI&quot;\n    &quot;kKiQNgFnNKwMDdRNfFRrcKZhUiIUTgJNLxpnjJrKkjJiGkJbBzZMmjkKocCOgGNNLzZlXYLPoO&quot;\n    &quot;ANcGbWKhoRPprSZaAzLlzZuexmMVNnsLlSMmaBbFfAvXEvNhgbTkQPaAhnCcNnOoNcwnqyYkcA&quot;\n    &quot;IGjcbRrMmdvaOFfogvuqrsdxywKLCCcVvFfMwMdLCcPaHboaAXIOWpPJjIDdQyxZvViMZzPpkR&quot;\n    &quot;NnwbBRXLGgXzZHhqhUbJVvtTsSGIfFLdQuUqFfzTZJnNjzrRGgtHVRrFwJyFpPeWzZfcZMmEYz&quot;\n    &quot;tWlLvwWrlafPRouUOKVGpZznNPeJwbATyAadLRRWmFfMWTFgGqQERYyitBbgJjSRICQJAawWjw&quot;\n    &quot;WORYsgReQSsnNqxYxXLlSKksObBVaAdfFDOowoOxfHhXxFUsZgtzZTgIyPrRUuFpPAafpyMsSr&quot;\n    &quot;JphVbBwPIYQqmDvaDZpMmMnNmzZLJaCDQbzZpPWXWGgwXVRVMAasOFynRreBbPQqfZvgjiIJOo&quot;\n    &quot;aAhTtaAKAakyoNnOBbIiJjVvVuVElcSsCmdDYyhuaysKmbDqQmMVFfTtGgXxnNodHhiqkiMmYR&quot;\n    &quot;mMkbIiqLaUhiyYmXBbgGPwtTqYyQWpwWYyLlYbfkeiHIsSsSyLlVvVGmMXeQkHMGgGaAkvVWwo&quot;\n    &quot;rZpPzmnSCFbAExaAuNzetvVTDmEWJoqmMhHYcCrcymTNDJroZfArRzZecaRrcCVkKzaLwAqAaQ&quot;\n    &quot;NlVvykDqQqlLlQxikzTwPNBbXxUuxXnprRWtnfXxDhaFaQqCHhDCctBeEknLnBhRrMzZiqQInL&quot;\n    &quot;lHVmMoEeSsCzGLHOADuUkVvcCBbEhhQqHMvVTOvVRIEsjCcgGrEDvVdMmPpKJLuLBCJjcfTtsT&quot;\n    &quot;UZUVdnUAgpcCPGcCaTYSsgGXInUueFzCVVtcCcZJyKaAdYYvAajmOEWTFEeJjfdhHMyZvVzWMv&quot;\n    &quot;VmWTgbLlqQfcJhcJjFfCBbwWFDdvKBbKkkFFfDlLdfvVJdLkAtZzTbchHCcEWpibgWIYyjobBK&quot;\n    &quot;tpUuxXZzPOadpPpPzZbSaMwEgaAWDUPpusVIQqiBaAlCUFRSCcUtVXxrRBbLlvICEnTUHhhsAa&quot;\n    &quot;vVSHctzVvZDaVvYyiMfjSsJFtTasOoSKkbPpUrLuHCchFvKNnCNnFdDGBhvVZlzZLzHbpPgfcN&quot;\n    &quot;nkAaPpVfUlRubBBPMmZzpAfFmIUuVvYyjJAdxuUXTCyGgYutNeceEiTusrfdDucLbvSFfdwGeW&quot;\n    &quot;mAGSsgsBDAoTkOsSJVLlvZIiziJjyYwGBIPlLweCBaKlDRrjVDdMbBPpmfHuUjCFqQBGtzZwmM&quot;\n    &quot;wYmDnNtweoMJVyyDmNnMlLkYjXkKxzCGgTvvcZsStTfEbBNixSswWFfytuNDvuvVmVvMzFfBbu&quot;\n    &quot;GgtLlSJjFblcCUKenMmNjJTtEkgGljbpPBkDdeRJSeirotmasSAqgGQHeKdaTtohVvoOlgRrZM&quot;\n    &quot;mYycOvhNBbmHbNcClNKMmkKJrRjbnNTdjJcAfiIAHdFNFDhHdfZKvuUrRVIXDdqLQdKYLnSsaT&quot;\n    &quot;tWlATmMtZvACEaFzORjdntfFMYCRQqyQEeOmGgMjwePpMdtEeTEZnUXeaBfcsNMROKgmhKqMmE&quot;\n    &quot;xGgQaAoOJjqgvYihIEhOoHKbBfFFByftTFWwxMIqQLlHuAlnNQBKryIKQIDOAavdBMkSIxXiYA&quot;\n    &quot;UHMLhsMmSHUbfFBuevUvOoUVvuLloOYlLcCHlsRrLlSLGVzFpwWEvVNYcCfCcoSmvrvxFfWwgX&quot;\n    &quot;xGxwCcBqdVvcAbBjloOPzdAVdcCMmyYMyiQqpWvHPjRlLmYmMsSyfPpFYYiGnDdNGzSuXWvoyX&quot;\n    &quot;EraAGPpSyroqcQlLqirsGfFTRrTtIOorefEFfeMmtwwrrlDxXYtayAaYBWRrjZzEVZzvpPgvCc&quot;\n    &quot;kiIrpFALRVwWwTZyezOowWQqCFwEfYjWfSsSsvhJjZQAdDaqDligsSgGBbXxjBuHQxlxrWrKmI&quot;\n    &quot;chHMmCtTzXYqiuUpPPpwnNoZzixOBhApNnNpPnlDmWmBbBbclkWYXDSRQUuhHUVZzGAVDBCJgi&quot;\n    &quot;aCKQNWCHpqKtHhBGHBbnYmrRMyVUBbOzZoJiywWYIEejspPYyOHZvVzkwBgCnaplyxnngGKgIC&quot;\n    &quot;cRFWwfNYyQqPXlnjGtuuHlLaAzkPpCnrmWkfGnqlyOrRCcrHhpUYyoaQqpPhmRMmUimPIiaoOt&quot;\n    &quot;TPtgGkSsZMxXmWwDGzEzqpPfzyvkpPuUKsnDdLlNMdDwWpBxfhHMAeEfHovebBPQqpmkKDdxXn&quot;\n    &quot;NvwKXCXSTtRbpMmZEezqdaCclgGmMvVBEmnYtTwbUhaKkmFAdzrMmpHCXxwuiFfIPSRrFfsxXh&quot;\n    &quot;mjwJPpYykLlyYwTucCUfsjVsLBbvkKXpgHMDLliIyKGgKNhiKoWDdVvxXdDyYaFxvwWEiLlIQq&quot;\n    &quot;wWztTYNIAYzaAiNnNMMmYytwsZzoOqEyUyIfcQbzZcBVahJOFCsSPZtitndVjzZtLWwZTDrRBB&quot;\n    &quot;UuYyXxZzHZaAmODdSsozEeWJjwsTtsSvppPosTBrRSEeNmGFkWacpYmMCcvhAaumMVdDSgLSsy&quot;\n    &quot;XxQqQqZSlOoKkPewWEQkiCugzxDQcIagLlGAlLnNpPqMSaelLWgGwEimBEHRUusuiYONpPPpns&quot;\n    &quot;kHTDVvcSfZghDJjckbJjCcJFoOwqQtpofFgPUAirvSsVrRNMmokxazrgGRISvnLdjtWOWXxQqX&quot;\n    &quot;TczZQqXTTttWwOCjPpwUeEXMOlmMmMmMIiSBNnRiHhpDBqEAyQCcqvdSseEmCcMtTGgGUOtEeg&quot;\n    &quot;neoasBavljauewWLplPpLXxuLQqXKRzSfFUujctTXxasFWdDZzjJdDWPXLnCcpkomYjowWinND&quot;\n    &quot;lLoNxBkKPnNpbkcXTLuuUVGTtYiIhsdbllibCYyzIoTtespDdXlNrUOkKgGaEeIpPQKkNYHNsS&quot;\n    &quot;nbDdNDbBfsmRHoyofFOFqQyroTwEefosoKkgmNlLndDvVeEfOrqQRXydnvyxmMEBjJWcCzZHxb&quot;\n    &quot;uUWwenNDXntjnMYymDjOojdqQEesuogGUuOBwPpjvlSYyKDFiIkQHeHqSsHhWKRYpEeyeNZRof&quot;\n    &quot;DdEGgeVomPZzeENrRbVvBbqkKwcCSfkKCsdDYGgEORPwQcoyAaCPfFmVwOMmouJmMjzzIXxRrb&quot;\n    &quot;JjBfFhpUSssjnNOQqZoLfphMmxWhRUJrRjvVsLlSUMJjGgzZRULvXNXGdwxXNHwTFzkKEeLQNr&quot;\n    &quot;mMNohGgWQqXSsubGufUkKoBzZbBGHpoyYDpWjJZzcfSsywOVVvlfgjJGyzEzZeZYFHhRtQteSD&quot;\n    &quot;dFfYDuKRjJLHJWMmwbBYIjRyYJYmbBEemqQiIkKihtZzyOrluOowehHmpkyiKmMmMwWQTiIPsU&quot;\n    &quot;lYBbMIlMMpPOoiVpglSTtEJTTTndaADwkeEHgxdDEYyMmDVhBVLvbBLxQqVDyGgjMfGIHTtjfH&quot;\n    &quot;hAajUuOoBNzZZzdDbBiWwyYLlAgGBhsCcBbDQYxXyqdPzkwaAIiRrBZzbFfoOZcCcafBbLnNCY&quot;\n    &quot;wHhVQKYroOkOolLnEeTcOAOWxXxvNIiXRrHlWUuSsDlozouqsSHuwxbvjTtsdDZGgzwpHhPjcJ&quot;\n    &quot;TDIVBbvPAkvzATHGPhHyqZJjAPGgvPGkRjJjgGeEJkuvCceUWwulEZnPpupfxFcCftjxeKxikL&quot;\n    &quot;GSoOsglaALlcBnyFPpqfFQfrlXHhxLEewVjJwWNnUsSyYaPrFfRfsWcCSzyzwlLFDcRrNVtTvf&quot;\n    &quot;FnCRlLnAgXxKaZenOxXodDmxXkTtKMNGAQcrMVPpxXOozlQkKtTZzBbiIIMmKkWauUAzZPpaYy&quot;\n    &quot;TnNtxRKkqQrzPhHpyJjhHPszZgUTttwoqQfYyrEOoEWwZFfmceEKLlbFWwizZSsShHmgqqQPpN&quot;\n    &quot;PGgjpcEBbosyXyNoOFfvpaNnYvVwWzzVqQvEBwiIWjOhHlutOoWwqQZJXnHXxPpxQqdmMdDRhH&quot;\n    &quot;qDetWcnmLGgLhHlVFflLeEtMCRdWrHIGPTNnACvZxXhHZzLuEXfajIimCmLLuUNzSVvxdFfshT&quot;\n    &quot;teobBOHhPGyYGhnNuULZJYyjiSRBDuQvVqhZQsSAoNgeKzaQhDdpvVGJbBjeElLaYydhHmMRkK&quot;\n    &quot;sxmLKpNnFJzZsnJPpTGLpDdwFfoOCJQghEejKkfNnSsChHtTjmoOXSbBcrUVvHJjlzaAjdPZqQ&quot;\n    &quot;bjJkKkxqAVoOebmzcKMnfFHHGgxmfiZBHdDDpjKAgoOQpyEwWcgmVEuUwXxgGVCcmOQIikKqTS&quot;\n    &quot;sWqQoOhalUAsPedmksSKeLwvOpIOoXRtrBZRruUlyuJjWAPKplLJTDZeEKgRZzGDanmMLfmMMJ&quot;\n    &quot;UQKeEzZknNbBNnRZDUzyJjawlLUUOZWIDxXdfUjJwWuMzHhZmaEeMTObBoOopPEeAPpVYCoOyq&quot;\n    &quot;YHhRSsbBqXxqLlZgeTAauUKUOxFfXouBbRrkUgGuJBbUDdlzZeEPbDdZzvVBmMkZjFfVeQqaAL&quot;\n    &quot;sSrRlrRSTWZzSMCZAOokKMSlMmRrIiyNFsSmTilUuvVssZzSBpRlHOYPpkKfToeuDZPpiSaAOg&quot;\n    &quot;GGXRNwXMmqRpPUuUurQAazHhqQdHhDNnndUAaoORraSsAslPyYdkKMmIZKxjNnMHOZzlvheNnE&quot;\n    &quot;HMjJUulMvpsSPEedwWhcCHVvDdzDdzYtAKuyYUOxXnnCfFcNNucpPSRrDSsGYcqOnqebRzZGaA&quot;\n    &quot;htmmMFfxpPdhwWzZmFfMHKkDfxPEgGdDNvRroOwWEeUunjQDdSAZzzsiIWSmMsJboOtsOkKofO&quot;\n    &quot;fFgRsSrrQJBbhBbxOPpoSZpPphnIiNqlEGMmLpaAPIiSOJjqAlLLlxSsHhnQlLqNGgPJjxeNat&quot;\n    &quot;TApPWwEwyYHzZxWeluzZeRPprvcCdcsDJeyYEmoVmMpZPpzQmbEzUtqyrDVjJvCZZzvyYLfFug&quot;\n    &quot;BRKZzzZIiHXhHDddFUuZHAPpUnNjJuXdOBNnHTxtWEewjOfFQdDcwgGdTYnNQYZzAaUukrWQqM&quot;\n    &quot;yzYiIyZaGYMKPaUlufFULunZVvUoRbBdodpPEyUpPuaAYkiBWuUirnyyYYkKNqQuqQSUeEUUup&quot;\n    &quot;OsSqCVJjmMvQQqLlqhAolsnDWaAjOaACcoIPQFMmfTtvhHVYdGgqIefFCvfOoCFOofcqQyYFbG&quot;\n    &quot;NQapcCEFMmGgurCbWcCwfUZyYqrRkrXeEfMHhbVyYCQqwrrQqYqANqQCFmGgCcNFKARwWxXRfF&quot;\n    &quot;ijJfrSsSTtzZnNSHhdKhHaeENYTtFZgAaQMbxFsCcSfqQkRrXPpxLevViIfVvKlIiHcCbSvVsT&quot;\n    &quot;kOWwHCPpchWcCTKnlSWPorRcCVaxynuaNOxOzZDdaAAcCaHhtBQzHMYTtzLOOrgGbBWsQcHLls&quot;\n    &quot;KWwkgvVbxXZJPpbqKvNnEYPpkKyhPWQqXnNLzZjJcgJrpMafFVvXxgGXwdGGgjgYcCyochHTbq&quot;\n    &quot;eyxXLVlGaAWPlyrXjSsSsapyjTAiVvUujMmkBbIiFftUAawWiIJjwWbJCcjVDnNcTyQuJFPnHw&quot;\n    &quot;XYyEGQoKXXyqaGTtBzZbxRFfEsSYqiIfFjJTRsSfWwAlLdDGgOhAMmdDJMvVkhUYRrflXleEwS&quot;\n    &quot;sWTHIgjBbSsjYyZoVvgNwVvWmMQqCcHVpParStUGmBiIIiZUQtGRtxwWpXQqlBLlbmadDuNiOB&quot;\n    &quot;qNXlmbBpPWhAaqQTUVBKqoObNYyHMmCdkIirLFfUueSshSQqsHBBHsPpBstqQNhXxDUlMmrRqn&quot;\n    &quot;NTJAghHDnNqQMZEScCseVMCtyTtptTdscCSlGgUuLHIiyPhHqZzqQQpiskjYyuUvYofkKvaeEa&quot;\n    &quot;UuAAVKOroORHhtkKtBsOxXfFLWwezqvVoFBaoMmVTtvOAbRrflaJsDEjbmUexdCcOoDRPprLXt&quot;\n    &quot;HGgCpjGLlgUwWCqBqWyicgkKOoBaIijwOoWmdMxTEtAchdfFRrcJjoZeElEZzKkeQHhcCfFcmM&quot;\n    &quot;ZzjJpRrlAYyaghpPlBbTPQqpgGPyYtLPpqQrkKciIAuUatjpBXxbOojGnsgGOjJliFFYyffIFF&quot;\n    &quot;PRThsVbwWAFsSOofaqFEeXxfEeJJszCxqYymMQXSsJjVvuuUcUuQqCeEmMAgaAGEdDCSsBbUWJ&quot;\n    &quot;sHHHNnhhuMmaAOHhjJXFfFrvjyYQquUaAWMhHUuiybBYuANozZomMUUuaMmDvSXPpAlLEeaxsv&quot;\n    &quot;WgdrSneEfFMmYyelkDxXpnZrZmMSLlsznuiIUlQqmCcwgGRSslldDfFnNRkHhvEcgDPRRrkDTt&quot;\n    &quot;OrRoYywLzZlQYSsieELYyzsQhgtlLXrtTvjZNbMlLVFMmfZWwzgGkyGuURrLStTsCRyYbSsgGB&quot;\n    &quot;rAdaADScHBcCbhvVAauUbkKBCwWSfQMmqiJkKbTtKevVpPKkFoOOZnNOoRnelIiwWLdqUCsIlL&quot;\n    &quot;yYwoONIZziRfEzYySsZQIXzEemoOMwBbpHhEeCvVYyhJjrXtbBTUujKXUzIBpPbsJitvvVVCcJ&quot;\n    &quot;OTHQqfFRfZzXtiITxUMmpnJVBaeEUuZuUeJjELWfZzNztcCTZnTLlaVWFIVPpCanvlhhHUeUuE&quot;\n    &quot;ePpqwtvbbBFgGshHQJHhcCkKDIwWSdviIVVvDsinKkNqErHhRNlRBbpvvbrKUEzWbMmBvkKVpc&quot;\n    &quot;EeYnfNGghUUuuFEedDNnjYyJbBqtqKvVCaKkAfqcCQVvqQXzDqCwWrrVhHaArRvcCdmEQqLbBK&quot;\n    &quot;kHhHhfAZhHzTvxKkNDqGPpgViJjHIdtBzZbndDllLcCgUuVEJjSAaslkKaAeEkeUJNnjuEeWmM&quot;\n    &quot;TcpiINngGXFYyLfFRyDlLHfFhRXJRfkMEWiIUumMcukKkQqKZjUqaoNsAXxwPpbSqDUuzZVvxv&quot;\n    &quot;yGnNIQqtjMnTaktJYOmcNxazZkhvVYyQdmHhHrUuRhMgGPpDdHvMUumtULPrRFGTXxtTtBKLlz&quot;\n    &quot;ZeKvRYMTfFDnNUudoOtjIiuUNyYtTnvVEufFHhEbYYyPpyHIjCZzMtTmcuUJwWCBbqQZVNhhHG&quot;\n    &quot;znfkFNdMmdDDuDdUMKUFnNuLlyVCcdDEpPeDyYFVLlcLJjzZUuZzuUHhwWZzKBeEPuUZUuEvWp&quot;\n    &quot;bMFVvvWhHFflLDrROYyiQqHLlUgGJXqQqXCbTtrRBXuUEQqeADdLlaLLlTxLXpDDddWwPyhQqH&quot;\n    &quot;YrhDiiIODdoXlLBnlTtfFfBgzZUuFRrwWFfbZzVDdwjQZONnQqoxuhzZoXRBQrqAMYyNdUUuuX&quot;\n    &quot;bBMfnCShHscIFxXzZynNKkFwWfuUtTXxYtRrPmGgFkKMZVbFzQekKEUiwlLjolQNjJDNsxwWrC&quot;\n    &quot;pgFfGPtTpczZziIjFFsXYmMQQqNvKzKVuyYUueEYYWxuUcatmZIDaAoYyZPomMzCNQeRrqQEqn&quot;\n    &quot;jeUxXiInaAUVyYvyjJBkKbWVPDvkKVdVXxvcZzErRaASCcKqVvXnNZFiIFfftJkKjxXAUuggGl&quot;\n    &quot;XfQyYGgPcGdDxDgAEzZMmMmdUrywTcCFfSsegGiIWzZlNeEbBCcoqMGoRrKiIsQvsSLWzmTGgt&quot;\n    &quot;ZOoEelUlxtTsSRrQzsSCjmTtMzCXyYhTtjThHtsSfrRrROQDdeJjMmGgXmMLThlLHMaAribicI&quot;\n    &quot;rROhHMOoGgPcCCcpmJVjQAYyLxXlacCShsShUCSsAkzprXDZZzkywqQbBaEUuOeVkKvbIiBjBc&quot;\n    &quot;CbfhTtLlFfgYyJRYoCcCOoTtYycOyrkGiIAagKjfFOAaxApnVOmhaABKkzZzGgiIsaYyASFAah&quot;\n    &quot;TSDfFPpkKDddmMxpPcbBBtTmMRTmJNnkfFnqvvuCcwWmZwkKkKRwzxcCxXTyLzZaVcCalLpPcp&quot;\n    &quot;MSsvWXJgPpOziOwWtOoKnNkVfFyIyYisiNBdYqQyxXUunFnNEtTcCBuEeLlUPpOpPMEedoOxUu&quot;\n    &quot;FfeLQfZNwRWtKkmcCAHEehacCPJrNIVvJeYCKhoHZyKkKkjJgaArkZeEKrRtuKkpEfFmuxXCcU&quot;\n    &quot;MCjJlmUutscCYyOoSsQqIiOmMlLoFfYylIkhLKaAkTvzZVqQNEeWwsSUZLlSsHdDhuFasSpPdu&quot;\n    &quot;KNnsekKsDdJhpEKkThHeHhneBbPXxeTaAtWsWLldDTtwfGoOeEbUBBwWuUuUXxHhoTtOWTAyMm&quot;\n    &quot;ZHhzKLSPAPdgGtDzZAXxLjjiELlOoMmGKDDxrhHWwWwJvjOoJaAVjhpJmNnEcdDBbChHOoQqYy&quot;\n    &quot;uHhUlQqXazzOJYyjRZzzZwkRDFxXzpeArRBsYIiySblXxeeIxIqVvVaHhARrUgvVuIhHaALliV&quot;\n    &quot;YyLTIihRrHtkJjQwIxHITtpJpwWPZSuUsJjJZaDZzDddAzwiIEgGJRPprByWwAaZaAzQdBhHFf&quot;\n    &quot;fFMmboMljJLmGWwEFfeQWHhsQsSAapgllLvVXxdDhhdNMoOmMNVIaoOnNiWGPRdYyaSsUlLrDY&quot;\n    &quot;GgylLzZdRuhHOAKkaWwFfFhaAUuDdaPwWvVpvPptdfCcFDfFNCrlfKkgYyLpPlGUusSHhqQuPU&quot;\n    &quot;cOogGNJeCyYnZaSsFdDfTtAdgGIiVMoLlLpbBPUvVvMxXmMmkKXaNnrRKkAwdDfYcCLlfFEeyW&quot;\n    &quot;kLlmcCMCxGhHgyKdHhDRBbTrRxeEppPTstnZAalffEeFfDubOBosIisSavVAHNnmTbBgNqgcCG&quot;\n    &quot;kEeyYTtuUnYXxeEfFyNHhoOdlHzZzgGAcNnHGgZGretQqAdcClLUkKtbMXxmkKshvVwSshJCnN&quot;\n    &quot;JtTenyFRMNnElDhWwimhHQqvVjJmUFgGfSsdjLChWjJgzYijlLJEecvfkvRQlSsDdLqpXtgrGg&quot;\n    &quot;UQUyYMwcCoUxZOLmGGgvTtfFfpPksbfYqQuiHZJeAacNpRrOTuUczYhHybCcFfjpPPpJKVyYGg&quot;\n    &quot;hrXxRfuUQyYiIfOoJFdwqWmgGMNtgYHXiwKWkDUudKqyYQBMmgGvhHVYFfRrhHzqQZyIiggGHw&quot;\n    &quot;WuSpQqPsIfpvoicsSpPCIpPKMnIVIivVJIiPWrzVujJcuFgGNnQSYUGUQqVQqhWWwZwauUapDp&quot;\n    &quot;PTNngFSsLlLAfFcCRrZztTDLxiIdDpaEKkHeCEefFCxbBiRrxoOVuUttTxYyRERbBMVvmqQGgX&quot;\n    &quot;xRrAsLlbBPSjJZRVwWvlwDdWzAaVVvzZKkcHhnNTvxPEuUepPEHhHnNmfKSlLsYykALaAaAlnA&quot;\n    &quot;EeuUOslLSVvSFfDdFvRWSsqiIQdDSsTaALWGcOsuOSdHqaCcSrOoZYpPLlJGgwWWwIuKkYyZnN&quot;\n    &quot;WwVgGpPJjiIjungyYmMGhHSviIVTtvALlDbATdKkXxqnNQoHJdmPFpPfOoqcFTMbBeFumMCcNd&quot;\n    &quot;RrIieEpPQtTeiIzZGMSMlLmeuUZDDdesHhSEDqUJSszOXxTjLSsbLHKuUkhlcqQMXgGWwmMjJH&quot;\n    &quot;huUWhHYxMmQVvIicKMjiIJmkAaCKxXuKQqcwWCeQqFFjDQwSQBtFfBbTiuRrUueEXxNncCXxNK&quot;\n    &quot;kPTtpsRrwWScCYydjXGzBVWeGgGZgCcWIsQZzqDrGgRdTxXHhEetpYyYyMwNnEevVzebZzBgGX&quot;\n    &quot;xgEetTyYBblLNOVvVLlvxkSsoOLGgoKBbkOHfFIOoinNiIkezZkKERwWMmVvpPeErqPpqqQAlX&quot;\n    &quot;RLtTinNcCBRJjVIiGdIiAGecCXVcyYFfJjDpYkxSjJnNtTQQicYQqVvyDxXdTNBLloGGyvoOpP&quot;\n    &quot;vVoJvNlNnLGgIiwBbWYvoGkKpPCWDbODjJtiIJjXGtpPvVTrRbauUIBErFNThHPuFEeiIfUPpp&quot;\n    &quot;gRrGPptSsUIZzFfiDPpWwQqdaAKqQkAaVveElLBUuZFfQpPoODgaAgGGkHhrRtTzvVYXxXZzxu&quot;\n    &quot;UrRCcjCaHCuUcQtTUuqTtXGpAajgQgSQxXkJjArRakuPwWOHjJFfhHXjYyUuJqkxgWmGiIgkZz&quot;\n    &quot;dDDysBIjDdOvYOoJjZzYKkJjyCcCwRFeyqKyYauAazZZMyYBupjTtbBTtJpvVXuwWZwWzjGgxX&quot;\n    &quot;ZzGgPItTPpiptWwuUCxXIaegGEAyYzSHhbBsZHhIwWuJKRugHhGUDdgXxGGNngpPfFiIdDKcQq&quot;\n    &quot;GgCvVsPpboODsSMmdDlLdIlKFWeJOoddDWmFnNEIRrieouUHhgGfVvGgLlYyAaFgGYxXJYQqyj&quot;\n    &quot;ymLlPpJjnBYhHybDTtdvVtTvVcCIiqbBQmDEvRaAtdDMkOZuoOStRdiIuCZjlRrUvVuPprRnNn&quot;\n    &quot;rgrtVvCcQCXxcUPnNYyJIPpgiDCdDaYyNCcBGgxxSBbsKHFckYhHyEpPQqexdFfDGgEeKkgMww&quot;\n    &quot;znVKRlFfvVExRYyrdDoOGgtTiLbBlIXergvVtwWHhkIirHhRXvhHvMmHjGXYuUNnyuGvsSWwrR&quot;\n    &quot;kFnNEfFUUkKBbmMfkIeFAVIsSuUcCRrCbgGhHBuUqQoMrRpFRrAJfFYtTXVpPhHEEACniINoPC&quot;\n    &quot;IDdhSsSKVHzZfJjqQKknyYmSsMWANaAnhHrbSAarRNREvVYLlqQcPpiIMHVvhtMmRrTtSAaZlv&quot;\n    &quot;VLPfOoNfRtWwyZSszYTcCDVvbBYoOyKkgvPnaLlAKlqdDunGIahnNSDdHXxPLHIidSsVurRKmM&quot;\n    &quot;rrbjeEOChTtcbwWgtrMYuwzvuUHhaqMIibQqaaAncdsybWLwWnVaqvxXuiIqhMJjmKKkJjqtXx&quot;\n    &quot;TEqhPZzgqeEQGzjocCuUcrRKJjkuTuUtRrfHhKJqQLlrRNRXxrRrLlKqQFfnQoOZzPXxoOfTtF&quot;\n    &quot;gGpNjJXVvQeEmXDGgdaAeEeyYExYkIXxyYeEiXOkKhzLZztTNnleETqQuloOyZzbeERQqePBpP&quot;\n    &quot;lLelLSULlZzarDGejEUNnoXxWwOugiGgFfpBboSIikKwWwxmiBOlLiIVvXGgxouUwnUYWwtTmt&quot;\n    &quot;TMAqQvoBbFWwfogGIXhnNHxiOOABZzXjmyswVvWGgIJaAPpFbBGqYGUbnNoXzbxXtcCvVvVIPp&quot;\n    &quot;zZWJHhjIfFaELleAhHLxWYywXVvPplHhogxXOoCcXaCNZWfBMmEUIhHihHDkywQrRqvoyYMqZV&quot;\n    &quot;rHlMFfyMUknddBGVvbzrRwkTtKYCiIwLTtlKktirREeHhgGWmFrAxZzAalLXoOnwLSdDIisKkF&quot;\n    &quot;lLeomGsowzZWpFfPngqBbgXNxMPHaASshHvvVVRsSrhHlQFWeEwtSzVvCcZJgxXGGgVvjoOsxF&quot;\n    &quot;fHhicAaCFfELPkKpcqyRrYQUjAaFbBPkSzZLeZNbBnzNIihHwWoSsATWPpmMwtaDFfZzMmMmdO&quot;\n    &quot;KVvrnNRhHUItxYyoOVGsIVvBcCbiyYvVOomDYybcGoVvOghHPmMpIzZmuUMawhcVpPJhQqXtTx&quot;\n    &quot;ZzmdDnNCTtcuIcCibBvEEejgLbBGgbIGbbBBTKkFfBiIXvVKVvYykxmglVfCeEipNnPrRIzZcC&quot;\n    &quot;pPNnINniIuoOSsRWwlGgLHpGipucyiYyFQquCxXcVXxGgwIdDiKLVvlFfkftTFEezfhBbGxXLJ&quot;\n    &quot;jpWvblLGgUOhHPpoOZzMkEbBexWwKLjJlNnfrRWQXxqEDdefaAFwFlzZGjBbJRrmMNntPpTtTb&quot;\n    &quot;qDnNdxFfXdDVIHqQrvVRMkBbszZwWJGgVFfvjSAaaAJjBbAQGfFQqMmsUuEdDesxgGOobBZXxz&quot;\n    &quot;kzlxXBTtYIimjJyeEbJZzsSjqJjhHcCLzeGgIvVqQWkvVJoOCjJCcSsUuQqMuRrgHfFqQRrhGU&quot;\n    &quot;cYKkuZzzZQqJjUJDdlLkKqLrhYyluyYULYywHhtTbBElLYyaAnNFOotgGObBoTKkPRrOpPotTk&quot;\n    &quot;XxKRfLlFfDWwcJOqRrQoHdxXDWlLqQDXxddyjmQqqQYyYyqJMmxCcdoODVvbBlLFfIiuUEYyoO&quot;\n    &quot;pPwrRIiWJjidHhTCULcbBCxXEMlwgYqumMBbpPYyUQIInNiMZzmsSegGeEvVEceRrqQPpJDRlM&quot;\n    &quot;uATGTCyYcWwzZtgdjJZAbiSsCiIcdDDSlzsSMOeNKUTTkKEetzZLlUudFIifTttcCTEeyYEBbM&quot;\n    &quot;IiPpSpPALjJVXyRHpPvYioOZNnvhHVRrjMsItTudFfJjJvdDNUlLFjnUDdunyiMmcCrWwBdtTD&quot;\n    &quot;bZBxCcGbJomMhHOMkbmMBVAavBbDbZoORsSrwWqQvUgGubBqQztTZMmSsvYyFfwUuHghkKHuBA&quot;\n    &quot;jJwXxJIJvVxPpwPpKlZzLLlkuiCciNnIwNnLLEObCIcENnVTyYdoOnNIRriHuTjCDdpzuUZJjd&quot;\n    &quot;YDBbdLlwvQqDdmMxUgGfCgzHrRgmePrRpuKkigGSQqQqkWCjJrNPpZznRBbYyCSsudWGmMzZAG&quot;\n    &quot;RQqcvGmMLlldDBBUQquPvfFRnQsAnNfqQQMmcgpPGbtyWwMmYNkKiIQyYcLbBlCqvVZEEsSZzP&quot;\n    &quot;TZSJjVcCvpleEZzLisSAEhHeThRrrRWwjoOLlNnJuiKvVkpWNkQMmEeHhpVUuqQqsRKCckrGgo&quot;\n    &quot;OZbnNwWBpPgOJjoLlWwaJjIilffsbBMmSGDCcgGmpPKEeQvVqkHLZBbaRrAFzdDeElKDdHKMmK&quot;\n    &quot;JBbjPpnNmAVtTvBoUpPHMmVvhfFqQuqQEeoOsSciXtTxwsdIfGjpKwqKjaAzxXnEiZAtTvVanE&quot;\n    &quot;emZAaVvKXxkTtgWwlxCFAaYykKfKtlLWwfFBbgTtBXuUGfcCaiuFJMmVvjfUgHhGOofaJWFfat&quot;\n    &quot;oOTAUdDpILcCwCIicWBFfoBHhbObKEekqSkzZjJaCcpIiMmPpPWmguUkzfdDBxXtToJYyzPich&quot;\n    &quot;lyYAaJjUeHhdfFHhlLEeVRvVFmMJSsjMgGhtXtQqJjTWXPjJphHuvBbpPqQVSsNctvyTdDtsPp&quot;\n    &quot;HCgGcwWWwybBeYNnYOoSsyVUIirSqvVoOoOjVjJvJkkzDqSzZsDMmdQdSsejtTGgJEfFxrhsSs&quot;\n    &quot;BqQVEpPjJjJjwWohvVZvVAaSuHhLcaNNnnmMyMmkqQYDdyzOoPpnPpNZKSdWiCSstJjTkKcCnc&quot;\n    &quot;CneExXTpPOsSoUuHhJzZjpPwRFCcJjHhHhmEeYycCbxpPnnEjkKJevHRrNlLDhgyYPlyYLVXnn&quot;\n    &quot;NvVcCZpPkkKKJlLqQjzZXGglFfJTFeQqhfFHEHbBTtGgKkhRQqrfKwWrRsSWwkIcCDdfFBbAac&quot;\n    &quot;CgEeGsqQGzTrRujJqQGeoopPceIYHhuUlIywWYQqiQqbompPwWLlBbIeEqILlitsONnZZzJonf&quot;\n    &quot;kKxtTRroHhrEkKePdgFfGDpEdDhUukKQUdrRLFfXcgeEibpSrqQdDJjwTtWnbAjoKOocCkiCcC&quot;\n    &quot;pUuqIaAiyoDdOsGgEeSKwHHhNMIwTXoZzaAociQqIufFKkUCvfmLTwWVgeEGvjJfFhjJiBbAnh&quot;\n    &quot;MvVjJmHNxwWsmMSSLZDdzKkmQqYcCAsSoUuKlLPPshPpGRrBxXbgEoOcCRreoOelcCBSsbKPyQ&quot;\n    &quot;pPxtXxTvvbZzBKoWwEtTGMmEegkKHhFfYnNcCAsSJhHjaRrjJkKNLaBQKkkKqbAlvBTtbnNtyw&quot;\n    &quot;WYuUpPpPITQqoOXzZKiYqQYTtyqDOFEqlLWKkkKzZzRrZMRrXxmwmMdDLleNcChuMmDdUAabIK&quot;\n    &quot;keEvBFfTteqQSsRrRrToOcuDdmkKWNnRPpjJBbvVXesoOSTtUuoRrQnHhhZfFAgGDPphpuUmQq&quot;\n    &quot;dDXkKnNeGLvVeExXSAaieEIeiIqNnUujJSBbUwWuKkoObBXRvVoGgnklLKOoYyGgSVdDLlGBbB&quot;\n    &quot;bgZzvsnNLRJjBmMDdMmAarRDdbrlyWPpKCckbBDqQduUtTGFrRMmfaAoOLGWysSxyCPrLKkAab&quot;\n    &quot;BoOlsSrRRANQqnuogYyCcSkKdfFDJLIOoiCcKklDdQpPHhJjIvVyjJNtToeEOoBbmoOZzWLliQ&quot;\n    &quot;qaAIaxlLQqXakkZuqhUBoOFfXxcKkCiISsDdbwhAoOKkwWaPzrRBXSseEuUxbVoQqNpQLXtLlk&quot;\n    &quot;uUwRrvfJjXxSDdsFaAbFfpjJJjwOoEezHxXBgcCJjsSxIHhdqQDhTzXYyaAzXxuLOoCcwWnNTV&quot;\n    &quot;vMwDdbVoqNnCYVvyIiSFfsVOVdDvofTGztFbiIMmOowWHOaAuCUurIiSsuUFVvIivdDvRpPdIk&quot;\n    &quot;KiDrlLHaARrjJlLhnNeEBbfFQuUBbmSsPsFBJRrmRCvVRGsSgrpPcOowQqFfWMuKKRFIKDHhMf&quot;\n    &quot;FkzetPSGgLDdgGWUuzcCXHeDtTdbIMARrIiiSgpVSsXCnZrIuXZzbycjkKJCYCcfaAKkHDXxdi&quot;\n    &quot;BLlbNnCgXxTOoavVHVvuUdbZnfFNwWyiIRiIpbBgGPUusSrrhEeHRbXxBbiItUHhuIiTgGfFhH&quot;\n    &quot;qHKoOtTkhQxrrZjJZRoGqbKkfaAEeQrNDaaAMCXqQxcmFyYKJsncRLlCRQqsSrcoOrAasSYwuB&quot;\n    &quot;RrFfHDHhdhUntgGkKxbPzpPZpkKiIiItTAZzqQJAarRXxjMrRwBbyepPFfEYyZzwWrRTtUDdDV&quot;\n    &quot;joHfFyDdhHQqnqQGgAaHQqhOYGcUudCcvsCiIxXczZkKoDdOBZHwWEPDdpeYyPpwWdDmFRrfMj&quot;\n    &quot;JYAayqKkQdtTLdByfSNeEjlLJwWRzZrRqfFWmFfKkOopEelZUktTqQdEeEndDrRKsSkiTpyELl&quot;\n    &quot;hHeoOsncreaUuAGrJjRIidJChlLHPhHzSrRqlLsSXxzZQWQqIuajJAUkKwWlLEvDdVkWuUEewK&quot;\n    &quot;lSzZOoZIYyzZbwWDdMmgrRozOoovVOUuFIcNwWOoyhcCHWrWcCjJXUpPrReXxWxXgGwPTlLhHo&quot;\n    &quot;AoOaOUuuUtdiIQFFfNntLlaHhxXASsTupenNERMmXdDDSpdwgGWDSJjsNnrRrRVLlbBbccmwtU&quot;\n    &quot;udDcClMTtACWwjJsSpPpLWwsSloWwrRSsCcRrGnNgGuUhgDdGPpRzZNPpBoKkOCbQzFfsKkDdS&quot;\n    &quot;PpchHKxXzUUdDuuYyaAsSkxKkrRrNaAnXTtxGnNCcxXaaAOopdDOowWCcssSsnVvsfpPRvbeEI&quot;\n    &quot;bBCkaADdxWDdNEenwRrKkEeXAZFfDtpPWwTdzGgOobBfEezZYyzbhHtTNmEtTecCqQVPphRBnN&quot;\n    &quot;bryYMpPSsrREAaAaPtKkQqIRJjrsgpPGSMfFpfHhFMmVfFMmvVWIZtTzaAiwkKvBbPDnNdDQqW&quot;\n    &quot;WwwhHXZzoHhCsScFIiXxfzZMWwHUuhMOSJjsySsYfKlLHhOweEdDWWwgCLbBNnnLllbiUXxulv&quot;\n    &quot;VgCOMmgIiGqQUuxGbBAaGgoNmMfMmnPWwgnNIqsLlLlUJjuSQCcDXqQHzZgCcnNGHhhVvQSbLB&quot;\n    &quot;uOfYyBbQsSqFXxojAgGvrRqdDQcvlLVxXpKGgEUuekGgKCrpXxMmkSLlNHFfVvhnqVrbBtTaAZ&quot;\n    &quot;jkcCKJEemijJIMzKYJjJjEeVXTtxAavmlCcKwWnPJjpNCcvVbqGwWgQSWwaloOLAstfAxXWwfW&quot;\n    &quot;wBDdSsfFUuMqQpmQgFwWZzQCkKxGbBcCgWwnNAaXSsfSGmwWiIVFfFuUeiIEpPZzrpPTtNwaAu&quot;\n    &quot;UGyYJjgeZzEfQTtqWwFTtAatiAaSMmGgoOsNndrtTsSQqgKkmwWXuUlLxmMlLAaHhCctXxwWwr&quot;\n    &quot;RriIEeJjbfFOofFJMmqqLKklQQQqfOoCcWwoocDgSscYGEyRbEidTEJjebbBBjJWwMILlivVJH&quot;\n    &quot;XxyFflxXPUGrdYyDcRrCUKkuNnoORgIiuIipjJPpLkqRrQWPpBzZpPLlLlLZzVDdpLlmsSNnuU&quot;\n    &quot;iILTqQeMmCcsSmMDfFrIFfivnjJjJNtTjJiIjnNsSJVUGgQXZztDdZFfSsHhqrgGRQvOoxXSCc&quot;\n    &quot;sVuGZqQxXzgEeNnSsyXERrdDDdqQWweMmVDdLltTvkyYKiIGgbBSsKkcUutTHhOoCuaAUkKyvd&quot;\n    &quot;DVgvVGqkBbCcKiIvVxNnuUdeEDXglLFnPVvpzAaZzSspHhCcPHhPpXNnfFZzzZxjJaWwATFfbB&quot;\n    &quot;yHLljUuGjJzZuUguUfLlFBbOvVoPBbdDpjjlaALejJPpHTtQqCcRVvrlyeExXGHfMHhmgGFhvV&quot;\n    &quot;sHMmviPpKIqRrPiIIiEFuUaxXwWAbhCcpvpvVPooOWBrRbHyYRKwhHWbBAZTfkKNhHwWkXpmMP&quot;\n    &quot;KkxKQjsLUumoOMzZRlxbBXbBuvrfFRBEebQPpcwcbpeEPBQCPpgGrnNRntOHfFrCcRgGhtbiIB&quot;\n    &quot;OdxXNnDgoOVvUujJGKjgyrdGgMmgqmMnRrZpPjvCcVXxeEJgEeSsGihHYcCUfAFfxYyXcziIuU&quot;\n    &quot;oONZuJjzZvkOsgeEzZVviIBfFoObnNglxXwLlWFfIiLlJeEjFftTZzLLlIrkzZKRcaACiuUdpz&quot;\n    &quot;ZPVhHqwWQvqQDJPpfWgFfVBbNnoMVmXqNnRrAaQOonQqMLlmUukckJvszobXtTmMjJeEUiAVMm&quot;\n    &quot;vyziIUoOuzZYPuICrRchTdDtbBTtXxQvHhbBQqGRrgAaqjJlOocfFCCbCcBQnyndDFvVffHNbx&quot;\n    &quot;HhhHIiTtlLXMmBmqQlnSsIhHitTNBbHhpSsDnDVXIisShovlqQPEeLLdtTOoZoOzDoZzOtMIhH&quot;\n    &quot;chzZHLlDdCEuoNnOIOozZgWwvVvVWzdDvVRSsVwWwbByrRYVvrRvVfFONnzZEtTEeWXDNndcCu&quot;\n    &quot;LlrRoOpZzGgbBChHcPTtWzBbwWGFfgZUGguwIDdsSsSFfYLlsSrkKRzZBNUuVsSLGglSsnNvHJ&quot;\n    &quot;HpfFWeEjJxCMhHmMmRruUuonNrRjuUJOUXxSsgGPAaBbCcZzpsQqoOLlWVwWyLlIiYSxXidtrx&quot;\n    &quot;XRTDKkDdkKdwwWUuOfUXXxxJwWOojYWQqOoOocNVvnwWxpPrRueEwWsPpYySFfwWBblKaAkjJZ&quot;\n    &quot;OVvozDdYaARCccCreEyRrARraepiIPoOjJHhEeRryUuwWeoOfFwkKGGggPpQYGgQgBbLFffhKk&quot;\n    &quot;HkKeysSYGgQPpnNqXkKqQwWirzZCwacdDeECAOoWcTtgbBGeEIWwWmzmFHhsSFfsSFnNFYCcaA&quot;\n    &quot;yfpwWdiIDTBbhHWwdDVCiItTcHhMgoOjJtRAnNlLarTGGXHhfgGlLyxESswWFftSQqsQYJjlrR&quot;\n    &quot;LIiPCjJxiItTuUjxdDXMcCBUurRTtpTXEZzeaAYEeyWDdKMmJiDdIUuEeVvkKHhVvKVvAQKkqf&quot;\n    &quot;uUxXFRWyYwvUudDVslLvCcZzVvwsSWCOTDdkzYKkyZibKkmZkcCKzOoQqrRvVkQQqpTtFfZzkK&quot;\n    &quot;NnPpMmPnNTHWErReXxrRwpdToOoOtKpghuUsSHGLlwWPiITiItQxvdvKkVOoxnNXDqmuyYvvjq&quot;\n    &quot;QvVhHLAXzZfFGgWwpPHhOKkmMKkhpPEeTtXxKngGVvUUEewBJUuZmMzxXHFIifdDFbBsSiIRqq&quot;\n    &quot;QQruUEDKkdTyoOYSSsSsvVidsSrRcFfCPplLDVvZzNntJAAaajrnNRecCEISeTlLjZzXyYxDnE&quot;\n    &quot;YUhHNAanuwfBbjJFNZzoEaAhHezgcpMDdmnNPHGguUhwWLlcCcCFCLlddtWvVLLsSlJjGgEXdD&quot;\n    &quot;oOLwWjBbJjjCcJDdJWWwwlVvkKUurXOGEeIigoNiInVMmvxCcbWwBwnbMmbHhBBMaPpmyYvVrR&quot;\n    &quot;iOAZzaogVvkKrRGWwZzWqQpPpPweHhybWwKdDkBzZLlNneEMsSxXmYlhTbBtxXDlIiONnozSGg&quot;\n    &quot;fFPyYcjJCXxpiuKOokUCWvVwcCTtnbBNbHhBPpMMmJJYyTtrXxTtzZRUIibicusSUhIKkaAoOu&quot;\n    &quot;UjQLyjJYQCsSGXsxXSnuVvUKkyYxXWAEeazZwXxEejLpaAPlMmbBhHAlLPpaSsJMwGrRGggXxp&quot;\n    &quot;aAPWiPpUDdvVFfubBqNkKlxXeEeEDfFdirRIDdgGeEveEVpPCcrUpPlkKLuNnRjJmIiAnNtTaM&quot;\n    &quot;DdbBLIiVvOGSsLloOaAQLlIwWyOocCtLzZlvdVmMSAaaDTORxXaRrArxevuzZpvVXcCIeEiVhH&quot;\n    &quot;CaAcbBjSoKkDdOBbgEFQqfEaJjfYtwQqWTzZzZyFAcCyYjJarRqsSQywWilKkLpJjEQqmMdDaA&quot;\n    &quot;vaASpEebBhHXPpsSxgqQUuhKkFfvVhHSxXmMYyFEzEenNJjazZAIWwhHSsWwZxkKiIXEyYeEKk&quot;\n    &quot;GwIiWNncCAlLtyYTalzRrZbBVvcCuxXUHhwEwWeBnNbLlXxDTQqbBtrRgnNGdWwTVvdDlUuHxm&quot;\n    &quot;kkKyYEYhkKsXxSKCckdzZmMHhHqQVFlLJjXxzZfUXAauURNnrxXxyYoOuMmVvmMHhfSsdeEDNn&quot;\n    &quot;JrRMsSENnUueEMmNnWWwKNVvngGkkKwwWZzNQqivVQjJqtTYySGZzncpWwPCeuNnUFaAcCfQSs&quot;\n    &quot;LxXefFELllqmMUujJHhhHQnNlmPwWpMnuUNRrEeVvAaLnNoOhfoOeGdqQVJjpPcbByYDdzrRPp&quot;\n    &quot;RrHhfKhcCwWHFfQqjJkFPHZzYXZzjJeExpPTIVGgRrHRrOohUutWwTDPplNnLvVZejkvGgVyXx&quot;\n    &quot;NEenHrRhNZzXxnwWWwYkNFfnmITtinIiNqQaGgQZqGgQRrBvkKvVJKkjVfiIQezZpPQOKkKqQo&quot;\n    &quot;EAIiDdaGgYyoOeOtQKknhHNhHrjJfFBfFHWwCchCnOGgLljJadMmDtCcDdGgeEnNkCcKdAAmDd&quot;\n    &quot;iIMZzKkWQquOoSsxXOogGXAaQqxoOhHEenNnqlLcIcCAlLHhtTzBbyYDdZfFtTzGgZYyEeSFPx&quot;\n    &quot;XpaAgGbdWwSsyYYJpHgjJGhVvHhPEexyYMmkSsnRrLlNxXVVytTeEbBYTtiIAaQLlkKqjJOovv&quot;\n    &quot;eGgEfFIpPizZimFZzJoODDdJjWdGgDgGRnkDdByYbKDfuUQqTJwWFPIIiXWbzZByaAYwWxXUuI&quot;\n    &quot;iSsUsCAOoWwaWwcSyeEYLlTWwlLreBMKkrRLlakKAfNnoOjJUiIpPCcrRQqvZzgGVwWpRnNRUL&quot;\n    &quot;AmMBzZPpbVWwXwWJRBbrEesSHKkhjJqQKmDrRAagjJkweEWKQuUGvVmAaMHpPhJvVHcCekKdbB&quot;\n    &quot;KcCyFfAaAaYSftXxDkKrKeEcCWwMmkVsRrStEBmMovVMmdDOzxGMmKpgDdGHcChaAIrRFqbBbB&quot;\n    &quot;dDsSBbQqDrCcRIIiqQiiIxXdvVvavVmlLMgZzsSEegGGTJjMHRrhmYJjyFftTtsfFSCcsnlLiO&quot;\n    &quot;oIKkvwjJWgGmMOoRrVVvfYyrQqROoZzFRgGvTgxKBbNnkbxXBBbOoUXxubBTtgnNGXYyJjWSbB&quot;\n    &quot;iIsWEecPpFZSsSzZWHhSeEJjJCcjsDdzZcnnVAakeERUunNQUuqRrwNSeEHhwWQqQRrEuUfFId&quot;\n    &quot;DBkKbNUungGoOwoOWQqlLEQiIqXgtTEApfFMiIjPDdpRrsOfFlLoWwKvVlByYbVmfFMMmZcvbB&quot;\n    &quot;VIiTtLlLbBCcHXxoOhhpPShyLtTOolYOpOQiIWTtxjJJjTtyFfYAaAEeQqSszIiZaRvVUurvVv&quot;\n    &quot;MmVYyqMYVtgcCGTvyXwAaqqQQdhdeEDCfFcRIiYoODdAarWwMmhnNjJJjeEpPICciMVcCvmhHG&quot;\n    &quot;gvEevVDxvGDSkKsnMXDWlLwWbDCcdUPpmFdDfDdMbBRrTKTtUQquCczZlLTtiKkIEeKwobBOWa&quot;\n    &quot;drRpPlHhJZfRrFzBbAOlqQLKkXxYyLSsMmIzZiDFfpPoOEeBOiIhIiOonDfOoOoqQDdWqQdBJj&quot;\n    &quot;AaIayCVjJvyYOAalLoyYOnNAaFnNvVflLMowWLlNnKkjJKkKknNCiIekIZzwEeWjJiHRrhIeET&quot;\n    &quot;iwWIOhHozDdvVvWwXxokxXKOQEbBajyYrqQpXNVvmMnxElgGTtLeGgiIHNncUuvDdbBOVqaTte&quot;\n    &quot;EAwWUuQqQqGPpOogGgLlhmMhHHqQowfFHhWXxcBbYyfopPEEeuUuuUxNsSAaAzZUufFdDaKWwb&quot;\n    &quot;BWwkqrTtRIZziiIgGCmMhHpPcKkBJjxEeDhbBHfFdAhHMmalLsoOIiSXyYSvVzoOlcCLUuhHWw&quot;\n    &quot;CHxXWwhLlVOovWKkwHmdDWKmMIeEANOouDpmMaAIhldDlLLDCcNLvVcCcyhoOHYRrMmCIiGlbB&quot;\n    &quot;XxtElLIYwWwWuUfzBbZTtgGbMCcqQtPpWgGkKwrWAaxXvVIiDhthlLgaAGHzZnNBbwWTHlCczh&quot;\n    &quot;HFZzfCczZZMKkmqQXFOofxEeGQqgLdAgdAaCIiXKkcCZzxjYyTtJITiISwWscEeCWaAOotTzRr&quot;\n    &quot;PpZtTlWwLmMpUuLtTtJjTNkKszZAMmaBtHcOhHLloCglLGcChTlWOohHzZIiITtCxLlPpfFtTK&quot;\n    &quot;LiEeMmeEUlLdDuuUoTtnNOfFRlSswzZcGgCamMfMmFHhbIiCugGvVtTUuPXxpUBbjJwWzIiEeZ&quot;\n    &quot;dVCuUcvkKGgyXxIiXxhHzZEeeKkcCXEeEXxzXxYyjJseESIlLEeVvIiKohHxXBvVboOGgQHhqg&quot;\n    &quot;GoNnQqXWbBwCcHHDdhRrYlLyYYgGyIJLAaXxlcCaYyisSDeEaAbBdMmoOvAaJcbBDdMxvVjJHD&quot;\n    &quot;zZABbJFfIijatTdOcChqQjJPqOoQpOozZVgfzZXeExUAaUyYXbPpBxuBbMmWwGfFDgGZzbFfBu&quot;\n    &quot;UsXxCDkKdYmMlbBEeLJjbhLlUuJjOoqZdDiItTOaeBbLlEhHJvVjXxKxvVVmUuxXMuUTtpZzPS&quot;\n    &quot;UuWcCfFFuUfoOgTtGUuJjgGkAajJKTEetsAagKcCbBdOhHoKhtBbwWrRTHIiDddIltfoOFgWwt&quot;\n    &quot;AAKKIUZkKzHjJduUDGgNnyYjJWwYdDyRrxGgpQgGqPmMVvXrRGgQqhKwnNWRUurbGgBaZQqRpP&quot;\n    &quot;rzAeEkxXEvnNpcMmnNeECPdDRrlCcxVvXxDPMmvVNLKuUxXFfIiklnofFgGWwOPpdDGgyYpPsS&quot;\n    &quot;YyuUhHTtFfOiIomOohHcyYCMoOhHmYZzyOgGaAOoYyohHMnNUuSfFsPxXpnmMNnGgLlNMmuwWZ&quot;\n    &quot;zKkZzMVzZqQcCjJRJjrWYHaAhVvywXvVaAMmMmHhUuJIiqQwuUWvJTssSHkvVKCchTUNAaDsDd&quot;\n    &quot;bBSdnXxRresStNnPLljTtfbsSkKEejJLbBOoLCuUcaARrlDdXGNngxHtThkKhbBjZzugGUUTCs&quot;\n    &quot;SgGckKHrRHhpbBzZzPeEeNLloOZzneEvrQqWwRyYNDdJjnRrTaAuUOiIocDdjJCNnCSgGDPGgp&quot;\n    &quot;dkmLUulVngGNMgGrFfjAykKHhlLuUoOAaetTElSxeIHhiWwpPHzZwWjJaAhGzZSWgGIGpPgsSi&quot;\n    &quot;LlEewcjJCUVvuBbAasSqAaQpPkuUUlLuKkXxEelLiNnMSsCcTtJjDdwWDdmBbIwWiyYDdpGrFY&quot;\n    &quot;dYyNnDBhHdDOobdPsqQSKsSdDHhkgGZztTiDmMjvVJjJTtdIsSFfphHOowkGSsgOsSbBRxzZNn&quot;\n    &quot;JjUTtuqQIiwvVBbJjVvfFEeNkKnoTtfFqfjJLlFQOtgGTPTEGgaACcHhDYytTtUuZlLHhpmuUM&quot;\n    &quot;KkWwTlRrXxMaAmeEQqyYXxLWwEMwWwWmBfFDdbGiITQqriIBbMnbBYJjMmSsbByolLmhHMWwHw&quot;\n    &quot;WhxKkXKOokkOoXxxXKwIiTyjJUWKkwuBbIxyYuUxytkKBbhHTdaABmLlwWMpPMyYTtmVveEbsM&quot;\n    &quot;mSFfMMmcCzZjMhHmPpKuzZUVvkSQqsiIXxuUMmKuUSshyYrRHPpXxongGMmilksSKLYyovVOIX&quot;\n    &quot;iIMmNnVvCGxXgVvrPpRpPcxXYFfyXdpPQqIioHoOqQhgNvVsSnGyYZAabBmPpyYzMmZGgwWMdD&quot;\n    &quot;oOCnaeEGDfFziaAXYyxICcWMlLcEetTUHhUuSsuuUfiIVAavFNvVnrRLIiliIduUnNLDhEeHzZ&quot;\n    &quot;NnXxdTtMGjJWwgDcyYejJshfFMLlmPpWXxRryYxBrRbUuDRrhHdNUujAaJAdDaCxvVXxSsXsSM&quot;\n    &quot;ENnZOSsqYynvVNBbMyYEeDdhEeHmBbblLVvqeEYyOobBuUCcCcCjJcSjJsuaAUMjJyYRrdPMUu&quot;\n    &quot;mYJjyxMihHIHhkpPPpKeXoOxEIigKkGgGmaAsylwWMmIMmNnCIfFiCJjMcCmDYyUKRrkWwRrsS&quot;\n    &quot;hHIhHFWxNnPpuRrMMmmMUAaumXlLHhKSsvVSskxUkdDKqBbQpSstUuTvVokKOPkKrUuRgGJZvV&quot;\n    &quot;tTbsbBvVUuHhhHxUWwiIhHFfKkumMjAatMmlLKkiDfdDLkKAaevVZzZzpVvGgboOBPEliInKiI&quot;\n    &quot;STtXqnNQxBNnVvbWwBboOwERcnNzfFZzhFfHPrUUuuRpDDHhddRrXFNncCoEWwEeRzEeEOoCqQ&quot;\n    &quot;cTjJtUuKkFfeOZzRrIDdGZzkKgUpVvPGgvVHhuotTLlHAahAaOoxXlUuLYyBViouUOIvbPpWuU&quot;\n    &quot;uROorUiIuLdDQqTtLlVvXxBOocCkjJKzZGgJjbBDdUDPpdBbjJHhUqgMTQqiIvVtRtTrsRrzUu&quot;\n    &quot;hmMHvVHhglxXQgGjxXMmyLlYlLPpyYJqJfFjhgGsSxXrWwFfyYLlVvqdTtxXeEgGihHfFIIitT&quot;\n    &quot;jJpPwWBbYySsbBrRDjJgGNnfFvxXgGfwWSsFSsjyYJVaAtxXfeEeEkKaNnUfFuvOosGgSjYsSa&quot;\n    &quot;AnNBfFvrZPpwWnNTtSsgMmGgqQTtRrMmwPpSsWhRrvViEeIHhHHhqaAQGJjzkKppPBbPZpaAZz&quot;\n    &quot;MmUuWgsVvmMcCgGLlbbdDBdEeGgDbvVhAaHdDGgBBlWprRPEewEsSYyEepPeEeLSrRuUIWwXxi&quot;\n    &quot;zsQpPkKqfFSeEUdDZlHhLNnzuPWwcCnNmMJjrRkWLqQJjYyixXvVIVvPpOcQqjoOiEeIeEJCoZ&quot;\n    &quot;hyYpgxUVvugjJGXbBkKLqQlETBuUbteUroOVvRBbiIdDZWwznNBGgDdsSbuNvVsSIieKiIGgjJ&quot;\n    &quot;dPoOpWwDyfFpPYIikdDTtpPEaUuAIizZUpPuYsSCcYyIFvNOoutTUfFyYHhWxXqQoyYOwaAvVy&quot;\n    &quot;YKkngGJjmGHAaiICcSRApPJjeEaSsiIeIiEdDrsQHhWwKkYyuUscCFVvfSAwoOyYWaYyeEUnNC&quot;\n    &quot;BbsLIilSvZNnzUuVuUpqQZzLlPcCsScpEBbNdDnegBbTtGPyOoYyYPvawDdWtJjlkKQlLKkhHq&quot;\n    &quot;WwSsLlfjPpKkJFjJRrEeHvFfVEQKkqdDJjXLhHlTtJjxLlWtRrTIlLiZhJjiIHzwxXjJKcMyYm&quot;\n    &quot;CCcHhksSLlenNWRrwScZzCDdsrqQRgGzPsSpBALlFfacCEzZOomMiIejGgtTJoOlLBbsSbzZpP&quot;\n    &quot;BZzTtJWwjLSslbBMsqQSsqGgQGgSqlayYeEAQFNnfqhHJjPpAadDvEeVcCbBpPKgGoOkRQqrer&quot;\n    &quot;REGwWfZzFeEgwmMZpPIiUWwuAazpbBvVqQPtTWStEeTvVaAskXRrxHhKbNnqQBIpPiebIgGasS&quot;\n    &quot;ACciEwOoSsWiIZzeBMcCmpCcPEFEefrRCZzcVqAaQJjvJjKZznaAaANAakoOPpbJjBDUvVudDd&quot;\n    &quot;YsSyIiBbZWSAaAaHhValLAwWAavLlHxXheEeEVixXjJIEyYxXiIxkKwvVRrOorRWwWQlLqLlUu&quot;\n    &quot;sSJjbBhlLZgGzGgncPpCNjlXKkxLoORrJHnPVEekKvpNWqKvVTJjtEuiIUwWwWCcyYPWwpcCji&quot;\n    &quot;ITtYcCyECceJjpnNPJBkhHfBbzkKMDHhdmJHhjtTFfyJjHhdUujyYXxJKkQqDMwWuUmmukKUZz&quot;\n    &quot;jJuUHOoOoDdqQqQaAUuyYrHhRkKbBWwhazloiIhHRrOLPpDNnZZzLzgGZYNnqQylfFaAeEiICc&quot;\n    &quot;zdmoOLeElmMnWLlwvvrRVaAVPsSqQpuUzYyRrZBboOKkgGTNGgntMmssSbtLfmMFciICdDloOT&quot;\n    &quot;fYyPpSNFfnWwiuUIBwNnAaWkZUuIiQqzKXxvoOtTmMbGgcTYydDRHtThrQqlLwAaalLALAalvV&quot;\n    &quot;XYNnNPpuUKkNLlpPkKnhtTHtTXxlEnNeAHhaUuVAavLHhBbzTtFfZGBbaAJjvEeVxXZzgKHhkg&quot;\n    &quot;pPxXPfFeEpGcCCDdDdreJjErRHhxBbzZXxXCaAyYclLDvPpVtTdCkDdJjKmmMlIiLMmMKkYXmM&quot;\n    &quot;QVvXxUuREeJVvqQLlBVvZzbjOGgSSsoOsGgHheEdDoRtTrXxWoOwOjJoYyMmVvAaFBbfEeTtqK&quot;\n    &quot;zZkIEepPNuUnEeGbBlLcFrEeREwWqhHQmOoMXxeAQPjJpJjCcoBbYypNYynWwIiaVvAcjJeEAp&quot;\n    &quot;PRrNnICcxXiaANnakKEehHRrgGCPoOsEeStTdfFDpPAaNnNIiTtZznPoOpiISsXxjIMpPmtcCT&quot;\n    &quot;ieIHhSlLgBbGglLLlheEtspPxXSTTrRlUDdupyYYyhHUNnFfpiIxXTMmtfXxxXFPeEmxXVKkfF&quot;\n    &quot;vCcrWLVvlwRqRrQFfPVvFfFlLZzdDfpHhbBWwyxQqXZmMVgbBGvlUuLxXYyzAabBCkQqZqQiIm&quot;\n    &quot;RnNrRhcCHrMpPQIiHDdhqdqQMmigGrRIiTtlLZwWYyEPpekHOohVvKDdRfSspiIPZLlzFrNnmw&quot;\n    &quot;WtTMnNrReXxifFIsSwuUWEOYyKrRkfrfcCFtTCcwNSsnWJOojOUXsSxBbjXxjJJuFfYMmCcyGt&quot;\n    &quot;WwTbBLnNyYlOohHSsLlgSDdoOHhhHsMQqYytTHtTjJOoLlhKkowWUbBTtsWwSuNnLlxXrROcyY&quot;\n    &quot;ZfFzvVCZzHhqHhNeiIckKVvuUuJjiIJjUXxzZCEGHiIyYhgDsSHLlfFUuhrNnRaApAsSaPXxdk&quot;\n    &quot;dDGgKzZntWwTkKbpvVuUGgPQcCqUHhuWwNnuUEelLYVvhHyBvGgVrRBbreEWwRsiIHYJjAIiat&quot;\n    &quot;DdaATWwyOohEdmMDYyTtYyDdJKkKGgkXLFflwEeWUuGgfVqQvBbFYyeEtrRCTHhvVTtpPtdMmD&quot;\n    &quot;iINnrHhQqfFrcCRdQqkCmMtTYyeEcGWwtTlgSrDdVvRsGgaARrOAaTtEjJQHhqeoWffFFuUwRJ&quot;\n    &quot;tTcuUCjruUmCcLjfFVhHXxrsSRnNuUVvaTtBnRrNiIbBbAxBbpPGuUgBbXbByYqQkKqQaDdALl&quot;\n    &quot;IiprTtROoPwDdGbBpPIigWxCcCtTcImSsYyMJjuJjUmMpPtMmhHpPTGgjJpPaAcHhCiTfFEcCe&quot;\n    &quot;tVwWvKTtkKkiItKkIiyYQqOoTVxXgGJXsSxWwrREeJzZjpRoOrPnNjvoOqQruURNnhHmJjMKkX&quot;\n    &quot;FOMmofFfNnbBZzJjpwWBbxXPYytTzZuUYEeyraARNnpKVSsvkFfVvAQnNqBbitTfFgmMGIoOPp&quot;\n    &quot;hHaTtTtMidDIWwRDdrRrcCvVqQqyYmeEMWeErRwcCXxBbryYKkRxXRHhmeEMmsSMSsruUFAFfm&quot;\n    &quot;tTMabIilLFWRrwfkIiYyeEPpKfFGgFffFTtBaAQLEelPyVvYpXlLPpYyxqxXVgGwIiWkLlKfkK&quot;\n    &quot;zZFZzwWVwWLlvvgUDduaAdWwDWDdwDdDqQdsSlLvVvVqQCcuUkKsSzZGjJoOfMOojJmlLjqQJj&quot;\n    &quot;JQtTVvapPCcAJXxjXxerRhHESsmxJjnNWhHyYwXOoSsYyxXeSsxXZzZzLlhXxHEGjJgjiIJGha&quot;\n    &quot;AsbBvVeuUFzZfZzESgaZzAzzZvVZbAaBGtTHrGgRrRBhHnaANbcsxXSApPonNObmMBanNtTkKY&quot;\n    &quot;WwaArKuUmMoOyYKkkbBWwfFRrRTtGgTpPeEtsWNNnnRroOjrRJwNcDduUCnORBbrGguUoZzsSs&quot;\n    &quot;SUNnuAaPpIYysSiNnhHdDFfkKSsQCXxaAEUuecxrHkKMmhHaAhQqtxXaAOHGghZTtzoTFfOoiI&quot;\n    &quot;cChHnNUmMuYyRJjdMmDPsSXxTtppPqQJjlLZCcHhqQgGzXPTtpMbBmqqQQAuUTdrRDnNxXWwtv&quot;\n    &quot;VYyiIIigGOoVvYyHhagGZzKkqVvrlLdDROGgovVKpPkBbwWOotTOrRgGWwHhfFKkgTtGWwNnwi&quot;\n    &quot;UiIuIQDdKkSsSsYQOoVvHhaABbsSDGguUsSCzZcIxXidziYyrrRRIqrRGgQWwMjJmFfZJjnNhq&quot;\n    &quot;QmiIYyAaMhnNHiInNMyqQkKIGgiYQXxqoOTpPtmyAaYpPqcbBCQqXxpPQExXexXiIHLzZSswWl&quot;\n    &quot;LlJUugGjoOrRJjqYyrxXRkoObFfDdBWwKkkKnBgGUubsSNqQpkcCKgGPdDgfFGxuUmCcfFmMSB&quot;\n    &quot;LlDdPRrkzZKIigGfNiInFYyGtTbBgnsSzZhHoONgOoGbBTFfrRzZFfgQqGyYmsSMtnNCwpPWwW&quot;\n    &quot;rQqRcoOBbhnNHjJpTtTtdbBfFSsDEebfFLlBbmGTtgMHhaAhHS&quot;;\n\nint main() {\n\n  int64_t input_len = sizeof(input) - 1;\n  char *s = input;\n\n  int64_t i = 0;\n  while (i &lt; input_len) {\n    if (abs(s[i] - s[i + 1]) == 32) {\n      memmove(s + i, s + i + 2, input_len - i - 2);\n      input_len -= 2;\n      i = i &gt; 0 ? i - 1 : 0;\n    } else\n      i++;\n  }\n\n  printf(&quot;`%zu`\\n&quot;, input_len);\n} The x64 implementation The full code BITS 64\nCPU X64\n\n%define SYSCALL_EXIT 60\n%define SYSCALL_WRITE 1\n\nsection .data\n\n\nprefix: db 1\ninput: db &quot;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&quot;\n;input: db &quot;dabAcCaCBAcCcaDA&quot;\nstatic input:data\n\n%define input_len 50000\n;%define input_len 16\n\nsection .text\n\nexit:\nstatic exit:function\n  mov rax, SYSCALL_EXIT\n  mov rdi, 0\n  syscall\n\nwrite_int_to_stdout:\nstatic write_int:function\n  push rbp\n  mov rbp, rsp\n\n  sub rsp, 32\n\n  %define ARG0 rdi\n  %define N rax\n  %define BUF rsi\n  %define BUF_LEN r10\n  %define BUF_END r9\n\n  lea BUF, [rsp+32]\n  mov BUF_LEN, 0\n  lea BUF_END, [rsp]\n  mov N, ARG0\n\n  .loop:\n    mov rcx, 10 ; Divisor.\n    mov rdx, 0 ; Reset rem.\n    div rcx ; rax /= rcx\n\n    add rdx, \'0\' ; Convert to ascii.\n\n    ; *(end--) = rem\n    dec BUF_END\n    mov [BUF_END], dl\n    \n    inc BUF_LEN\n\n    cmp N, 0\n    jnz .loop\n\n  mov rax, SYSCALL_WRITE\n  mov rdi, 1\n  mov rsi, BUF_END\n  mov rdx, BUF_LEN\n  syscall\n\n\n  %undef ARG0\n  %undef N\n  %undef BUF\n  %undef BUF_LEN\n  %undef BUF_END\n\n  add rsp, 32\n  pop rbp\n  ret\n\nsolve:\nstatic solve:function\n  push rbp\n  mov rbp, rsp\n\n  %define INPUT_LEN r10\n  %define CURRENT r9\n  %define NEXT r11\n  %define REMAINING_COUNT rax\n  %define END r8\n\n  lea CURRENT, [input] \n  lea NEXT, [input + 1] \n  mov INPUT_LEN, input_len\n  mov REMAINING_COUNT, INPUT_LEN\n  lea END, [input]\n  add END, INPUT_LEN\n  \n\n.loop:\n  movzx dx, BYTE [CURRENT]\n  movzx cx, BYTE [NEXT]\n  sub dx, cx\n  imul dx, dx\n\n  mov rcx, 32*32\n\n  cmp rdx, rcx\n  jnz .else\n  .then:\n    mov BYTE [CURRENT], 0\n    mov BYTE [NEXT], 0\n\n    sub REMAINING_COUNT, 2\n\n    .reverse_search:\n    dec CURRENT\n    mov dl, [CURRENT]\n    cmp dl, 0\n    jz .reverse_search\n\n\n    jmp .endif\n  .else:\n    mov CURRENT, NEXT\n  .endif:\n\n  inc NEXT\n  cmp NEXT, END\n  jl .loop\n\n  %undef INPUT_LEN\n  %undef CURRENT\n  %undef NEXT\n  %undef REMAINING_COUNT\n  %undef END\n\n\n  pop rbp\n  ret\n\nglobal _start\n_start:\n  call solve\n\n  mov rdi, rax\n  call write_int_to_stdout\n\n  call exit ",
titles:[
{
title:"The new solution",
slug:"the-new-solution",
offset:1408,
},
{
title:"The x86_64 implementation",
slug:"the-x86-64-implementation",
offset:3964,
},
{
title:"Benchmarking",
slug:"benchmarking",
offset:8609,
},
{
title:"Learnings",
slug:"learnings",
offset:9337,
},
{
title:"Appendix: The full code",
slug:"appendix-the-full-code",
offset:10404,
},
{
title:"The old C implementation",
slug:"the-old-c-implementation",
offset:10428,
},
{
title:"The x64 implementation",
slug:"the-x64-implementation",
offset:72423,
},
],
},
{
html_file_name:"wayland_from_scratch.html",
title:"Learn Wayland by writing a GUI from scratch",
text:"Discussions: Hacker News , Lobsters . Wayland is all the rage those days. Distributions left and right switch to it, many readers of my previous article on writing a X11 GUI from scratch in x86_64 assembly asked for a follow-up article about Wayland, and I now run Wayland on my desktop. So here we go, let\'s write a (very simple) GUI program with Wayland, without any libraries, this time in C. Here is what we are working towards: We display the Wayland logo in its own window (we can see the mountain wallpaper in the background since we use a fixed size buffer). It\'s not quite Visual Studio yet, I know, but it\'s a good foundation for more in future articles, perhaps. Why not in assembly again you ask? Well, the Wayland protocol has some peculiarities that necessitate the use of some C standard library macros to make it work reliably on different platforms (Linux, FreeBSD, etc): namely, sending a file descriptor over a UNIX socket. Maybe it could be done in assembly, but it would be much more tedious. Also, the Wayland protocol is completely asynchronous by nature, whereas the X11 protocol was more of a request-(maybe) response chatter, and as such, we have to keep track of some state in our program, and C makes it easier. Now, if you want to follow along and translate the C snippets into assembly, go for it, it is doable, just tedious. If you spot an error, please open a Github issue ! What do we need? Not much: We\'ll use C99 so any C compiler of the last 20 years will do. Having a Wayland desktop to test the application will also greatly help. Note that I have only run it on Linux; it should work (meaning: compile and run) on other platforms running Wayland such as FreeBSD, it\'s just that I have not tried. Note that the code in this article has not been written in the most robust way, it simply exits when things are not how they should be for example. So, not production ready, but still a good learning resource and a good foundation for more. Wayland basics Wayland is a protocol specification for GUI applications (and more), in short. We will write the client side, while the server side is a compositor which understands our protocol. If you have a Wayland desktop right now, a Wayland compositor is already running so there is nothing to do. Much like X11, a client opens a UNIX socket, sends some commands in a specific format (which are different from the X11 ones), to open a window and the server can also send messages to notify the client to resize the window, that there is some keyboard input, etc. It\'s important to note that contrary to X11, in Wayland, the client only has access to its own window. It is also interesting to note that Wayland is quite a limited protocol and any GUI will have to use extension protocols. Most client applications use libwayland which is a library composed of C files that are autogenerated from a XML file describing the protocol.\nThe same goes for extension protocols: they simply are one XML file that is turned into C files, which are then compiled and linked to a GUI application. Now, we will not do any of this: we will instead write our own serialization and deserialization functions, which is really not a lot of work as you will see. There are many advantages: No need to link to external libraries: no build system complexities, no dynamic linking issues, and so on. We do not have to use the callback system that libwayland requires. We can use the I/O mechanism we wish to listen to incoming messages: blocking, poll , select , epoll , io_uring , kqueue on some systems, etc. Here, we will use blocking calls for simplicity but the world is your oyster. Easy troubleshooting: 100% of the code is our own. No XML The protocols we will use are stable so the numeric values on the wire should not change underneath us, but in the unlikely event they do, we simply have to fix them in our code and compile again. So at this point you might be thinking: this is going to be so much work! Well, not really. Here are all of the Wayland protocol numeric values we will need, including the extension protocols: static const uint32_t wayland_display_object_id = 1;\nstatic const uint16_t wayland_wl_registry_event_global = 0;\nstatic const uint16_t wayland_shm_pool_event_format = 0;\nstatic const uint16_t wayland_wl_buffer_event_release = 0;\nstatic const uint16_t wayland_xdg_wm_base_event_ping = 0;\nstatic const uint16_t wayland_xdg_toplevel_event_configure = 0;\nstatic const uint16_t wayland_xdg_toplevel_event_close = 1;\nstatic const uint16_t wayland_xdg_surface_event_configure = 0;\nstatic const uint16_t wayland_wl_display_get_registry_opcode = 1;\nstatic const uint16_t wayland_wl_registry_bind_opcode = 0;\nstatic const uint16_t wayland_wl_compositor_create_surface_opcode = 0;\nstatic const uint16_t wayland_xdg_wm_base_pong_opcode = 3;\nstatic const uint16_t wayland_xdg_surface_ack_configure_opcode = 4;\nstatic const uint16_t wayland_wl_shm_create_pool_opcode = 0;\nstatic const uint16_t wayland_xdg_wm_base_get_xdg_surface_opcode = 2;\nstatic const uint16_t wayland_wl_shm_pool_create_buffer_opcode = 0;\nstatic const uint16_t wayland_wl_surface_attach_opcode = 1;\nstatic const uint16_t wayland_xdg_surface_get_toplevel_opcode = 1;\nstatic const uint16_t wayland_wl_surface_commit_opcode = 6;\nstatic const uint16_t wayland_wl_display_error_event = 0;\nstatic const uint32_t wayland_format_xrgb8888 = 1;\nstatic const uint32_t wayland_header_size = 8;\nstatic const uint32_t color_channels = 4; So, not that much! Opening a socket The first step is opening a UNIX domain socket. Note that this step is exactly the same as for X11, save for the path of the socket. Also, X11 is designed to be used over the network so it does not have to be a UNIX domain socket, on the same machine - but everybody does so on their desktop machine anyway. To craft the socket path, we follow these simple steps: If $WAYLAND_DISPLAY is set, attempt to connect to $XDG_RUNTIME_DIR/$WAYLAND_DISPLAY Otherwise, attempt to connect to $XDG_RUNTIME_DIR/wayland-0 Otherwise, fail Here goes, along with two utility macros we\'ll use everywhere: #define cstring_len(s) (sizeof(s) - 1)\n\n#define roundup_4(n) (((n) + 3) &amp; -4)\n\nstatic int wayland_display_connect() {\n  char *xdg_runtime_dir = getenv(&quot;XDG_RUNTIME_DIR&quot;);\n  if (xdg_runtime_dir == NULL)\n    return EINVAL;\n\n  uint64_t xdg_runtime_dir_len = strlen(xdg_runtime_dir);\n\n  struct sockaddr_un addr = {.sun_family = AF_UNIX};\n  assert(xdg_runtime_dir_len &lt;= cstring_len(addr.sun_path));\n  uint64_t socket_path_len = 0;\n\n  memcpy(addr.sun_path, xdg_runtime_dir, xdg_runtime_dir_len);\n  socket_path_len += xdg_runtime_dir_len;\n\n  addr.sun_path[socket_path_len++] = \'/\';\n\n  char *wayland_display = getenv(&quot;WAYLAND_DISPLAY&quot;);\n  if (wayland_display == NULL) {\n    char wayland_display_default[] = &quot;wayland-0&quot;;\n    uint64_t wayland_display_default_len = cstring_len(wayland_display_default);\n\n    memcpy(addr.sun_path + socket_path_len, wayland_display_default,\n           wayland_display_default_len);\n    socket_path_len += wayland_display_default_len;\n  } else {\n    uint64_t wayland_display_len = strlen(wayland_display);\n    memcpy(addr.sun_path + socket_path_len, wayland_display,\n           wayland_display_len);\n    socket_path_len += wayland_display_len;\n  }\n\n  int fd = socket(AF_UNIX, SOCK_STREAM, 0);\n  if (fd == -1)\n    exit(errno);\n\n  if (connect(fd, (struct sockaddr *)&amp;addr, sizeof(addr)) == -1)\n    exit(errno);\n\n  return fd;\n} In Wayland, there is no connection setup to do, such as sending some special messages, so there is nothing more to do. Creating a registry Now, to do anything useful, we want to create a registry: it is an object that allows us to query at runtime the capabilities of the compositor. In Wayland, to create an object, we simply send the right message followed by an id of our own. Ids should be unique so we simply increment a number each time we want to create a new resource. After this is done, we will remember this number to be able to refer to it in later messages: This is coincidentally our first message we send, so let\'s briefly go over the structure of a Wayland message. It is basically a RPC mechanism. All bytes are in the host endianness so there is nothing special to do about it: 4 bytes: The id of the resource (\'object\') we want to call a method on 2 bytes: The opcode of the method we want to call 2 bytes: The size of the message Depending on the method, arguments in their wire format follow The object id in this case is 1 , which is the singleton wl_display that already exists.\nThe method is: get_registry(u32 new_id) whose opcode we listed before.\nThe sole argument takes 4 bytes and is this incremental number we keep track of client-side.\nIt does not necessarily have to be incremental, but that\'s what libwayland does and also it\'s the easiest. For convenience and efficiency, we always craft the message on the stack and do not allocate dynamic memory. We first introduce a few utility functions to read and write parts of messages: static void buf_write_u32(char *buf, uint64_t *buf_size, uint64_t buf_cap,\n                          uint32_t x) {\n  assert(*buf_size + sizeof(x) &lt;= buf_cap);\n  assert(((size_t)buf + *buf_size) % sizeof(x) == 0);\n\n  *(uint32_t *)(buf + *buf_size) = x;\n  *buf_size += sizeof(x);\n}\n\nstatic void buf_write_u16(char *buf, uint64_t *buf_size, uint64_t buf_cap,\n                          uint16_t x) {\n  assert(*buf_size + sizeof(x) &lt;= buf_cap);\n  assert(((size_t)buf + *buf_size) % sizeof(x) == 0);\n\n  *(uint16_t *)(buf + *buf_size) = x;\n  *buf_size += sizeof(x);\n}\n\nstatic void buf_write_string(char *buf, uint64_t *buf_size, uint64_t buf_cap,\n                             char *src, uint32_t src_len) {\n  assert(*buf_size + src_len &lt;= buf_cap);\n\n  buf_write_u32(buf, buf_size, buf_cap, src_len);\n  memcpy(buf + *buf_size, src, roundup_4(src_len));\n  *buf_size += roundup_4(src_len);\n}\n\nstatic uint32_t buf_read_u32(char **buf, uint64_t *buf_size) {\n  assert(*buf_size &gt;= sizeof(uint32_t));\n  assert((size_t)*buf % sizeof(uint32_t) == 0);\n\n  uint32_t res = *(uint32_t *)(*buf);\n  *buf += sizeof(res);\n  *buf_size -= sizeof(res);\n\n  return res;\n}\n\nstatic uint16_t buf_read_u16(char **buf, uint64_t *buf_size) {\n  assert(*buf_size &gt;= sizeof(uint16_t));\n  assert((size_t)*buf % sizeof(uint16_t) == 0);\n\n  uint16_t res = *(uint16_t *)(*buf);\n  *buf += sizeof(res);\n  *buf_size -= sizeof(res);\n\n  return res;\n}\n\nstatic void buf_read_n(char **buf, uint64_t *buf_size, char *dst, uint64_t n) {\n  assert(*buf_size &gt;= n);\n\n  memcpy(dst, *buf, n);\n\n  *buf += n;\n  *buf_size -= n;\n} And we finally can send our first message: static uint32_t wayland_wl_display_get_registry(int fd) {\n  uint64_t msg_size = 0;\n  char msg[128] = &quot;&quot;;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), wayland_display_object_id);\n\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg),\n                wayland_wl_display_get_registry_opcode);\n\n  uint16_t msg_announced_size =\n      wayland_header_size + sizeof(wayland_current_id);\n  assert(roundup_4(msg_announced_size) == msg_announced_size);\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg), msg_announced_size);\n\n  wayland_current_id++;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), wayland_current_id);\n\n  if ((int64_t)msg_size != send(fd, msg, msg_size, MSG_DONTWAIT))\n    exit(errno);\n\n  printf(&quot;-&gt; wl_display@%u.get_registry: wl_registry=%u\\n&quot;,\n         wayland_display_object_id, wayland_current_id);\n\n  return wayland_current_id;\n} And by calling it, we have created our very first Wayland resource! From this point on, the utility functions to send Wayland messages ( wayland_* ) will not be included in the code snippets for brevity (but you will find all of the code at the end!), just because they all are similar to the one above. Shared memory: the frame buffer To avoid drawing  a frame in our application, and having to send all of the bytes over the socket to the compositor, there is a smarter approach: the buffer should be shared between the two processes, so that no copying is required. We need to synchronize the access between the two so that presenting the frame does not happen while we are still drawing it, and Wayland has us covered here. First, we need to create this buffer. We are going to make it easier for us by using a fixed size. Wayland is going to send us \'resize\' events, whenever the window size changes, which we will acknowledge and ignore. This is done here just to simplify a bit the article, obviously in a real application, you would resize the buffer. First, we introduce a struct that will hold all of the client-side state so that we remember which resources we have created so far. We also need a super simple state machine for later to track whether the surface (i.e. the \'frame\' data) should be drawn to, as mentioned: typedef enum state_state_t state_state_t;\nenum state_state_t {\n  STATE_NONE,\n  STATE_SURFACE_ACKED_CONFIGURE,\n  STATE_SURFACE_ATTACHED,\n};\n\ntypedef struct state_t state_t;\nstruct state_t {\n  uint32_t wl_registry;\n  uint32_t wl_shm;\n  uint32_t wl_shm_pool;\n  uint32_t wl_buffer;\n  uint32_t xdg_wm_base;\n  uint32_t xdg_surface;\n  uint32_t wl_compositor;\n  uint32_t wl_surface;\n  uint32_t xdg_toplevel;\n  uint32_t stride;\n  uint32_t w;\n  uint32_t h;\n  uint32_t shm_pool_size;\n  int shm_fd;\n  uint8_t *shm_pool_data;\n\n  state_state_t state;\n}; We use it so in main() : state_t state = {\n      .wl_registry = wayland_wl_display_get_registry(fd),\n      .w = 117,\n      .h = 150,\n      .stride = 117 * color_channels,\n  };\n\n  // Single buffering.\n  state.shm_pool_size = state.h * state.stride; The window is a rectangle, of width w and height h . We will use the color format xrgb8888 which is 4 color channels, each taking one bytes, so 4 bytes per pixel. This is one of the two formats that is guaranteed to be supported by the compositor per the specification. The stride counts how many bytes a horizontal row takes: w * 4 . And so, our buffer size for the frame is : w * h * 4 . We use single buffering again for simplicity and also because we want to display a static image. We could choose to use double or even triple buffering, thus respectively doubling or tripling the buffer size. The compositor is none the wiser - we would simply keep a counter client-side that increments each time we render a frame (and wraps around back to 0 when reaching the number of buffers), we would draw in the right location of this big buffer (i.e. at an offset), and attach the right part of the buffer to the surface.\nAll the Wayland calls would remain the same. Alright, time to really create this buffer, and not only keep track of its size: static void create_shared_memory_file(uint64_t size, state_t *state) {\n  char name[255] = &quot;/&quot;;\n  for (uint64_t i = 1; i &lt; cstring_len(name); i++) {\n    name[i] = ((double)rand()) / (double)RAND_MAX * 26 + \'a\';\n  }\n\n  int fd = shm_open(name, O_RDWR | O_EXCL | O_CREAT, 0600);\n  if (fd == -1)\n    exit(errno);\n\n  assert(shm_unlink(name) != -1);\n\n  if (ftruncate(fd, size) == -1)\n    exit(errno);\n\n  state-&gt;shm_pool_data =\n      mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);\n  assert((void*)-1 != state-&gt;shm_pool_data);\n  assert(state-&gt;shm_pool_data != NULL);\n  state-&gt;shm_fd = fd;\n} We use shm_open(3) to create a POSIX shared memory object, so that we later can send the corresponding file descriptor to the compositor so that the latter also has access to it. The flags mean: O_RDWR : Read-write. O_CREAT : If the file does not exist, create it. O_EXCL : Return an error if the shared memory object with this name already exists (we do not want that another running instance of the application gets by mistake the same memory buffer). We alternatively could use memfd_create(2) which spares us from crafting a unique path but this is Linux specific. We craft a unique, random path to avoid clashes with other running applications. Right after, we remove the file on the filesystem with shm_unlink to not leave any traces when the program finishes. Note that the file descriptor remains valid since our process still has the file open (there is a reference counting mechanism in the kernel behind the scenes). We then resize with ftruncate and memory map this file with mmap(2) , effectively allocating memory, with the MAP_SHARED flag to allow the compositor to also read this memory. Later, we will send the file descriptor over the UNIX domain socket as ancillary data to the compositor. Alright, we now have some memory to draw our frame to, but the compositor does not know of it yet. Let\'s tackle that now. Chatting with the compositor We are going to exchange messages back and forth over the socket with the compositor. Let\'s use plain old blocking calls in main like it\'s the 70\'s. We read as much as we can from the socket: while (1) {\n    char read_buf[4096] = &quot;&quot;;\n    int64_t read_bytes = recv(fd, read_buf, sizeof(read_buf), 0);\n    if (read_bytes == -1)\n      exit(errno);\n\n    char *msg = read_buf;\n    uint64_t msg_len = (uint64_t)read_bytes;\n\n    while (msg_len &gt; 0)\n      wayland_handle_message(fd, &amp;state, &amp;msg, &amp;msg_len);\n    }\n  } The read buffer very likely now contains a sequence of various messages, which we parse and handle with wayland_handle_message eagerly until the end of the buffer.\nThis might break if a message is spanning two different read buffers - a ring buffer would be more appropriate to handle this case gracefully, but again, for this article this is fine. wayland_handle_message reads the header part of every message as described in the beginning, and reacts to known opcodes and objects: static void wayland_handle_message(int fd, state_t *state, char **msg,\n                                   uint64_t *msg_len) {\n  assert(*msg_len &gt;= 8);\n\n  uint32_t object_id = buf_read_u32(msg, msg_len);\n  assert(object_id &lt;= wayland_current_id);\n\n  uint16_t opcode = buf_read_u16(msg, msg_len);\n\n  uint16_t announced_size = buf_read_u16(msg, msg_len);\n  assert(roundup_4(announced_size) &lt;= announced_size);\n\n  uint32_t header_size =\n      sizeof(object_id) + sizeof(opcode) + sizeof(announced_size);\n  assert(announced_size &lt;= header_size + *msg_len);\n\n  if (object_id == state-&gt;wl_registry &amp;&amp;\n      opcode == wayland_wl_registry_event_global) {\n      // TODO\n  }\n  // Following: Lots of `if (opcode == ...) {... } else if (opcode = ...) { ... } [...]`\n} Reacting to events: binding interfaces At this point we have sent one message to the compositor: wl_display@1.get_registry() thanks to our C function wayland_wl_display_get_registry .\nThe compositor responds with a series of events, listing the available global objects, such as shared memory support, extension protocols, etc. Each event contains the interface name, which is a string. Now, in the Wayland protocol, the string length gets padded to a multiple of four, so we have read those padding bytes as well. If we see a global object that we are interested in, we create one of this type, and record the new id in our state structure for later use. While we\'re at it, we also handle error events. If the compositor does not like our messages, it will complain with some useful error messages in there: if (object_id == state-&gt;wl_registry &amp;&amp;\n      opcode == wayland_wl_registry_event_global) {\n    uint32_t name = buf_read_u32(msg, msg_len);\n\n    uint32_t interface_len = buf_read_u32(msg, msg_len);\n    uint32_t padded_interface_len = roundup_4(interface_len);\n\n    char interface[512] = &quot;&quot;;\n    assert(padded_interface_len &lt;= cstring_len(interface));\n\n    buf_read_n(msg, msg_len, interface, padded_interface_len);\n    // The length includes the NULL terminator.\n    assert(interface[interface_len - 1] == 0);\n\n    uint32_t version = buf_read_u32(msg, msg_len);\n\n    printf(&quot;&lt;- wl_registry@%u.global: name=%u interface=%.*s version=%u\\n&quot;,\n           state-&gt;wl_registry, name, interface_len, interface, version);\n\n    assert(announced_size == sizeof(object_id) + sizeof(announced_size) +\n                                 sizeof(opcode) + sizeof(name) +\n                                 sizeof(interface_len) + padded_interface_len +\n                                 sizeof(version));\n\n    char wl_shm_interface[] = &quot;wl_shm&quot;;\n    if (strcmp(wl_shm_interface, interface) == 0) {\n      state-&gt;wl_shm = wayland_wl_registry_bind(\n          fd, state-&gt;wl_registry, name, interface, interface_len, version);\n    }\n\n    char xdg_wm_base_interface[] = &quot;xdg_wm_base&quot;;\n    if (strcmp(xdg_wm_base_interface, interface) == 0) {\n      state-&gt;xdg_wm_base = wayland_wl_registry_bind(\n          fd, state-&gt;wl_registry, name, interface, interface_len, version);\n    }\n\n    char wl_compositor_interface[] = &quot;wl_compositor&quot;;\n    if (strcmp(wl_compositor_interface, interface) == 0) {\n      state-&gt;wl_compositor = wayland_wl_registry_bind(\n          fd, state-&gt;wl_registry, name, interface, interface_len, version);\n    }\n\n    return;\n  } else if (object_id == wayland_display_object_id &amp;&amp; opcode == wayland_wl_display_error_event) {\n    uint32_t target_object_id = buf_read_u32(msg, msg_len);\n    uint32_t code = buf_read_u32(msg, msg_len);\n    char error[512] = &quot;&quot;;\n    uint32_t error_len = buf_read_u32(msg, msg_len);\n    buf_read_n(msg, msg_len, error, roundup_4(error_len));\n\n    fprintf(stderr, &quot;fatal error: target_object_id=%u code=%u error=%s\\n&quot;,\n            target_object_id, code, error);\n    exit(EINVAL);\n  } Remember: Since the Wayland protocol is a kind of RPC, we need to create the objects first before calling remote methods on them. In terms of robustness, we do not have guarantees that every feature (i.e.: interface) we need in our application will be supported by the compositor. It could be a good idea to bail if the interfaces we require are not present. Using the interfaces we created We can now call methods on the new interfaces to create more entities we will need, namely: A wl_surface A xdg_surface A xdg_toplevel The last two being entities from extension protocols, which is inconsequential in our implementation since we do not link against any libraries. This is just the same logic as the other messages and events from the core protocol. Once we have done that, the surface is setup, and we commit it, to signal to the compositor to atomically apply the changes to the surface. while (msg_len &gt; 0)\n      wayland_handle_message(fd, &amp;state, &amp;msg, &amp;msg_len);\n\n    if (state.wl_compositor != 0 &amp;&amp; state.wl_shm != 0 &amp;&amp;\n        state.xdg_wm_base != 0 &amp;&amp;\n        state.wl_surface == 0) { // Bind phase complete, need to create surface.\n      assert(state.state == STATE_NONE);\n\n      state.wl_surface = wayland_wl_compositor_create_surface(fd, &amp;state);\n      state.xdg_surface = wayland_xdg_wm_base_get_xdg_surface(fd, &amp;state);\n      state.xdg_toplevel = wayland_xdg_surface_get_toplevel(fd, &amp;state);\n      wayland_wl_surface_commit(fd, &amp;state);\n    }\n  } Reacting to events: ping/pong For some entities, the Wayland compositor will send us a ping message and expect a pong back to ensure our application is responsive and not deadlocked or frozen. We just have to add one more if to the long list of if s to handle each event from the compositor: if (object_id == state-&gt;xdg_wm_base &amp;&amp;\n             opcode == wayland_xdg_wm_base_event_ping) {\n    uint32_t ping = buf_read_u32(msg, msg_len);\n    printf(&quot;&lt;- xdg_wm_base@%u.ping: ping=%u\\n&quot;, state-&gt;xdg_wm_base, ping);\n    wayland_xdg_wm_base_pong(fd, state, ping);\n\n    return;\n  } Reacting to events: configure/ACK configure Akin to the previous ping/pong mechanism, we receive a configure event for the xdg_surface and we reply with a ack_configure message. This is an important milestone since from that point on, we can start rendering our frame! We thus advance our little state machine: if (object_id == state-&gt;xdg_surface &amp;&amp;\n             opcode == wayland_xdg_surface_event_configure) {\n    uint32_t configure = buf_read_u32(msg, msg_len);\n    printf(&quot;&lt;- xdg_surface@%u.configure: configure=%u\\n&quot;, state-&gt;xdg_surface,\n           configure);\n    wayland_xdg_surface_ack_configure(fd, state, configure);\n    state-&gt;state = STATE_SURFACE_ACKED_CONFIGURE;\n\n    return;\n  }  Rendering a frame: the red rectangle Once the configure/ack configure step has been completed, we can render a frame. To do so, we need to create two final entities: a shared memory pool ( wl_shm_pool ) and a wl_buffer if they do not exist yet. Finally, we fiddle with the pixel data anyway we want, remembering the color format we picked (XRGB8888), attach the buffer to the surface, and commit the surface. This acts as synchronization mechanism between the client and the compositor to avoid presenting a half-rendered frame. To sum up: The ack_configure event signals us that we can start rendering the frame We render the frame client-side by setting the pixel data to whatever we want We send the attach + commit messages to notify the compositor that the frame is ready to be presented We advance our state machine to avoid writing to the frame data while the compositor is presenting it So let\'s show a red rectangle as a warm-up. The alpha component is completely ignored as far as I can tell in this color format: if (state.state == STATE_SURFACE_ACKED_CONFIGURE) {\n      // Render a frame.\n      assert(state.wl_surface != 0);\n      assert(state.xdg_surface != 0);\n      assert(state.xdg_toplevel != 0);\n\n      if (state.wl_shm_pool == 0)\n        state.wl_shm_pool = wayland_wl_shm_create_pool(fd, &amp;state);\n      if (state.wl_buffer == 0)\n        state.wl_buffer = wayland_wl_shm_pool_create_buffer(fd, &amp;state);\n\n      assert(state.shm_pool_data != 0);\n      assert(state.shm_pool_size != 0);\n\n      uint32_t *pixels = (uint32_t *)state.shm_pool_data;\n      for (uint32_t i = 0; i &lt; state.w * state.h; i++) {\n        uint8_t r = 0xff;\n        uint8_t g = 0;\n        uint8_t b = 0;\n        pixels[i] = (r &lt;&lt; 16) | (g &lt;&lt; 8) | b;\n      }\n      wayland_wl_surface_attach(fd, &amp;state);\n      wayland_wl_surface_commit(fd, &amp;state);\n\n      state.state = STATE_SURFACE_ATTACHED;\n    } Result: Rendering a frame: The Wayland logo Let\'s render something more interesting. We download the Wayland logo , but we do not want to have to deal with a complicated format like PNG (because we then have to uncompress the image data with zlib or similar). We thus convert it offline to a simpler image format, PPM6, and then embed the raw pixel data in our code as a byte array, skipping over the first 15 bytes which are metadata: $ file wayland.png\nwayland.png: PNG image data, 117 x 150, 8-bit/color RGBA, non-interlaced\n$ convert wayland.png wayland.ppm\n$ file wayland.ppm\nwayland.ppm: Netpbm image data, size = 117 x 150, rawbits, pixmap\n$ xxd -s +15 -i wayland.ppm  &gt; wayland-logo.h\n$ sed -i \'s/wayland_ppm/wayland_logo/g\' wayland-logo.h The resulting C array created by xxd will be named after the input file i.e. wayland_ppm . We rename it with the last command to something more human-readable. The image is now in the RGB format (3 bytes per pixel), which we have to convert to the XRGB format (4 bytes per pixel). Our frame rendering loop becomes: #include &quot;wayland-logo.h&quot;\n\n[...]\n\n      for (uint32_t i = 0; i &lt; state.w * state.h; i++) {\n        uint8_t r = wayland_logo[i * 3 + 0];\n        uint8_t g = wayland_logo[i * 3 + 1];\n        uint8_t b = wayland_logo[i * 3 + 2];\n        pixels[i] = (r &lt;&lt; 16) | (g &lt;&lt; 8) | b;\n      } And finally we see the result. Tiled: Floating: Note: We handle the absolute minimum set of events coming from the compositor to make it work in a simple way. If your particular compositor sends more events, they will have to be read (and possibly ignored). Since the Wayland protocol uses a Tag-Length-Value (TLV) encoding, one can simply skip over &lt;length&gt; bytes if the opcode is unknown. But some events will demand a reply (e.g. ping/pong)! The end It was not that much work to go from zero to a working GUI application, albeit a simplistic one. Compared to X11, it was a bit more work, but not that much. The barrier of entry is higher but the concepts and architecture are more sound, it seems to me. The setup is a bit tedious but once this is done, we are in practice going to spend all of our time in the frame rendering code, and perhaps add support for a few additional events (we do not yet support keyboard or mouse events, for example, or animations, which would require us to notify the compositor that a region was \'damaged\' meaning modified, and needs re-rendering). Thus, I have the feeling that Wayland really goes out of the way once the initial scaffolding is done. As for the next steps, I would like to draw some text, and react to user input events. Maybe even port something like microui , which only needs a few drawing routines, to our application. Addendum: the full code Do not forget to generate wayland-logo.h with the aforementioned commands! Compile with: cc -std=c99 wayland.c -Ofast . The full code #define _POSIX_C_SOURCE 200112L\n#include &lt;assert.h&gt;\n#include &lt;errno.h&gt;\n#include &lt;fcntl.h&gt;\n#include &lt;inttypes.h&gt;\n#include &lt;stddef.h&gt;\n#include &lt;stdint.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;sys/mman.h&gt;\n#include &lt;sys/socket.h&gt;\n#include &lt;sys/time.h&gt;\n#include &lt;sys/un.h&gt;\n#include &lt;unistd.h&gt;\n\n#include &quot;wayland-logo.h&quot;\n\n#define cstring_len(s) (sizeof(s) - 1)\n\n#define roundup_4(n) (((n) + 3) &amp; -4)\n\nstatic uint32_t wayland_current_id = 1;\n\nstatic const uint32_t wayland_display_object_id = 1;\nstatic const uint16_t wayland_wl_registry_event_global = 0;\nstatic const uint16_t wayland_shm_pool_event_format = 0;\nstatic const uint16_t wayland_wl_buffer_event_release = 0;\nstatic const uint16_t wayland_xdg_wm_base_event_ping = 0;\nstatic const uint16_t wayland_xdg_toplevel_event_configure = 0;\nstatic const uint16_t wayland_xdg_toplevel_event_close = 1;\nstatic const uint16_t wayland_xdg_surface_event_configure = 0;\nstatic const uint16_t wayland_wl_display_get_registry_opcode = 1;\nstatic const uint16_t wayland_wl_registry_bind_opcode = 0;\nstatic const uint16_t wayland_wl_compositor_create_surface_opcode = 0;\nstatic const uint16_t wayland_xdg_wm_base_pong_opcode = 3;\nstatic const uint16_t wayland_xdg_surface_ack_configure_opcode = 4;\nstatic const uint16_t wayland_wl_shm_create_pool_opcode = 0;\nstatic const uint16_t wayland_xdg_wm_base_get_xdg_surface_opcode = 2;\nstatic const uint16_t wayland_wl_shm_pool_create_buffer_opcode = 0;\nstatic const uint16_t wayland_wl_surface_attach_opcode = 1;\nstatic const uint16_t wayland_xdg_surface_get_toplevel_opcode = 1;\nstatic const uint16_t wayland_wl_surface_commit_opcode = 6;\nstatic const uint16_t wayland_wl_display_error_event = 0;\nstatic const uint32_t wayland_format_xrgb8888 = 1;\nstatic const uint32_t wayland_header_size = 8;\nstatic const uint32_t color_channels = 4;\n\ntypedef enum state_state_t state_state_t;\nenum state_state_t {\n  STATE_NONE,\n  STATE_SURFACE_ACKED_CONFIGURE,\n  STATE_SURFACE_ATTACHED,\n};\n\ntypedef struct state_t state_t;\nstruct state_t {\n  uint32_t wl_registry;\n  uint32_t wl_shm;\n  uint32_t wl_shm_pool;\n  uint32_t wl_buffer;\n  uint32_t xdg_wm_base;\n  uint32_t xdg_surface;\n  uint32_t wl_compositor;\n  uint32_t wl_surface;\n  uint32_t xdg_toplevel;\n  uint32_t stride;\n  uint32_t w;\n  uint32_t h;\n  uint32_t shm_pool_size;\n  int shm_fd;\n  uint8_t *shm_pool_data;\n\n  state_state_t state;\n};\n\nstatic int wayland_display_connect() {\n  char *xdg_runtime_dir = getenv(&quot;XDG_RUNTIME_DIR&quot;);\n  if (xdg_runtime_dir == NULL)\n    return EINVAL;\n\n  uint64_t xdg_runtime_dir_len = strlen(xdg_runtime_dir);\n\n  struct sockaddr_un addr = {.sun_family = AF_UNIX};\n  assert(xdg_runtime_dir_len &lt;= cstring_len(addr.sun_path));\n  uint64_t socket_path_len = 0;\n\n  memcpy(addr.sun_path, xdg_runtime_dir, xdg_runtime_dir_len);\n  socket_path_len += xdg_runtime_dir_len;\n\n  addr.sun_path[socket_path_len++] = \'/\';\n\n  char *wayland_display = getenv(&quot;WAYLAND_DISPLAY&quot;);\n  if (wayland_display == NULL) {\n    char wayland_display_default[] = &quot;wayland-0&quot;;\n    uint64_t wayland_display_default_len = cstring_len(wayland_display_default);\n\n    memcpy(addr.sun_path + socket_path_len, wayland_display_default,\n           wayland_display_default_len);\n    socket_path_len += wayland_display_default_len;\n  } else {\n    uint64_t wayland_display_len = strlen(wayland_display);\n    memcpy(addr.sun_path + socket_path_len, wayland_display,\n           wayland_display_len);\n    socket_path_len += wayland_display_len;\n  }\n\n  int fd = socket(AF_UNIX, SOCK_STREAM, 0);\n  if (fd == -1)\n    exit(errno);\n\n  if (connect(fd, (struct sockaddr *)&amp;addr, sizeof(addr)) == -1)\n    exit(errno);\n\n  return fd;\n}\n\nstatic void buf_write_u32(char *buf, uint64_t *buf_size, uint64_t buf_cap,\n                          uint32_t x) {\n  assert(*buf_size + sizeof(x) &lt;= buf_cap);\n  assert(((size_t)buf + *buf_size) % sizeof(x) == 0);\n\n  *(uint32_t *)(buf + *buf_size) = x;\n  *buf_size += sizeof(x);\n}\n\nstatic void buf_write_u16(char *buf, uint64_t *buf_size, uint64_t buf_cap,\n                          uint16_t x) {\n  assert(*buf_size + sizeof(x) &lt;= buf_cap);\n  assert(((size_t)buf + *buf_size) % sizeof(x) == 0);\n\n  *(uint16_t *)(buf + *buf_size) = x;\n  *buf_size += sizeof(x);\n}\n\nstatic void buf_write_string(char *buf, uint64_t *buf_size, uint64_t buf_cap,\n                             char *src, uint32_t src_len) {\n  assert(*buf_size + src_len &lt;= buf_cap);\n\n  buf_write_u32(buf, buf_size, buf_cap, src_len);\n  memcpy(buf + *buf_size, src, roundup_4(src_len));\n  *buf_size += roundup_4(src_len);\n}\n\nstatic uint32_t buf_read_u32(char **buf, uint64_t *buf_size) {\n  assert(*buf_size &gt;= sizeof(uint32_t));\n  assert((size_t)*buf % sizeof(uint32_t) == 0);\n\n  uint32_t res = *(uint32_t *)(*buf);\n  *buf += sizeof(res);\n  *buf_size -= sizeof(res);\n\n  return res;\n}\n\nstatic uint16_t buf_read_u16(char **buf, uint64_t *buf_size) {\n  assert(*buf_size &gt;= sizeof(uint16_t));\n  assert((size_t)*buf % sizeof(uint16_t) == 0);\n\n  uint16_t res = *(uint16_t *)(*buf);\n  *buf += sizeof(res);\n  *buf_size -= sizeof(res);\n\n  return res;\n}\n\nstatic void buf_read_n(char **buf, uint64_t *buf_size, char *dst, uint64_t n) {\n  assert(*buf_size &gt;= n);\n\n  memcpy(dst, *buf, n);\n\n  *buf += n;\n  *buf_size -= n;\n}\n\nstatic uint32_t wayland_wl_display_get_registry(int fd) {\n  uint64_t msg_size = 0;\n  char msg[128] = &quot;&quot;;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), wayland_display_object_id);\n\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg),\n                wayland_wl_display_get_registry_opcode);\n\n  uint16_t msg_announced_size =\n      wayland_header_size + sizeof(wayland_current_id);\n  assert(roundup_4(msg_announced_size) == msg_announced_size);\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg), msg_announced_size);\n\n  wayland_current_id++;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), wayland_current_id);\n\n  if ((int64_t)msg_size != send(fd, msg, msg_size, 0))\n    exit(errno);\n\n  printf(&quot;-&gt; wl_display@%u.get_registry: wl_registry=%u\\n&quot;,\n         wayland_display_object_id, wayland_current_id);\n\n  return wayland_current_id;\n}\n\nstatic uint32_t wayland_wl_registry_bind(int fd, uint32_t registry,\n                                         uint32_t name, char *interface,\n                                         uint32_t interface_len,\n                                         uint32_t version) {\n  uint64_t msg_size = 0;\n  char msg[512] = &quot;&quot;;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), registry);\n\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg), wayland_wl_registry_bind_opcode);\n\n  uint16_t msg_announced_size =\n      wayland_header_size + sizeof(name) + sizeof(interface_len) +\n      roundup_4(interface_len) + sizeof(version) + sizeof(wayland_current_id);\n  assert(roundup_4(msg_announced_size) == msg_announced_size);\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg), msg_announced_size);\n\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), name);\n  buf_write_string(msg, &amp;msg_size, sizeof(msg), interface, interface_len);\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), version);\n\n  wayland_current_id++;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), wayland_current_id);\n\n  assert(msg_size == roundup_4(msg_size));\n\n  if ((int64_t)msg_size != send(fd, msg, msg_size, 0))\n    exit(errno);\n\n  printf(&quot;-&gt; wl_registry@%u.bind: name=%u interface=%.*s version=%u\\n&quot;,\n         registry, name, interface_len, interface, version);\n\n  return wayland_current_id;\n}\n\nstatic uint32_t wayland_wl_compositor_create_surface(int fd, state_t *state) {\n  assert(state-&gt;wl_compositor &gt; 0);\n\n  uint64_t msg_size = 0;\n  char msg[128] = &quot;&quot;;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), state-&gt;wl_compositor);\n\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg),\n                wayland_wl_compositor_create_surface_opcode);\n\n  uint16_t msg_announced_size =\n      wayland_header_size + sizeof(wayland_current_id);\n  assert(roundup_4(msg_announced_size) == msg_announced_size);\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg), msg_announced_size);\n\n  wayland_current_id++;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), wayland_current_id);\n\n  if ((int64_t)msg_size != send(fd, msg, msg_size, 0))\n    exit(errno);\n\n  printf(&quot;-&gt; wl_compositor@%u.create_surface: wl_surface=%u\\n&quot;,\n         state-&gt;wl_compositor, wayland_current_id);\n\n  return wayland_current_id;\n}\n\nstatic void create_shared_memory_file(uint64_t size, state_t *state) {\n  char name[255] = &quot;/&quot;;\n  for (uint64_t i = 1; i &lt; cstring_len(name); i++) {\n    name[i] = ((double)rand()) / (double)RAND_MAX * 26 + \'a\';\n  }\n\n  int fd = shm_open(name, O_RDWR | O_EXCL | O_CREAT, 0600);\n  if (fd == -1)\n    exit(errno);\n\n  assert(shm_unlink(name) != -1);\n\n  if (ftruncate(fd, size) == -1)\n    exit(errno);\n\n  state-&gt;shm_pool_data =\n      mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);\n  assert((void*)-1 != state-&gt;shm_pool_data);\n  assert(state-&gt;shm_pool_data != NULL);\n  state-&gt;shm_fd = fd;\n}\n\nstatic void wayland_xdg_wm_base_pong(int fd, state_t *state, uint32_t ping) {\n  assert(state-&gt;xdg_wm_base &gt; 0);\n  assert(state-&gt;wl_surface &gt; 0);\n\n  uint64_t msg_size = 0;\n  char msg[128] = &quot;&quot;;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), state-&gt;xdg_wm_base);\n\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg), wayland_xdg_wm_base_pong_opcode);\n\n  uint16_t msg_announced_size = wayland_header_size + sizeof(ping);\n  assert(roundup_4(msg_announced_size) == msg_announced_size);\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg), msg_announced_size);\n\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), ping);\n\n  if ((int64_t)msg_size != send(fd, msg, msg_size, 0))\n    exit(errno);\n\n  printf(&quot;-&gt; xdg_wm_base@%u.pong: ping=%u\\n&quot;, state-&gt;xdg_wm_base, ping);\n}\n\nstatic void wayland_xdg_surface_ack_configure(int fd, state_t *state,\n                                              uint32_t configure) {\n  assert(state-&gt;xdg_surface &gt; 0);\n\n  uint64_t msg_size = 0;\n  char msg[128] = &quot;&quot;;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), state-&gt;xdg_surface);\n\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg),\n                wayland_xdg_surface_ack_configure_opcode);\n\n  uint16_t msg_announced_size = wayland_header_size + sizeof(configure);\n  assert(roundup_4(msg_announced_size) == msg_announced_size);\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg), msg_announced_size);\n\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), configure);\n\n  if ((int64_t)msg_size != send(fd, msg, msg_size, 0))\n    exit(errno);\n\n  printf(&quot;-&gt; xdg_surface@%u.ack_configure: configure=%u\\n&quot;, state-&gt;xdg_surface,\n         configure);\n}\n\nstatic uint32_t wayland_wl_shm_create_pool(int fd, state_t *state) {\n  assert(state-&gt;shm_pool_size &gt; 0);\n\n  uint64_t msg_size = 0;\n  char msg[128] = &quot;&quot;;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), state-&gt;wl_shm);\n\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg), wayland_wl_shm_create_pool_opcode);\n\n  uint16_t msg_announced_size = wayland_header_size +\n                                sizeof(wayland_current_id) +\n                                sizeof(state-&gt;shm_pool_size);\n\n  assert(roundup_4(msg_announced_size) == msg_announced_size);\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg), msg_announced_size);\n\n  wayland_current_id++;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), wayland_current_id);\n\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), state-&gt;shm_pool_size);\n\n  assert(roundup_4(msg_size) == msg_size);\n\n  // Send the file descriptor as ancillary data.\n  // UNIX/Macros monstrosities ahead.\n  char buf[CMSG_SPACE(sizeof(state-&gt;shm_fd))] = &quot;&quot;;\n\n  struct iovec io = {.iov_base = msg, .iov_len = msg_size};\n  struct msghdr socket_msg = {\n      .msg_iov = &amp;io,\n      .msg_iovlen = 1,\n      .msg_control = buf,\n      .msg_controllen = sizeof(buf),\n  };\n\n  struct cmsghdr *cmsg = CMSG_FIRSTHDR(&amp;socket_msg);\n  cmsg-&gt;cmsg_level = SOL_SOCKET;\n  cmsg-&gt;cmsg_type = SCM_RIGHTS;\n  cmsg-&gt;cmsg_len = CMSG_LEN(sizeof(state-&gt;shm_fd));\n\n  *((int *)CMSG_DATA(cmsg)) = state-&gt;shm_fd;\n  socket_msg.msg_controllen = CMSG_SPACE(sizeof(state-&gt;shm_fd));\n\n  if (sendmsg(fd, &amp;socket_msg, 0) == -1)\n    exit(errno);\n\n  printf(&quot;-&gt; wl_shm@%u.create_pool: wl_shm_pool=%u\\n&quot;, state-&gt;wl_shm,\n         wayland_current_id);\n\n  return wayland_current_id;\n}\n\nstatic uint32_t wayland_xdg_wm_base_get_xdg_surface(int fd, state_t *state) {\n  assert(state-&gt;xdg_wm_base &gt; 0);\n  assert(state-&gt;wl_surface &gt; 0);\n\n  uint64_t msg_size = 0;\n  char msg[128] = &quot;&quot;;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), state-&gt;xdg_wm_base);\n\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg),\n                wayland_xdg_wm_base_get_xdg_surface_opcode);\n\n  uint16_t msg_announced_size = wayland_header_size +\n                                sizeof(wayland_current_id) +\n                                sizeof(state-&gt;wl_surface);\n  assert(roundup_4(msg_announced_size) == msg_announced_size);\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg), msg_announced_size);\n\n  wayland_current_id++;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), wayland_current_id);\n\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), state-&gt;wl_surface);\n\n  if ((int64_t)msg_size != send(fd, msg, msg_size, 0))\n    exit(errno);\n\n  printf(&quot;-&gt; xdg_wm_base@%u.get_xdg_surface: xdg_surface=%u wl_surface=%u\\n&quot;,\n         state-&gt;xdg_wm_base, wayland_current_id, state-&gt;wl_surface);\n\n  return wayland_current_id;\n}\n\nstatic uint32_t wayland_wl_shm_pool_create_buffer(int fd, state_t *state) {\n  assert(state-&gt;wl_shm_pool &gt; 0);\n\n  uint64_t msg_size = 0;\n  char msg[128] = &quot;&quot;;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), state-&gt;wl_shm_pool);\n\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg),\n                wayland_wl_shm_pool_create_buffer_opcode);\n\n  uint16_t msg_announced_size =\n      wayland_header_size + sizeof(wayland_current_id) + sizeof(uint32_t) * 5;\n  assert(roundup_4(msg_announced_size) == msg_announced_size);\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg), msg_announced_size);\n\n  wayland_current_id++;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), wayland_current_id);\n\n  uint32_t offset = 0;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), offset);\n\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), state-&gt;w);\n\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), state-&gt;h);\n\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), state-&gt;stride);\n\n  uint32_t format = wayland_format_xrgb8888;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), format);\n\n  if ((int64_t)msg_size != send(fd, msg, msg_size, 0))\n    exit(errno);\n\n  printf(&quot;-&gt; wl_shm_pool@%u.create_buffer: wl_buffer=%u\\n&quot;, state-&gt;wl_shm_pool,\n         wayland_current_id);\n\n  return wayland_current_id;\n}\n\nstatic void wayland_wl_surface_attach(int fd, state_t *state) {\n  assert(state-&gt;wl_surface &gt; 0);\n  assert(state-&gt;wl_buffer &gt; 0);\n\n  uint64_t msg_size = 0;\n  char msg[128] = &quot;&quot;;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), state-&gt;wl_surface);\n\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg), wayland_wl_surface_attach_opcode);\n\n  uint16_t msg_announced_size =\n      wayland_header_size + sizeof(state-&gt;wl_buffer) + sizeof(uint32_t) * 2;\n  assert(roundup_4(msg_announced_size) == msg_announced_size);\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg), msg_announced_size);\n\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), state-&gt;wl_buffer);\n\n  uint32_t x = 0, y = 0;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), x);\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), y);\n\n  if ((int64_t)msg_size != send(fd, msg, msg_size, 0))\n    exit(errno);\n\n  printf(&quot;-&gt; wl_surface@%u.attach: wl_buffer=%u\\n&quot;, state-&gt;wl_surface,\n         state-&gt;wl_buffer);\n}\n\nstatic uint32_t wayland_xdg_surface_get_toplevel(int fd, state_t *state) {\n  assert(state-&gt;xdg_surface &gt; 0);\n\n  uint64_t msg_size = 0;\n  char msg[128] = &quot;&quot;;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), state-&gt;xdg_surface);\n\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg),\n                wayland_xdg_surface_get_toplevel_opcode);\n\n  uint16_t msg_announced_size =\n      wayland_header_size + sizeof(wayland_current_id);\n  assert(roundup_4(msg_announced_size) == msg_announced_size);\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg), msg_announced_size);\n\n  wayland_current_id++;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), wayland_current_id);\n\n  if ((int64_t)msg_size != send(fd, msg, msg_size, 0))\n    exit(errno);\n\n  printf(&quot;-&gt; xdg_surface@%u.get_toplevel: xdg_toplevel=%u\\n&quot;,\n         state-&gt;xdg_surface, wayland_current_id);\n\n  return wayland_current_id;\n}\n\nstatic void wayland_wl_surface_commit(int fd, state_t *state) {\n  assert(state-&gt;wl_surface &gt; 0);\n\n  uint64_t msg_size = 0;\n  char msg[128] = &quot;&quot;;\n  buf_write_u32(msg, &amp;msg_size, sizeof(msg), state-&gt;wl_surface);\n\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg), wayland_wl_surface_commit_opcode);\n\n  uint16_t msg_announced_size = wayland_header_size;\n  assert(roundup_4(msg_announced_size) == msg_announced_size);\n  buf_write_u16(msg, &amp;msg_size, sizeof(msg), msg_announced_size);\n\n  if ((int64_t)msg_size != send(fd, msg, msg_size, 0))\n    exit(errno);\n\n  printf(&quot;-&gt; wl_surface@%u.commit: \\n&quot;, state-&gt;wl_surface);\n}\n\nstatic void wayland_handle_message(int fd, state_t *state, char **msg,\n                                   uint64_t *msg_len) {\n  assert(*msg_len &gt;= 8);\n\n  uint32_t object_id = buf_read_u32(msg, msg_len);\n  assert(object_id &lt;= wayland_current_id);\n\n  uint16_t opcode = buf_read_u16(msg, msg_len);\n\n  uint16_t announced_size = buf_read_u16(msg, msg_len);\n  assert(roundup_4(announced_size) &lt;= announced_size);\n\n  uint32_t header_size =\n      sizeof(object_id) + sizeof(opcode) + sizeof(announced_size);\n  assert(announced_size &lt;= header_size + *msg_len);\n\n  if (object_id == state-&gt;wl_registry &amp;&amp;\n      opcode == wayland_wl_registry_event_global) {\n    uint32_t name = buf_read_u32(msg, msg_len);\n\n    uint32_t interface_len = buf_read_u32(msg, msg_len);\n    uint32_t padded_interface_len = roundup_4(interface_len);\n\n    char interface[512] = &quot;&quot;;\n    assert(padded_interface_len &lt;= cstring_len(interface));\n\n    buf_read_n(msg, msg_len, interface, padded_interface_len);\n    // The length includes the NULL terminator.\n    assert(interface[interface_len - 1] == 0);\n\n    uint32_t version = buf_read_u32(msg, msg_len);\n\n    printf(&quot;&lt;- wl_registry@%u.global: name=%u interface=%.*s version=%u\\n&quot;,\n           state-&gt;wl_registry, name, interface_len, interface, version);\n\n    assert(announced_size == sizeof(object_id) + sizeof(announced_size) +\n                                 sizeof(opcode) + sizeof(name) +\n                                 sizeof(interface_len) + padded_interface_len +\n                                 sizeof(version));\n\n    char wl_shm_interface[] = &quot;wl_shm&quot;;\n    if (strcmp(wl_shm_interface, interface) == 0) {\n      state-&gt;wl_shm = wayland_wl_registry_bind(\n          fd, state-&gt;wl_registry, name, interface, interface_len, version);\n    }\n\n    char xdg_wm_base_interface[] = &quot;xdg_wm_base&quot;;\n    if (strcmp(xdg_wm_base_interface, interface) == 0) {\n      state-&gt;xdg_wm_base = wayland_wl_registry_bind(\n          fd, state-&gt;wl_registry, name, interface, interface_len, version);\n    }\n\n    char wl_compositor_interface[] = &quot;wl_compositor&quot;;\n    if (strcmp(wl_compositor_interface, interface) == 0) {\n      state-&gt;wl_compositor = wayland_wl_registry_bind(\n          fd, state-&gt;wl_registry, name, interface, interface_len, version);\n    }\n\n    return;\n  } else if (object_id == wayland_display_object_id &amp;&amp;\n             opcode == wayland_wl_display_error_event) {\n    uint32_t target_object_id = buf_read_u32(msg, msg_len);\n    uint32_t code = buf_read_u32(msg, msg_len);\n    char error[512] = &quot;&quot;;\n    uint32_t error_len = buf_read_u32(msg, msg_len);\n    buf_read_n(msg, msg_len, error, roundup_4(error_len));\n\n    fprintf(stderr, &quot;fatal error: target_object_id=%u code=%u error=%s\\n&quot;,\n            target_object_id, code, error);\n    exit(EINVAL);\n  } else if (object_id == state-&gt;wl_shm &amp;&amp;\n             opcode == wayland_shm_pool_event_format) {\n\n    uint32_t format = buf_read_u32(msg, msg_len);\n    printf(&quot;&lt;- wl_shm: format=%#x\\n&quot;, format);\n    return;\n  } else if (object_id == state-&gt;wl_buffer &amp;&amp;\n             opcode == wayland_wl_buffer_event_release) {\n    // No-op, for now.\n\n    printf(&quot;&lt;- xdg_wl_buffer@%u.release\\n&quot;, state-&gt;wl_buffer);\n    return;\n  } else if (object_id == state-&gt;xdg_wm_base &amp;&amp;\n             opcode == wayland_xdg_wm_base_event_ping) {\n    uint32_t ping = buf_read_u32(msg, msg_len);\n    printf(&quot;&lt;- xdg_wm_base@%u.ping: ping=%u\\n&quot;, state-&gt;xdg_wm_base, ping);\n    wayland_xdg_wm_base_pong(fd, state, ping);\n\n    return;\n  } else if (object_id == state-&gt;xdg_toplevel &amp;&amp;\n             opcode == wayland_xdg_toplevel_event_configure) {\n    uint32_t w = buf_read_u32(msg, msg_len);\n    uint32_t h = buf_read_u32(msg, msg_len);\n    uint32_t len = buf_read_u32(msg, msg_len);\n    char buf[256] = &quot;&quot;;\n    assert(len &lt;= sizeof(buf));\n    buf_read_n(msg, msg_len, buf, len);\n\n    printf(&quot;&lt;- xdg_toplevel@%u.configure: w=%u h=%u states[%u]\\n&quot;,\n           state-&gt;xdg_toplevel, w, h, len);\n\n    return;\n  } else if (object_id == state-&gt;xdg_surface &amp;&amp;\n             opcode == wayland_xdg_surface_event_configure) {\n    uint32_t configure = buf_read_u32(msg, msg_len);\n    printf(&quot;&lt;- xdg_surface@%u.configure: configure=%u\\n&quot;, state-&gt;xdg_surface,\n           configure);\n    wayland_xdg_surface_ack_configure(fd, state, configure);\n    state-&gt;state = STATE_SURFACE_ACKED_CONFIGURE;\n\n    return;\n  } else if (object_id == state-&gt;xdg_toplevel &amp;&amp;\n             opcode == wayland_xdg_toplevel_event_close) {\n    printf(&quot;&lt;- xdg_toplevel@%u.close\\n&quot;, state-&gt;xdg_toplevel);\n    exit(0);\n  }\n\n  fprintf(stderr, &quot;object_id=%u opcode=%u msg_len=%lu\\n&quot;, object_id, opcode,\n          *msg_len);\n  assert(0 &amp;&amp; &quot;todo&quot;);\n}\n\nint main() {\n  struct timeval tv = {0};\n  assert(gettimeofday(&amp;tv, NULL) != -1);\n  srand(tv.tv_sec * 1000 * 1000 + tv.tv_usec);\n\n  int fd = wayland_display_connect();\n\n  state_t state = {\n      .wl_registry = wayland_wl_display_get_registry(fd),\n      .w = 117,\n      .h = 150,\n      .stride = 117 * color_channels,\n  };\n\n  // Single buffering.\n  state.shm_pool_size = state.h * state.stride;\n  create_shared_memory_file(state.shm_pool_size, &amp;state);\n\n  while (1) {\n    char read_buf[4096] = &quot;&quot;;\n    int64_t read_bytes = recv(fd, read_buf, sizeof(read_buf), 0);\n    if (read_bytes == -1)\n      exit(errno);\n\n    char *msg = read_buf;\n    uint64_t msg_len = (uint64_t)read_bytes;\n\n    while (msg_len &gt; 0)\n      wayland_handle_message(fd, &amp;state, &amp;msg, &amp;msg_len);\n\n    if (state.wl_compositor != 0 &amp;&amp; state.wl_shm != 0 &amp;&amp;\n        state.xdg_wm_base != 0 &amp;&amp;\n        state.wl_surface == 0) { // Bind phase complete, need to create surface.\n      assert(state.state == STATE_NONE);\n\n      state.wl_surface = wayland_wl_compositor_create_surface(fd, &amp;state);\n      state.xdg_surface = wayland_xdg_wm_base_get_xdg_surface(fd, &amp;state);\n      state.xdg_toplevel = wayland_xdg_surface_get_toplevel(fd, &amp;state);\n      wayland_wl_surface_commit(fd, &amp;state);\n    }\n\n    if (state.state == STATE_SURFACE_ACKED_CONFIGURE) {\n      // Render a frame.\n      assert(state.wl_surface != 0);\n      assert(state.xdg_surface != 0);\n      assert(state.xdg_toplevel != 0);\n\n      if (state.wl_shm_pool == 0)\n        state.wl_shm_pool = wayland_wl_shm_create_pool(fd, &amp;state);\n      if (state.wl_buffer == 0)\n        state.wl_buffer = wayland_wl_shm_pool_create_buffer(fd, &amp;state);\n\n      assert(state.shm_pool_data != 0);\n      assert(state.shm_pool_size != 0);\n\n      uint32_t *pixels = (uint32_t *)state.shm_pool_data;\n      for (uint32_t i = 0; i &lt; state.w * state.h; i++) {\n        uint8_t r = wayland_logo[i * 3 + 0];\n        uint8_t g = wayland_logo[i * 3 + 1];\n        uint8_t b = wayland_logo[i * 3 + 2];\n        pixels[i] = (r &lt;&lt; 16) | (g &lt;&lt; 8) | b;\n      }\n      wayland_wl_surface_attach(fd, &amp;state);\n      wayland_wl_surface_commit(fd, &amp;state);\n\n      state.state = STATE_SURFACE_ATTACHED;\n    }\n  }\n} ",
titles:[
{
title:"What do we need?",
slug:"what-do-we-need",
offset:1407,
},
{
title:"Wayland basics",
slug:"wayland-basics",
offset:1976,
},
{
title:"Opening a socket",
slug:"opening-a-socket",
offset:5496,
},
{
title:"Creating a registry",
slug:"creating-a-registry",
offset:7607,
},
{
title:"Shared memory: the frame buffer",
slug:"shared-memory-the-frame-buffer",
offset:11846,
},
{
title:"Chatting with the compositor",
slug:"chatting-with-the-compositor",
offset:16659,
},
{
title:"Reacting to events: binding interfaces",
slug:"reacting-to-events-binding-interfaces",
offset:18485,
},
{
title:"Using the interfaces we created",
slug:"using-the-interfaces-we-created",
offset:21968,
},
{
title:"Reacting to events: ping/pong",
slug:"reacting-to-events-ping-pong",
offset:23130,
},
{
title:"Reacting to events: configure/ACK configure",
slug:"reacting-to-events-configure-ack-configure",
offset:23732,
},
{
title:"Rendering a frame: the red rectangle",
slug:"rendering-a-frame-the-red-rectangle",
offset:24457,
},
{
title:"Rendering a frame: The Wayland logo",
slug:"rendering-a-frame-the-wayland-logo",
offset:26383,
},
{
title:"The end",
slug:"the-end",
offset:28197,
},
{
title:"Addendum: the full code",
slug:"addendum-the-full-code",
offset:29128,
},
],
},
{
html_file_name:"roll_your_own_memory_profiling.html",
title:"Roll your own memory profiling: it\'s actually not hard",
text:"Discussions: /r/c_programming Or: An exploration of the pprof memory profiler and its textual format for fun and profit. Say that you are using a programming language where memory is manually managed, and you have decided to use a custom allocator for one reason or another, for example an arena allocator, and are wondering: How do I track every allocation, recording how many bytes were allocated and what was the call stack at that time? How much memory is my program using, and what is the peak use? How much memory does my program free? Is it all of it (are there leaks)? Which line of code in my function is allocating, and how much? I want a flamegraph showing allocations by function What to do? Mainstream allocators such as tcmalloc and jemalloc can provide us this information but we have lost this ability by using our own! Well, it turns out that this can all be achieved very simply without adding dependencies to your application, in ~100 lines of code (including lots of comments). I\'ll show one way and then explore other possibilities. And here are the results we are working towards: Profiling the memory usage of my micro-kotlin project. Showing which lines of code are allocating in a function. A flamegraph based on the previous data. The only requirement to make it all work is to be able to run a bit of code on each allocation. Another good reason to do this, is when the system\'s malloc comes with some form of memory profiling which is not suitable for your needs and you want something different/better/the same on every platform. If you spot an error, please open a Github issue ! Pprof Here is the plan: Each time there is an allocation in our program, we record information about it in an array. At the end of the program (or upon receiving a signal, a special TCP packet, whatever), we dump the information in the (original) pprof format, which is basically just a text file with one line per allocation (more details on that in a bit). We can then use the (original) pprof which is just a giant Perl script which will extract interesting information and most importantly symbolize (meaning: transform memory addresses into line/column/function/file information). I will showcase this approach with C code using an arena allocator. The full code can be found in my project micro-kotlin . But this can be done in any language since the pprof text format is so simple! Also, using arenas, we do not bother to free anything so the memory profiling part is even simpler. The original pprof written in Perl is not to be confused with the rewritten pprof in Go which offers a superset of the features of the original but based on a completely different and incompatible file format (protobuf)! The text format Here is the text format we want to generate: heap profile:    &lt;in use objects sum&gt;:  &lt;in use bytes sum&gt; [   &lt;space objects sum&gt;:  &lt;space bytes sum&gt;] @ heapprofile\n&lt;in use objects&gt;: &lt;in use bytes&gt; [&lt;space objects&gt;: &lt;space bytes&gt;] @ &lt;rip1&gt; &lt;rip2&gt; &lt;rip3&gt; [...]\n&lt;in use objects&gt;: &lt;in use bytes&gt; [&lt;space objects&gt;: &lt;space bytes&gt;] @ &lt;rip1&gt; &lt;rip2&gt; &lt;rip3&gt; [...]\n&lt;in use objects&gt;: &lt;in use bytes&gt; [&lt;space objects&gt;: &lt;space bytes&gt;] @ &lt;rip1&gt; &lt;rip2&gt; &lt;rip3&gt; [...]\n                                                                             \nMAPPED_LIBRARIES:\n[...] The first line is a header identifying that this is a heap profile (contrary to a CPU profile which pprof can also analyze, which uses a different, binary, format) and gives for each of the four fields we will record, their sum. Then comes one line per entry. Each entry has these four fields that the header just gave us a sum of: in use objects : How many objects are \'live\' i.e. in use on the heap at the time of the allocation. Allocating increases its value, freeing decreases it. in use bytes : How many bytes are \'live\' i.e. in use on the heap at the time of the allocation. Allocating increases its value, freeing decreases it. space objects : How many objects have been allocated since the start of the program. It is not affected by freeing memory, it only increases. space bytes : How many bytes have been allocated since the start of the program. It is not affected by freeing memory, it only increases. So when we allocate an object e.g. new(Foo) in C++: in use objects and space objects increment by 1 in use bytes and space bytes increment by sizeof(Foo) When we allocate an array of N elements of type Foo : in use objects and space objects increment by N in use bytes and space bytes increment by N * sizeof(Foo) When we free an object: in use objects decrements by 1 in use bytes decrements by sizeof(Foo) When we free an array of N elements of type Foo : in use objects decrements by N in use bytes decrements by N * sizeof(Foo) These 4 dimensions are really useful to spot memory leaks ( in use objects and in use bytes increase over time), peak memory usage ( space bytes ), whether we are doing many small allocations versus a few big allocations, etc. pprof also supports sampling and we could supply a sampling rate here optionally but we want to track each and every allocation so we do not bother with that. Each entry (i.e. line) ends with the call stack which is a space-separated list of addresses. We\'ll see that it is easy to get that information without resorting to external libraries such as libunwind by simply walking the stack, a topic I touched on in a previous article . Very importantly, multiple allocation records with the same stack must be merged together into one, summing their values. In that sense, each line conceptually an entry in a hashmap where the key is the call stack (the part of the right of the @ character) and the value is a 4-tuple: (u64, u64, u64, u64) (the part on the left of the @ character). The text file ends with a trailer which is crucial for symbolication (to transform memory addresses into source code locations), which on Linux is trivial to get: This is just a copy of the file /proc/self/maps . It lists of the loaded libraries and at which address they are. I have not implemented it myself but a quick internet search shows that the other major operating systems have a similar capability, named differently: Windows: VirtualQuery macOS: mach_vm_region_info FreeBSD: procstat_getvmmap Here is a small example: #include &lt;stdlib.h&gt;\n\nvoid b(int n) { malloc(n); }\n\nvoid a(int n) {\n  malloc(n);\n  b(n);\n}\n\nint main() {\n  for (int i = 0; i &lt; 2; i++)\n    a(2);\n\n  b(3);\n} Leveraging tcmalloc , this program will generate a heap profile: $ cc /tmp/test_alloc.c -ltcmalloc  -g3\n$ HEAPPROFILE=/tmp/heapprof ./a.out\nStarting tracking the heap\nDumping heap profile to /tmp/heapprof.0001.heap (Exiting, 11 bytes in use) This is just an example to showcase the format, we will from this point on use our own code to generate this text format. heap profile:      5:       11 [     5:       11] @ heapprofile\n     2:        4 [     2:        4] @ 0x558e804cc165 0x558e804cc18e 0x558e804cc1b0 0x7f452a4daa90 0x7f452a4dab49 0x558e804cc085\n     2:        4 [     2:        4] @ 0x558e804cc184 0x558e804cc1b0 0x7f452a4daa90 0x7f452a4dab49 0x558e804cc085\n     1:        3 [     1:        3] @ 0x558e804cc165 0x558e804cc1c4 0x7f452a4daa90 0x7f452a4dab49 0x558e804cc085\n\nMAPPED_LIBRARIES:\n558e804cb000-558e804cc000 r--p 00000000 00:00 183128      /tmp/a.out\n558e804cc000-558e804cd000 r-xp 00001000 00:00 183128      /tmp/a.out\n558e804cd000-558e804ce000 r--p 00002000 00:00 183128      /tmp/a.out\n558e804ce000-558e804cf000 r--p 00002000 00:00 183128      /tmp/a.out\n558e804cf000-558e804d0000 rw-p 00003000 00:00 183128      /tmp/a.out\n558e814b7000-558e81db8000 rw-p 00000000 00:00 0           [heap]\n7f4529e7e000-7f452a112000 rw-p 00000000 00:00 0           \n7f452a112000-7f452a115000 r--p 00000000 00:00 678524      /usr/lib/x86_64-linux-gnu/liblzma.so.5.4.1\n7f452a115000-7f452a136000 r-xp 00003000 00:00 678524      /usr/lib/x86_64-linux-gnu/liblzma.so.5.4.1\n7f452a136000-7f452a142000 r--p 00024000 00:00 678524      /usr/lib/x86_64-linux-gnu/liblzma.so.5.4.1\n7f452a142000-7f452a143000 r--p 00030000 00:00 678524      /usr/lib/x86_64-linux-gnu/liblzma.so.5.4.1\n7f452a143000-7f452a144000 rw-p 00031000 00:00 678524      /usr/lib/x86_64-linux-gnu/liblzma.so.5.4.1\n7f452a144000-7f452a152000 r--p 00000000 00:00 668348      /usr/lib/x86_64-linux-gnu/libm.so.6\n7f452a152000-7f452a1d0000 r-xp 0000e000 00:00 668348      /usr/lib/x86_64-linux-gnu/libm.so.6\n7f452a1d0000-7f452a22b000 r--p 0008c000 00:00 668348      /usr/lib/x86_64-linux-gnu/libm.so.6\n7f452a22b000-7f452a22c000 r--p 000e6000 00:00 668348      /usr/lib/x86_64-linux-gnu/libm.so.6\n7f452a22c000-7f452a22d000 rw-p 000e7000 00:00 668348      /usr/lib/x86_64-linux-gnu/libm.so.6\n7f452a22d000-7f452a22f000 rw-p 00000000 00:00 0           \n7f452a22f000-7f452a2cb000 r--p 00000000 00:00 678806      /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.32\n7f452a2cb000-7f452a3fc000 r-xp 0009c000 00:00 678806      /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.32\n7f452a3fc000-7f452a489000 r--p 001cd000 00:00 678806      /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.32\n7f452a489000-7f452a494000 r--p 0025a000 00:00 678806      /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.32\n7f452a494000-7f452a497000 rw-p 00265000 00:00 678806      /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.32\n7f452a497000-7f452a49b000 rw-p 00000000 00:00 0           \n7f452a49b000-7f452a49e000 r--p 00000000 00:00 702044      /usr/lib/x86_64-linux-gnu/libunwind.so.8.0.1\n7f452a49e000-7f452a4a8000 r-xp 00003000 00:00 702044      /usr/lib/x86_64-linux-gnu/libunwind.so.8.0.1\n7f452a4a8000-7f452a4ab000 r--p 0000d000 00:00 702044      /usr/lib/x86_64-linux-gnu/libunwind.so.8.0.1\n7f452a4ab000-7f452a4ac000 r--p 0000f000 00:00 702044      /usr/lib/x86_64-linux-gnu/libunwind.so.8.0.1\n7f452a4ac000-7f452a4ad000 rw-p 00010000 00:00 702044      /usr/lib/x86_64-linux-gnu/libunwind.so.8.0.1\n7f452a4ad000-7f452a4b7000 rw-p 00000000 00:00 0           \n7f452a4b7000-7f452a4d9000 r--p 00000000 00:00 668342      /usr/lib/x86_64-linux-gnu/libc.so.6\n7f452a4d9000-7f452a651000 r-xp 00022000 00:00 668342      /usr/lib/x86_64-linux-gnu/libc.so.6\n7f452a651000-7f452a6a9000 r--p 0019a000 00:00 668342      /usr/lib/x86_64-linux-gnu/libc.so.6\n7f452a6a9000-7f452a6ad000 r--p 001f1000 00:00 668342      /usr/lib/x86_64-linux-gnu/libc.so.6\n7f452a6ad000-7f452a6af000 rw-p 001f5000 00:00 668342      /usr/lib/x86_64-linux-gnu/libc.so.6\n7f452a6af000-7f452a6bc000 rw-p 00000000 00:00 0           \n7f452a6bc000-7f452a6bf000 r--p 00000000 00:00 677590      /usr/lib/x86_64-linux-gnu/libgcc_s.so.1\n7f452a6bf000-7f452a6da000 r-xp 00003000 00:00 677590      /usr/lib/x86_64-linux-gnu/libgcc_s.so.1\n7f452a6da000-7f452a6de000 r--p 0001e000 00:00 677590      /usr/lib/x86_64-linux-gnu/libgcc_s.so.1\n7f452a6de000-7f452a6df000 r--p 00021000 00:00 677590      /usr/lib/x86_64-linux-gnu/libgcc_s.so.1\n7f452a6df000-7f452a6e0000 rw-p 00022000 00:00 677590      /usr/lib/x86_64-linux-gnu/libgcc_s.so.1\n7f452a6e0000-7f452a6f3000 r--p 00000000 00:00 182678      /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4.5.9\n7f452a6f3000-7f452a719000 r-xp 00013000 00:00 182678      /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4.5.9\n7f452a719000-7f452a729000 r--p 00039000 00:00 182678      /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4.5.9\n7f452a729000-7f452a72a000 r--p 00048000 00:00 182678      /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4.5.9\n7f452a72a000-7f452a72b000 rw-p 00049000 00:00 182678      /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4.5.9\n7f452a72b000-7f452a8e1000 rw-p 00000000 00:00 0           \n7f452a8e4000-7f452a8f8000 rw-p 00000000 00:00 0           \n7f452a8f8000-7f452a8f9000 r--p 00000000 00:00 668336      /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2\n7f452a8f9000-7f452a921000 r-xp 00001000 00:00 668336      /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2\n7f452a921000-7f452a92b000 r--p 00029000 00:00 668336      /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2\n7f452a92b000-7f452a92d000 r--p 00033000 00:00 668336      /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2\n7f452a92d000-7f452a92f000 rw-p 00035000 00:00 668336      /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2\n7fff91a4d000-7fff91a6e000 rw-p 00000000 00:00 0           [stack]\n7fff91b3f000-7fff91b43000 r--p 00000000 00:00 0           [vvar]\n7fff91b43000-7fff91b45000 r-xp 00000000 00:00 0           [vdso]\nffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0           [vsyscall] We see that at the end of the program, we have (looking at the first line): 5 objects in use 11 bytes in use 5 objects allocated in total 11 bytes allocated in total Since we never freed any memory, the in use counters are the same as the space counters. We have 3 unique call stacks that allocate, in the same order as they appear in the text file (although order does not matter for pprof ): b &lt;- a &lt;- main a &lt;- main b &lt;- main Since our program is a Position Independant Executable (PIE), the loader picks a random address for where to load our program in virtual memory. Consequently, addresses collected from within our program have this offset added to them and this offset is different every run. Thankfully, the MAPPED_LIBRARIES section lists address ranges (the first column of each line in that section) for each library that gets loaded. As such, pprof only needs to find for each address the relevant range, subtract the start of the range from this address, and it has the real address in our executable. It then runs addr2line or similar to get the code location. Finally we can use pprof to extract human-readable information from this text file: $ pprof --text ./a.out ./heapprof.0001.heap\nUsing local file ./a.out.\nUsing local file /tmp/heapprof.0001.heap.\nTotal: 0.0 MB\n     0.0  63.6%  63.6%      0.0  63.6% b\n     0.0  36.4% 100.0%      0.0  72.7% a\n     0.0   0.0% 100.0%      0.0 100.0% __libc_start_call_main\n     0.0   0.0% 100.0%      0.0 100.0% __libc_start_main_impl\n     0.0   0.0% 100.0%      0.0 100.0% _start\n     0.0   0.0% 100.0%      0.0 100.0% main Generating a pprof profile Let\'s start with a very simple arena (directly based on https://nullprogram.com/blog/2023/09/27/ ) and show how it is used: #define _GNU_SOURCE\n#include &lt;stdint.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;sys/mman.h&gt;\n#include &lt;fcntl.h&gt;\n#include &lt;unistd.h&gt;\n\n\ntypedef struct {\n  u8 *start;\n  u8 *end;\n} arena_t;\n\nstatic void * arena_alloc(arena_t *a, size_t size, size_t align, size_t count) {\n  pg_assert(a-&gt;start &lt;= a-&gt;end);\n  pg_assert(align == 1 || align == 2 || align == 4 || align == 8);\n\n  size_t available = a-&gt;end - a-&gt;start;\n  size_t padding = -(size_t)a-&gt;start &amp; (align - 1);\n\n  size_t offset = padding + size * count;\n  if (available &lt; offset) {\n    fprintf(stderr,\n            &quot;Out of memory: available=%lu &quot;\n            &quot;allocation_size=%lu\\n&quot;,\n            available, offset);\n    abort();\n  }\n\n  uint8_t *res = a-&gt;start + padding;\n\n  a-&gt;start += offset;\n\n  return (void *)res;\n} Now, we are ready to add memory profiling to our simple allocator. First, we model a record with the 4 counters and the call stack: typedef struct {\n  uint64_t in_use_space, in_use_objects, alloc_space, alloc_objects;\n  uint64_t *call_stack;\n  uint64_t call_stack_len;\n} mem_record_t; Then, the profile, which contains the 4 counters as a sum and an array of records. An arena now has an (optional) pointer to a memory profile: typedef struct mem_profile_t mem_profile_t;\ntypedef struct {\n  uint8_t *start;\n  uint8_t *end;\n  mem_profile_t* profile;\n} arena_t;\n\nstruct mem_profile_t {\n  mem_record_t *records;\n  uint64_t records_len;\n  uint64_t records_cap;\n  uint64_t in_use_space, in_use_objects, alloc_space, alloc_objects;\n  arena_t arena;\n}; Note that the memory profile needs to allocate to store this metadata and as such needs an arena. Which makes these two structures cyclic! The way we solve it is: We create a small arena dedicated to the memory profiling and this arena does not have a memory profile attached (otherwise we would end up in an infinite recursion, and we are not interested in profiling the memory usage of the memory profiler anyway; its memory usage is capped by the size of its dedicated arena). We create the memory profile using this arena. We create the main arena for our program to use and attach the profile to it. static arena_t arena_new(uint64_t cap, mem_profile_t *profile) {\n  uint8_t *mem = mmap(NULL, cap, PROT_READ | PROT_WRITE,\n                      MAP_ANONYMOUS | MAP_PRIVATE, -1, 0);\n\n  arena_t arena = {\n      .profile = profile,\n      .start = mem,\n      .end = mem + cap,\n  };\n  return arena;\n}\n\nint main(){\n  arena_t mem_profile_arena = arena_new(1 &lt;&lt; 16, NULL);\n  mem_profile_t mem_profile = {.arena = mem_profile_arena};\n\n  arena_t arena = arena_new(1 &lt;&lt; 22, &amp;mem_profile);\n} Now, in arena_alloc , if there is a non-NULL memory profile, we record the allocation just before returning the freshly allocated pointer: static void *arena_alloc(arena_t *a, size_t size, size_t align, size_t count) {\n  [...]\n\n  if (a-&gt;profile) {\n    mem_profile_record_alloc(a-&gt;profile, count, offset);\n  }\n\n  return (void *)res;\n} We now have to implement mem_profile_record_alloc and exporting the profile to the text format, and we are done. When recording an allocation, we need to capture the call stack, so we walk the stack upwards until we reach a frame address that is 0 or does not have the alignement of a pointer (8); at which point we know not to dereference it and go further. This will break if we disable frame pointers when compiling ( -fomit-frame-pointer ) which is in my opinion always a bad idea. There are other ways to get a call stack fortunately but they all are more involved and potentially slower. Note that this approach probably only works on x86_64, no idea how ARM does that. Here is a deep dive on getting a stack trace in different environments. static uint8_t record_call_stack(uint64_t *dst, uint64_t cap) {\n  uintptr_t *rbp = __builtin_frame_address(0);\n\n  uint64_t len = 0;\n\n  while (rbp != 0 &amp;&amp; ((uint64_t)rbp &amp; 7) == 0 &amp;&amp; *rbp != 0) {\n    const uintptr_t rip = *(rbp + 1);\n    rbp = (uintptr_t *)*rbp;\n\n    // `rip` points to the return instruction in the caller, once this call is\n    // done. But: We want the location of the call i.e. the `call xxx`\n    // instruction, so we subtract one byte to point inside it, which is not\n    // quite \'at\' it, but good enough.\n    dst[len++] = rip - 1;\n\n    if (len &gt;= cap)\n      return len;\n  }\n  return len;\n} Now we can record the allocation proper, upserting the new record into our existing list of records, trying to find an existing record with the same call stack.\nThat part is important to avoid having a huge profile and that\'s why pprof made this design decision. The code is slightly lengthy because we need to roll our own arrays here in this minimal example, but in a real application you\'d have your own array structure and helper functions, most likely: static void mem_profile_record_alloc(mem_profile_t *profile,\n                                     uint64_t objects_count,\n                                     uint64_t bytes_count) {\n  // Record the call stack by stack walking.\n  uint64_t call_stack[64] = {0};\n  uint64_t call_stack_len =\n      record_call_stack(call_stack, sizeof(call_stack) / sizeof(call_stack[0]));\n\n  // Update the sums.\n  profile-&gt;alloc_objects += objects_count;\n  profile-&gt;alloc_space += bytes_count;\n  profile-&gt;in_use_objects += objects_count;\n  profile-&gt;in_use_space += bytes_count;\n\n  // Upsert the record.\n  for (uint64_t i = 0; i &lt; profile-&gt;records_len; i++) {\n    mem_record_t *r = &amp;profile-&gt;records[i];\n\n    if (r-&gt;call_stack_len == call_stack_len &amp;&amp;\n        memcmp(r-&gt;call_stack, call_stack, call_stack_len * sizeof(uint64_t)) ==\n            0) {\n      // Found an existing record, update it.\n      r-&gt;alloc_objects += objects_count;\n      r-&gt;alloc_space += bytes_count;\n      r-&gt;in_use_objects += objects_count;\n      r-&gt;in_use_space += bytes_count;\n      return;\n    }\n  }\n\n  // Not found, insert a new record\n  mem_record_t record = {\n      .alloc_objects = objects_count,\n      .alloc_space = bytes_count,\n      .in_use_objects = objects_count,\n      .in_use_space = bytes_count,\n  };\n  record.call_stack = arena_alloc(&amp;profile-&gt;arena, sizeof(uint64_t),\n                                  _Alignof(uint64_t), call_stack_len);\n  memcpy(record.call_stack, call_stack, call_stack_len * sizeof(uint64_t));\n  record.call_stack_len = call_stack_len;\n\n  if (profile-&gt;records_len &gt;= profile-&gt;records_cap) {\n    uint64_t new_cap = profile-&gt;records_cap * 2;\n    // Grow the array.\n    mem_record_t *new_records = arena_alloc(\n        &amp;profile-&gt;arena, sizeof(mem_record_t), _Alignof(mem_record_t), new_cap);\n    memcpy(new_records, profile-&gt;records,\n           profile-&gt;records_len * sizeof(mem_record_t));\n    profile-&gt;records_cap = new_cap;\n    profile-&gt;records = new_records;\n  }\n  profile-&gt;records[profile-&gt;records_len++] = record;\n} Finally, we can dump this profile in the pprof textual representation: static void mem_profile_write(mem_profile_t *profile, FILE *out) {\n  fprintf(out, &quot;heap profile: %lu: %lu [     %lu:    %lu] @ heapprofile\\n&quot;,\n          profile-&gt;in_use_objects, profile-&gt;in_use_space,\n          profile-&gt;alloc_objects, profile-&gt;alloc_space);\n\n  for (uint64_t i = 0; i &lt; profile-&gt;records_len; i++) {\n    mem_record_t r = profile-&gt;records[i];\n\n    fprintf(out, &quot;%lu: %lu [%lu: %lu] @ &quot;, r.in_use_objects, r.in_use_space,\n            r.alloc_objects, r.alloc_space);\n\n    for (uint64_t j = 0; j &lt; r.call_stack_len; j++) {\n      fprintf(out, &quot;%#lx &quot;, r.call_stack[j]);\n    }\n    fputc(\'\\n\', out);\n  }\n\n  fputs(&quot;\\nMAPPED_LIBRARIES:\\n&quot;, out);\n\n  static uint8_t mem[4096] = {0};\n  int fd = open(&quot;/proc/self/maps&quot;, O_RDONLY);\n  assert(fd != -1);\n  ssize_t read_bytes = read(fd, mem, sizeof(mem));\n  assert(read_bytes != -1);\n  close(fd);\n\n  fwrite(mem, 1, read_bytes, out);\n\n  fflush(out);\n} And we\'re done! Let\'s try it with our initial example (bumping the size of the allocations a bit because pprof ignores tiny allocations for readability - although this is configurable): void b(int n, arena_t *arena) {\n  arena_alloc(arena, sizeof(int), _Alignof(int), n);\n}\n\nvoid a(int n, arena_t *arena) {\n  arena_alloc(arena, sizeof(int), _Alignof(int), n);\n  b(n, arena);\n}\n\nint main() {\n  [...]\n\n  arena_t arena = arena_new(1 &lt;&lt; 28, &amp;mem_profile);\n\n  for (int i = 0; i &lt; 2; i++)\n    a(2 * 1024 * 1024, &amp;arena);\n\n  b(3 * 1024 * 1024, &amp;arena);\n\n  mem_profile_write(&amp;mem_profile, stderr);\n} $ cc -g3 example.c\n$ ./a.out 2&gt; heap.profile\n$ pprof --web ./a.out heap.profile And we see in our browser: And we can even generate a flamegraph for it leveraging the great OG flamegraph project : $ pprof --collapsed ./a.out heap.profile | flamegraph.pl &gt; out.svg Variations and limitations For this article we always do memory profiling and abort once the arena is full; but it does not have to be this way. Memory profiling could be enabled in a CLI program with a command line flag; if it is disabled we do not create a memory profile nor an arena for it. Or, it could be enabled/disabled dynamically, after a given amount of time, etc. It could also stop when its dedicated arena is full instead of aborting the whole program. Sampling could be easily added to mem_profile_record_alloc to only record some records, say 1% The current maximum call stack depth is 64, for brevity in the context of this article. We can store a bigger one by having a dynamically sized array or storing each address in a more compact format, e.g. varint instead of a fixed 8 bytes Stack traces won\'t work across library calls that are compiled without frame pointers. To which I\'d say: It\'s likely easier to compile all of the code you depend on with the build flags you require than try to come up with alternative ways to walk the stack. Your mileage may vary. We use linear scanning to find an existing record with the same call stack. When having lots of records, it would be advantageous to use a binary search on a sorted array or perhaps a hashtable. Alternatives pprof (the Perl one) is not the only way to get this information. It turns out that your browser comes with a built-in profiler and a nice one to use at that! And it has support for native allocations, stack traces and so forth. Another possibility is the new pprof (the Go one). They all have more features than the original pprof that are really handy, most notably: A built-in interactive flamegraph feature Tracking the time at which an allocation happened, which can then be used to produce a flamechart representing allocations over time (for example to observe a memory leak increasing the memory usage over time, and discover where it comes from) To make use of these, our application needs to generate the information we gathered in the format the profiler expects, just like we did with pprof . Chrome expects a JSON file , which I did not experiment with yet. Firefox expects a different JSON file . A good starting point is https://github.com/mstange/samply . I experimented with it but dropped this avenue because of several frustrating aspects: It is very JS-centric so much of the profile has to be filled with null values or explicitly saying that the each sample is not for JS. All fields must be provided even if empty, including arrays. Failing to do so throws an obscure exception in the profiler, that has to be tracked in the browser debugger, which shows the minified JS profiler code, which is not fun (yes, the profiler is written mostly/entirely in JS). The consequence is that most of the profile file is made of lengthy arrays only containing null values. Thus, most of the code to generate it is boilerplate noise. Memory traces are supported but it seems that a CPU trace is required for each memory trace which makes the profile even bigger, and harder to generate. Only providing memory samples shows nothing in the graphs. The new pprof (the Go version) expects a relatively simple gzipped protobuf file , but that means adding code generation and a library dependency. I use this tool when writing Go quite often and it is helpful. It also supports adding labels to samples, for example we could label the allocations coming from different arenas differently to be able to distinguish them in the same profile. Conclusion I like that one of the most common memory profilers uses a very simple text format that anyone can generate, and that\'s it\'s stand-alone. It\'s very UNIXy! Nonetheless, I will in the future explore the other aforementioned profilers (probably the Chrome one because it seems the most straightforward) and I do not think it should be much additional work. It\'s nice to leverage the existing browser to avoid having to install a profiler. After all, it\'s been done before ! Addendum: the full code The full code #define _GNU_SOURCE\n#include &lt;assert.h&gt;\n#include &lt;fcntl.h&gt;\n#include &lt;stdint.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;sys/mman.h&gt;\n#include &lt;unistd.h&gt;\n\ntypedef struct {\n  uint64_t in_use_space, in_use_objects, alloc_space, alloc_objects;\n  uint64_t *call_stack;\n  uint64_t call_stack_len;\n} mem_record_t;\n\ntypedef struct mem_profile mem_profile_t;\ntypedef struct {\n  uint8_t *start;\n  uint8_t *end;\n  mem_profile_t *profile;\n} arena_t;\n\nstruct mem_profile {\n  mem_record_t *records;\n  uint64_t records_len;\n  uint64_t records_cap;\n  uint64_t in_use_space, in_use_objects, alloc_space, alloc_objects;\n  arena_t arena;\n};\n\nstatic void *arena_alloc(arena_t *a, size_t size, size_t align, size_t count);\n\nstatic uint8_t record_call_stack(uint64_t *dst, uint64_t cap) {\n  uintptr_t *rbp = __builtin_frame_address(0);\n\n  uint64_t len = 0;\n\n  while (rbp != 0 &amp;&amp; ((uint64_t)rbp &amp; 7) == 0 &amp;&amp; *rbp != 0) {\n    const uintptr_t rip = *(rbp + 1);\n    rbp = (uintptr_t *)*rbp;\n\n    // `rip` points to the return instruction in the caller, once this call is\n    // done. But: We want the location of the call i.e. the `call xxx`\n    // instruction, so we subtract one byte to point inside it, which is not\n    // quite \'at\' it, but good enough.\n    dst[len++] = rip - 1;\n\n    if (len &gt;= cap)\n      return len;\n  }\n  return len;\n}\nstatic void mem_profile_record_alloc(mem_profile_t *profile,\n                                     uint64_t objects_count,\n                                     uint64_t bytes_count) {\n  // Record the call stack by stack walking.\n  uint64_t call_stack[64] = {0};\n  uint64_t call_stack_len =\n      record_call_stack(call_stack, sizeof(call_stack) / sizeof(call_stack[0]));\n\n  // Update the sums.\n  profile-&gt;alloc_objects += objects_count;\n  profile-&gt;alloc_space += bytes_count;\n  profile-&gt;in_use_objects += objects_count;\n  profile-&gt;in_use_space += bytes_count;\n\n  // Upsert the record.\n  for (uint64_t i = 0; i &lt; profile-&gt;records_len; i++) {\n    mem_record_t *r = &amp;profile-&gt;records[i];\n\n    if (r-&gt;call_stack_len == call_stack_len &amp;&amp;\n        memcmp(r-&gt;call_stack, call_stack, call_stack_len * sizeof(uint64_t)) ==\n            0) {\n      // Found an existing record, update it.\n      r-&gt;alloc_objects += objects_count;\n      r-&gt;alloc_space += bytes_count;\n      r-&gt;in_use_objects += objects_count;\n      r-&gt;in_use_space += bytes_count;\n      return;\n    }\n  }\n\n  // Not found, insert a new record.\n  mem_record_t record = {\n      .alloc_objects = objects_count,\n      .alloc_space = bytes_count,\n      .in_use_objects = objects_count,\n      .in_use_space = bytes_count,\n  };\n  record.call_stack = arena_alloc(&amp;profile-&gt;arena, sizeof(uint64_t),\n                                  _Alignof(uint64_t), call_stack_len);\n  memcpy(record.call_stack, call_stack, call_stack_len * sizeof(uint64_t));\n  record.call_stack_len = call_stack_len;\n\n  if (profile-&gt;records_len &gt;= profile-&gt;records_cap) {\n    uint64_t new_cap = profile-&gt;records_cap * 2;\n    // Grow the array.\n    mem_record_t *new_records = arena_alloc(\n        &amp;profile-&gt;arena, sizeof(mem_record_t), _Alignof(mem_record_t), new_cap);\n    memcpy(new_records, profile-&gt;records,\n           profile-&gt;records_len * sizeof(mem_record_t));\n    profile-&gt;records_cap = new_cap;\n    profile-&gt;records = new_records;\n  }\n  profile-&gt;records[profile-&gt;records_len++] = record;\n}\n\nstatic void mem_profile_write(mem_profile_t *profile, FILE *out) {\n  fprintf(out, &quot;heap profile: %lu: %lu [     %lu:    %lu] @ heapprofile\\n&quot;,\n          profile-&gt;in_use_objects, profile-&gt;in_use_space,\n          profile-&gt;alloc_objects, profile-&gt;alloc_space);\n\n  for (uint64_t i = 0; i &lt; profile-&gt;records_len; i++) {\n    mem_record_t r = profile-&gt;records[i];\n\n    fprintf(out, &quot;%lu: %lu [%lu: %lu] @ &quot;, r.in_use_objects, r.in_use_space,\n            r.alloc_objects, r.alloc_space);\n\n    for (uint64_t j = 0; j &lt; r.call_stack_len; j++) {\n      fprintf(out, &quot;%#lx &quot;, r.call_stack[j]);\n    }\n    fputc(\'\\n\', out);\n  }\n\n  fputs(&quot;\\nMAPPED_LIBRARIES:\\n&quot;, out);\n\n  static uint8_t mem[4096] = {0};\n  int fd = open(&quot;/proc/self/maps&quot;, O_RDONLY);\n  assert(fd != -1);\n  ssize_t read_bytes = read(fd, mem, sizeof(mem));\n  assert(read_bytes != -1);\n  close(fd);\n\n  fwrite(mem, 1, read_bytes, out);\n\n  fflush(out);\n}\n\nstatic void *arena_alloc(arena_t *a, size_t size, size_t align, size_t count) {\n  size_t available = a-&gt;end - a-&gt;start;\n  size_t padding = -(size_t)a-&gt;start &amp; (align - 1);\n\n  size_t offset = padding + size * count;\n  if (available &lt; offset) {\n    fprintf(stderr,\n            &quot;Out of memory: available=%lu &quot;\n            &quot;allocation_size=%lu\\n&quot;,\n            available, offset);\n    abort();\n  }\n\n  uint8_t *res = a-&gt;start + padding;\n\n  a-&gt;start += offset;\n\n  if (a-&gt;profile) {\n    mem_profile_record_alloc(a-&gt;profile, count, offset);\n  }\n\n  return (void *)res;\n}\n\nstatic arena_t arena_new(uint64_t cap, mem_profile_t *profile) {\n  uint8_t *mem = mmap(NULL, cap, PROT_READ | PROT_WRITE,\n                      MAP_ANONYMOUS | MAP_PRIVATE, -1, 0);\n\n  arena_t arena = {\n      .profile = profile,\n      .start = mem,\n      .end = mem + cap,\n  };\n  return arena;\n}\n\nvoid b(int n, arena_t *arena) {\n  arena_alloc(arena, sizeof(int), _Alignof(int), n);\n}\n\nvoid a(int n, arena_t *arena) {\n  arena_alloc(arena, sizeof(int), _Alignof(int), n);\n  b(n, arena);\n}\n\nint main() {\n  arena_t mem_profile_arena = arena_new(1 &lt;&lt; 16, NULL);\n  mem_profile_t mem_profile = {\n      .arena = mem_profile_arena,\n      .records = arena_alloc(&amp;mem_profile_arena, sizeof(mem_record_t),\n                             _Alignof(mem_record_t), 16),\n      .records_cap = 16,\n  };\n\n  arena_t arena = arena_new(1 &lt;&lt; 28, &amp;mem_profile);\n\n  for (int i = 0; i &lt; 2; i++)\n    a(2 * 1024 * 1024, &amp;arena);\n\n  b(3 * 1024 * 1024, &amp;arena);\n\n  mem_profile_write(&amp;mem_profile, stderr);\n} ",
titles:[
{
title:"Pprof",
slug:"pprof",
offset:1610,
},
{
title:"The text format",
slug:"the-text-format",
offset:2720,
},
{
title:"Generating a pprof profile",
slug:"generating-a-pprof-profile",
offset:14101,
},
{
title:"Variations and limitations",
slug:"variations-and-limitations",
offset:23208,
},
{
title:"Alternatives",
slug:"alternatives",
offset:24486,
},
{
title:"Conclusion",
slug:"conclusion",
offset:26744,
},
{
title:"Addendum: the full code",
slug:"addendum-the-full-code",
offset:27226,
},
],
},
{
html_file_name:"gnuplot_lang.html",
title:"Solving a problem with Gnuplot, the programming language (not the plotting software!)",
text:"Is it any good? Can you solve real problems with it? Most people know Gnuplot as a way to plot data. Two lines of code and we can visualize data: set output &quot;plot.png&quot;\nplot &quot;out.dat&quot; with lines where out.data is a text file with a number on each line. The software engineering advice that I heard a long time ago and left a mark on me is: Find a way to visualize your problem. So Gnuplot is definitely worth of a spot in a Software Engineer\'s toolbox. However, few know that Gnuplot is actually also Turing complete programming language. It is similar in syntax to Perl or Awk. So I scratched an itch and solved a problem with it. The problem In short, we get a text file where each line contains random ASCII characters. For each line, we must find the first and last digit characters, combine them into a number and at the end output the sum of all these numbers. The way we read the data into a variable is through a shell command: data = system(&quot;cat in.txt&quot;) Gnuplot has the plot command to turn input data into a plot, but nothing built-in to read input data into a variable, it seems. No matter, system which spawns a command in a subshell does the trick. Since we need to check whether a character is a string, let\'s define our own little function for it. Yes, Gnuplot has user defined functions! The unfortunate limitation is that the body has to be an expression: is_digit(c) = c eq &quot;0&quot; || c eq &quot;1&quot; || c eq &quot;2&quot; || c eq &quot;3&quot; || c eq &quot;4&quot; || c eq &quot;5&quot; || c eq &quot;6&quot; || c eq &quot;7&quot; || c eq &quot;8&quot; || c eq &quot;9&quot; Characters are not a thing; instead we deal with a string of length 1. Comparing strings for equality is done with the operator eq . Then, we iterate over each line in the data. Gnuplot has a for-each construct we can use for that. We then iterate over each character in the line with a for-range loop, isolating the \'character\' (remember, it\'s just a string of length 1) with a slicing syntax that many modern languages have: sum = 0\n\ndo for [line in data] {\n  len = strlen(line)\n\n  do for [i = 1:len] {\n    c = line[i:i]\n  }\n} One thing to note here is that strings are 1-indexed and the slicing syntax is: foo[start_inclusive:end_inclusive] . We then set first to the first digit character we find: do for [line in data] {\n  len = strlen(line)\n\n  first= &quot;&quot;\n\n  do for [i = 1:len] {\n    c = line[i:i]\n    if (is_digit(c)) {\n      if (first eq &quot;&quot;) {\n        first = c\n        break\n      }  \n    }\n  }\n} We do the same for the last character, iterating in reverse order: last = &quot;&quot;\n\n  do for [i = len:1:-1] {\n    c = line[i:i]\n    if (is_digit(c)) {\n      if (last eq &quot;&quot;) {\n        last = c\n        break\n      }  \n    }\n  } Finally, we concatenate the two digits (which are still two strings of length 1 at that point) with the . operator, convert it to a number with the + 0 idiom, and increase our sum: num = first . last + 0\n  sum = sum + num We just have to print the sum at the end: print(sum) Closing thoughts Pretty straightforward, isn\'t it? Well, no. The language is weirdly restrictive, for example sum += num does not parse. for and while loops cannot for some reason be used interchangeably due to the weird do prefix for for-loops. Very few builtin functions are available.\nThere does not seem to be basic data structures such as arrays and maps. Every variable is global. And so on. It\'s weird because the language also has very modern constructs that some mainstream languages still do not have, like the slicing syntax. Awk, Lua or Perl are honestly better in every way, to pick relatively simple, dynamic languages that people usually reach to for Unixy text transformations. And these will have better tooling, such as a debugger. Heck, even shell scripting is probably easier and more straightforward, and that\'s a low bar. Everything points to the fact that Gnuplot expects it\'s input data in some prearranged tabular form, and just wants to plot it, not transform it. That means that another (real) programming language is expected to do prior work and Gnuplot is at the end of the data pipeline as a \'dumb\' visualization tool. I can also see how the limited language can still be useful for Physicists or Mathematicians to write simple numerical, pure functions e.g. f(x) = x*2 + 1 . I\'ll investigate Julia and perhaps R in the future, which are in the same niche of science/data visualization but are full programming languages with plentiful tooling. Addendum: The full code Run with gnuplot my_file.dem . data = system(&quot;cat in.txt&quot;)\n\nis_digit(c) = c eq &quot;0&quot; || c eq &quot;1&quot; || c eq &quot;2&quot; || c eq &quot;3&quot; || c eq &quot;4&quot; || c eq &quot;5&quot; || c eq &quot;6&quot; || c eq &quot;7&quot; || c eq &quot;8&quot; || c eq &quot;9&quot;\n\nsum = 0\n\ndo for [line in data] {\n  len = strlen(line)\n\n  first= &quot;&quot;\n\n  do for [i = 1:len] {\n    c = line[i:i]\n    if (is_digit(c)) {\n      if (first eq &quot;&quot;) {\n        first = c\n        break\n      }  \n    }\n  }\n\n\n  last = &quot;&quot;\n\n  do for [i = len:1:-1] {\n    c = line[i:i]\n    if (is_digit(c)) {\n      if (last eq &quot;&quot;) {\n        last = c\n        break\n      }  \n    }\n  }\n  num = first . last + 0\n  sum = sum + num\n}\n\nprint(sum) ",
titles:[
{
title:"The problem",
slug:"the-problem",
offset:651,
},
{
title:"Closing thoughts",
slug:"closing-thoughts",
offset:3073,
},
{
title:"Addendum: The full code",
slug:"addendum-the-full-code",
offset:4549,
},
],
},
{
html_file_name:"feed.html",
title:"This blog now has an Atom feed, and yours should probably too",
text:"Find it here or in the header on the top right-hand corner. Imagine a world where you can see the content of each website you like inside the app of your choosing, read the articles offline and save them on disk for later, be notified whenever the website has something new, and all of which is implemented with an open standard. Well that was most of the web some years ago and this blog now does all of that. And it\'s not hard! The only thing we need is to serve a feed.xml file that lists articles with some metadata such as \'updated at\' and a UUID to be able to uniquely identify an article. This XML file is an Atom feed which has a nice RFC . I implemented that in under an hour, skimming at the RFC and examples. It\'s a bit hacky but it works. The script to do so is here . And you can do too! Again, it\'s not hard. Here goes: We pick a UUID for our feed. I just generated one and stuck it as a constant in the script. The \'updated at\' field for the feed is just time.Now() . It\'s not exactly accurate, it should probably be the most recent mtime across articles but it\'s good enough. For each article ( *.html ) file in the directory, we add an entry ( &lt;entry&gt; ) in the XML document with: The link to the article, that\'s just the filename in my case. The \'updated at\' field, which is just the mtime of the file locally queried from git The \'published at\' field, which is just the ctime of the file locally queried from git A UUID. Here I went with UUIDv5 which is simply the sha1 of the file name in the UUID format. It\'s nifty because it means that the script is stateless and idempotent. If the article is later updated, the UUID remains the same (but the updated at will still hint at the update). And...that\'s it really. Enjoy reading these articles in your favorite app! ",
titles:[
],
},
{
html_file_name:"body_of_work.html",
title:"Body of work",
text:"I am here recounting chronologically what I have achieved in my career until now. I have a sub par memory so it\'s great for me to look back on that, and may it also hopefully serve as an interesting insight for fellow practitioners and recruiters into the kind of work I enjoy and am experienced with. Cr\u{e9}dit Mutuel (Bank), Strasbourg, France; 2013 This was my first professional experience (a 8 weeks internship) and happened at one of the big banks in the country. Like most banks that have been founded in the 20th century, they have millions and millions of lines of code in COBOL, and realized this is not tenable and need to migrate to a modern tech stack to be able to keep it simply running. I migrated a business application used internally. The original application was a terminal (as in: made for a hardware terminal), with a basic UI, talking to a DB2 database, and running on IBM\'s z/OS on a (real) mainframe in the confines of the bank. The final application was a C# web application, still talking to the same database, but this time running on a Windows server. In retrospect, that was such a unique experience to work on a 30+ year old codebase running on a tech stack, OS and hardware that most developers will never encounter. The team insisted (although not unanimously) that I write new COBOL code for this brand new applications for the layer that talks to the database. I think they were worried that C# could not do this job properly, for some reason? So I got to do that, in a COBOL IDE made by IBM which only COBOL developers know of. To-do list item ticked, I guess. All in all, that was a very interesting social experience and a great insight on how business and developers think and (try to) evolve, and how one can attempt to change the tech stack of an existing running application, which is a challenge any company will face at a moment or another. And how we engineers have a professional duty to keep learning and adapting to this changing world. CNRS Intern Software Engineer experimenting with the Oculus Rift (VR) CNRS, Strasbourg, France; 2014 My second internship (10 weeks), and perhaps the project I loved the most. This took place at an astronomy lab, I had the incredible privilege to have my office in the old library that was probably a few centuries old, filled with old books; the building was this 19th century observatory with a big park with bee houses... This will never be topped. The work was also such a blast: I got to experiment with the first version of the Oculus Rift, and tinker with it. My adviser and I decided to work on two different projects: A (from scratch) 3D visualization of planets inside the Oculus Rift for kids to \'fly\' through the solar system and hopefully spark in them an interest in space exploration and astronomy. The second project was to add to an existing and large 3D simulation of planets a VR mode. It was an exciting time, VR was all the rage and everything had to be figured out: the motion sickness, the controller (it turns out that most people are not so good at using a keyboard and mouse while being completely blind and a video game console controller is much more intuitive), how to plug the Oculus Rift SDK to an existing codebase, the performance, etc. Even though I wished I had a tad more time, I delivered both. Since we foresaw we would not have time for everything, the solar system visualization got cut down to blue sparkling cubes in 3D space, but still, the Oculus Rift worked beautifully with it. Even though the field was in its infancy, the feeling of realism was already jarring - our brain is tricked easily! The second project also worked, although we had visual artifacts in some cases when traveling far distances, which I suspected was due to floating point precision issues in the existing codebase. There are articles online discussing this fact with physical simulations where huge distances are present and I discussed it with codebase authors during our weekly check-ins and demo sessions. Performance was initially also a challenge since VR consists of rendering the same scene twice, once for each eye, with a slight change in where the camera is in 3D space (since the camera is your eye, in a way). And 3D rendering will always be a domain where performance is paramount. It\'s interesting to note that new 3D APIs such as Vulkan do offer features for VR in the form of extensions to speed it up in hardware, having the GPU do the heavy lifting. But back in 2014, there was nothing like that. Also, 3D APIs have really evolved in the last decade, becoming more low level and giving the developer more control, power, but also responsibilities. My major performance stepping stone was moving from rendering everything in the scene to using an octree to only render entities in the \'zone\' where the camera is, or is looking at. I used OpenGL and C++ for the first project, and C for the second one since the existing codebase was in C. 3D, VR, extending an existing codebase, starting a new project from scratch with \'carte blanche\': I learned a ton! And my adviser, fellow coworkers exploring this space (notably trying to do the same with a different VR headset, the Sony Morpheus), and I even got to publish a paper based on our work, that got submitted: Immersive-3D visualization of astronomical data , link 1 , link 2 . Finally, I on-boarded my successor on the codebase and the build system and helped them troubleshoot some cross-platform issues. Full-stack Software Engineer EdgeLab, Lausanne, Switzerland; 2015-2017 My first full-time job, initially being a 6 months internship concluding my Master\'s degree of Computer Science, and then extending into a full time position. The company was a financial startup building simulations of the stock market: how does the price of a bond or an option evolve if there is an earthquake or a housing crash? The idea was to observe how these events affected the stock market historically and simulate these happening on your portfolio to get a sense of how robust or risky your positions are. The startup was filled with super smart Math and Physics doctors and it was such a chance to work alongside them. It was lots of new stuff for me: new country, new way of working, being in a small startup (something like 10 people when I joined, if at all), and having to ship something very quickly for a demo coming up in a few days! My main achievements were at first to optimize and simplify the front-end experience which had complex and at times slow pages (I remember the infamous Tree Table: a classic HTML table, except that each cell shows tree shaped data, like a file system!). Then, realizing it would not scale easily, I convinced the team to migrate to Typescript (that\'s quite early at this point: end of 2015!). That was so effective that while the migration was on-going, you could tell which page of the application was written in JavaScript or Typescript based on whether it had random bugs or not. Most of the gains of Typescript was not so much the readability of static typing or the warnings, although these helped, but rather reducing the dynamism of the code by forcing the developer to stick to one type for a given variable. That of course dramatically improved correctness and reduced bugs, but incidentally also helped the performance! I then moved to the back-end, extending the financial models and simulations in C++. That was a lot of matrix code and financial math to learn!\nI also contributed to modernizing the codebase to C++11 and improving the build system to make it easier to on-board new people. Eventually, the company got acquired for 8 digits by a Swiss bank. Back-end Software Engineer &amp; DevOps PPRO, Munich, Germany; 2017-2023 I knew I did not want to stay in Switzerland, and found a job as a Software Engineer in Munich, Germany, at a FinTech company. This time not the stock market kind but the online payments kind. Think Paypal or the now defunct Wirecard (but we were honest and law abiding, and they were not). I joined to kick start the effort of transforming web applications from server-side rendered HTML with a tiny bit of JavaScript to fully dynamic single page applications (SPA) in Typescript. At the time, SPAs were all the rage, and it is true that C++ back-ends with a slow and complex deployment process are not a great fit to a fast growing company. Still, looking back, I am not sure if static HTML does not cover 90% of the use cases. However, it was so much fun to get feedback or a request from the users, implement it in a few hours, deploy it, and tell them to try it! They got used to that rapid cycle very quickly. I then moved to the back-end, working for a time in a full-stack manner, and then spear-heading the company-wide effort to move to the cloud and Kubernetes, thus I morphed into a DevOps person, migrating without any downtimes numerous applications. Without the customers even noticing!\nI also trained and helped other teams to adopt Kubernetes and the cloud, conducted very many interviews that resulted in hiring a number of very fine folks that I still hold dear to my heart to this day. I then again spear-headed a new transformation: Adopting the JVM, seen as more fitting to \'micro\'-services than C++, more specifically Kotlin. One Kotlin application I worked on was an internal (soft) real-time ledger application for accounting and compliance purposes using the Cassandra database and then Postgres. After deploying several production Kotlin services, I moved to the payment platform team where I led a brand new transformation (again!): Moving from batch services running at night (and usually being troubleshooted during the day), to a real time event based architecture using Kafka (and then later Kinesis). My work focused on writing from scratch the main producer of these events in Go: a bridge from a traditional RDBMS, to Kafka; as well as educating consumers on this new way of writing software. Challenges were plentiful: Events had to be first manually added to the existing C++ payment software, tracking down each location that mutated data and storing the right event in the database, without breaking the crucial payment flows. Then, our bridge would poll multiple such databases in multiple datacenters, (the databases being of course different RDBMS, versions, and OSes!), exporting (i.e. producing) in a live fashion these events to Kafka, 24/7/365. As more and more services consuming these events blossomed in the company in various teams (be it reporting, billing, compliance, different internal UIs, etc), correctness, reliability and performance were paramount and I made sure that we nailed these factors, among other ways, by improving observability (with metrics, logs, alerts, and opentelemetry), and by constant profiling. The surest way to care about the quality of your software is to be on-call for it, and I was. Another achievement of mine is helping a coworker finish a compliance project analyzing in real time each payment events being produced by my application and, based on rules conceived with business experts, deciding if a payment is fraudulent or not, potentially blocking it from progressing further and notifying a human to inspect it. Finally, I helped move the observability stack the company used to Datadog (by comparing several alternatives) and experimented with the serverless architecture by writing and deploying a production lambda in Go which ran flawlessly for months and never had an issue, costing less than 10$ to the company monthly, when the company was contemplating this path. I also had a short stint as a team manager, but after 2 months I decided this was not for me and stepped down. I am a Software Engineer and practitioner at heart. I look fondly on all these achievements, achieving business targets with a variety of tech stacks and cloud services, majorly contributing to fundamentally transform the company from a slow moving, datacenter based software stack, where developers sometimes wait for weeks for one deployment to happen, and a new project is a herculean effort of synchronizing every team; to a fast-moving, cloud based, self-service and event-oriented architecture, where each team is autonomous, gets a real-time stream of events containing all the information they need, and has nigh complete control and visibility on the whole lifecycle of their application. Senior Software Engineer Giesecke+Devrient, Munich, Germany; 2023-2025 After 6+ years in my job, I decided I was ready for the next challenge and joined my current company to help productionize an innovative Central Bank Digital Currency project (similar to a cryptocurrency, but backed by a sovereign state, using the state\'s currency, and with all the high standards one expects from regulated financial institutions). The most interesting feature, technically and product wise, is offline payments (card to card) with a focus on privacy. My first focus has been making the product more reliable and fast, relentlessly optimizing the performance and memory usage to ensure that an entire nation can use this suite of applications 24/7, surviving network disruptions, datacenter disasters, etc; while adding crucial business features such as non-repudiation of payments. My second focus has been security: going through regular threat analysis exercises with the team, adding scanning of dependencies and docker images to every project in order to find vulnerabilities or insecure code patterns, fuzzing, SBOM, establishing secure processes e.g. for vulnerability responses, reviewing cryptography code, etc. Indeed, I inherited a C++ codebase where the original author moved on, which was a central part of the company\'s offering. After investing some time to get it up to modern standards, I convinced stakeholders and developers to incrementally rewrite it in a memory-safe language (Rust). I led this effort by mentoring fellow developers, establishing a roadmap, doing the implementation work, and presenting regular demos of the progress to stakeholders. This software runs on various OSes and architectures including mobile platforms, and is now 100% Rust. My final project was to entirely refactor (but not rewrite) a stateful Go service to make it stateless, so that it can be run with multiple instances e.g. in multiple regions. The challenge was that the application state was held in numerous in-memory maps protected by mutexes, both on the Go side and on the C side, since this project used CGO extensively with some of the logic being done in C. I managed to move all of the state to the database and optimize database queries to keep performance the same or better than using in-memory state. Senior Software Engineer Ory, Munich, Germany; 2025-present To be continued... ",
titles:[
{
title:"Cr\u{e9}dit Mutuel (Bank), Strasbourg, France; 2013",
slug:"cr-dit-mutuel-bank-strasbourg-france-2013",
offset:302,
},
{
title:"CNRS Intern Software Engineer experimenting with the Oculus Rift (VR) CNRS, Strasbourg, France; 2014",
slug:"cnrs-intern-software-engineer-experimenting-with-the-oculus-rift-vr-cnrs-strasbourg-france-2014",
offset:1983,
},
{
title:"Full-stack Software Engineer EdgeLab, Lausanne, Switzerland; 2015-2017",
slug:"full-stack-software-engineer-edgelab-lausanne-switzerland-2015-2017",
offset:5479,
},
{
title:"Back-end Software Engineer &amp; DevOps PPRO, Munich, Germany; 2017-2023",
slug:"back-end-software-engineer-amp-devops-ppro-munich-germany-2017-2023",
offset:7673,
},
{
title:"Senior Software Engineer Giesecke+Devrient, Munich, Germany; 2023-2025",
slug:"senior-software-engineer-gieseckeplusdevrient-munich-germany-2023-2025",
offset:12420,
},
{
title:"Senior Software Engineer Ory, Munich, Germany; 2025-present",
slug:"senior-software-engineer-ory-munich-germany-2025-present",
offset:14731,
},
],
},
{
html_file_name:"image_size_reduction.html",
title:"Quick and easy PNG image size reduction",
text:"I seredenpitously noticed that my blog had somewhat big PNG images. But these are just very simple screenshots. There surely must be a way to reduce their size, without affecting their size or legibility?\nWell yes, let\'s quantize them!\nWhat? Quant-what? Quoting Wikipedia: Quantization, involved in image processing, is a lossy compression technique achieved by compressing a range of values to a single quantum (discrete) value. When the number of discrete symbols in a given stream is reduced, the stream becomes more compressible. For example, reducing the number of colors required to represent a digital image makes it possible to reduce its file size In other words, by picking the right color palette for an image, we can reduce its size without the human eye noticing. For example, an image which has multiple red variants, all very close, are a prime candidate to be converted to the same red color (perhaps the average value) so long as the human eye does not see the difference. Since PNG images use compression, it will compress better. At least, that\'s my layman understanding. Fortunately there is an open-source command line tool that is very easy to use and works great. So go give them a star and come back! I simply ran the tool on all images to convert them in place in parallel: $ ls *.png | parallel \'pngquant {} -o {}.tmp &amp;&amp; mv {}.tmp {}\' It finished instantly, and here is the result: $ git show 2e126f55a77e75e182ea18b36fb535a0e37793e4 --compact-summary\ncommit 2e126f55a77e75e182ea18b36fb535a0e37793e4 (HEAD -&gt; master, origin/master, origin/HEAD)\n\n    use pgnquant to shrink images\n\n feed.png                        | Bin 167641 -&gt; 63272 bytes\n gnuplot.png                     | Bin 4594 -&gt; 3316 bytes\n mem_prof1.png                   | Bin 157587 -&gt; 59201 bytes\n mem_prof2.png                   | Bin 209046 -&gt; 81028 bytes\n mem_prof3.png                   | Bin 75019 -&gt; 27259 bytes\n mem_prof4.png                   | Bin 50964 -&gt; 21345 bytes\n wayland-screenshot-floating.png | Bin 54620 -&gt; 19272 bytes\n wayland-screenshot-red.png      | Bin 101047 -&gt; 45230 bytes\n wayland-screenshot-tiled.png    | Bin 188549 -&gt; 107573 bytes\n wayland-screenshot-tiled1.png   | Bin 505994 -&gt; 170804 bytes\n x11_x64_black_window.png        | Bin 32977 -&gt; 16898 bytes\n x11_x64_final.png               | Bin 47985 -&gt; 16650 bytes\n 12 files changed, 0 insertions(+), 0 deletions(-) Eye-balling it, every image was on average halved. Not bad, for no visible difference! Initially, I wanted to use the new hotness: AVIF. Here\'s an example using the avifenc tool on the original image: $ avifenc feed.png feed.avif\n$ stat -c \'%n %s\' feed.{png,avif}\nfeed.png 167641\nfeed.avif 36034 That\'s almost a x5 reduction in size! However this format is not yet well supported by all browsers. It\'s recommended to still serve a PNG as fallback, which is a bit too complex for this blog. Still, this format is very promising so I thought I should mention it. So as of now, all PNG images on this blog are much lighter! Not too bad for 10m of work. ",
titles:[
],
},
{
html_file_name:"you_inherited_a_legacy_cpp_codebase_now_what.html",
title:"You\'ve just inherited a legacy C++ codebase, now what?",
text:"Discussions: Hacker News , Lobster.rs , /r/programming . I\'ve got great suggestions from the comments, see the addendum at the end! You were minding your own business, and out of nowhere something fell on your lap. Maybe you started a new job, or perhaps changed teams, or someone experienced just left. And now you are responsible for a C++ codebase. It\'s big, complex, idiosyncratic; you stare too long at it and it breaks in various interesting ways. In a word, legacy. But somehow bugs still need to be fixed, the odd feature to be added. In short, you can\'t just ignore it or better yet nuke it out of existence. It matters. At least to someone who\'s paying your salary. So, it matters to you. What do you do now? Well, fear not, because I have experience this many times in numerous places (the snarky folks in the back will mutter: what C++ codebase isn\'t exactly like I described above), and there is a way out, that\'s not overly painful and will make you able to actually fix the bugs, add features, and, one can dream, even rewrite it some day. So join me on a recollection of what worked for me and what one should absolutely avoid. And to be fair to C++, I do not hate it (per se), it just happens to be one of these languages that people abuse and invariably leads to a horrifying mess and poor C++ is just the victim here and the C++ committee will fix it in C++45, worry not, by adding std::cmake to the standard library and you\'ll see how it\'s absolutely a game changer, and - Ahem, ok let\'s go back to the topic at hand. So here\'s an overview of the steps to take: Get it to work locally, by only doing the minimal changes required in the code and build system, ideally none. No big refactorings yet, even if itches really bad! Get out the chainsaw and rip out everything that\'s not absolutely required to provide the features your company/open source project is advertising and selling Make the project enter the 21st century by adding CI, linters, fuzzing, auto-formatting, etc Finally we get to make small, incremental changes to the code, Rinse and repeat until you\'re not awaken every night by nightmares of Russian hackers p@wning your application after a few seconds of poking at it If you can, contemplate rewrite some parts in a memory safe language The overarching goal is exerting the least amount of effort to get the project in an acceptable state in terms of security, developer experience, correctness, and performance. It\'s crucial to always keep that in mind. It\'s not about \'clean code\', using the new hotness language features, etc. Ok, let\'s dive in! By the way, everything here applies to a pure C codebase or a mixed C and C++ codebase, so if that\'s you, keep reading! Get buy-in You thought I was going to compare the different sanitizers, compile flags, or build systems? No sir, before we do any work, we talk to people. Crazy, right? Software engineering needs to be a sustainable practice, not something you burn out of after a few months or years. We cannot do this after hours, on a death march, or even, alone! We need to convince people to support this effort, have them understand what we are doing, and why. And that encompasses everyone: your boss, your coworkers, even non-technical folks. And who knows, maybe you\'ll go on vacation and return to see that people are continuing this effort when you\'re out of office. All of this only means: explain in layman terms the problem with a few simple facts, the proposed solution, and a timebox. Simple right? For example (to quote South Park: All characters and events in this show\u{2014}even those based on real people\u{2014}are entirely fictional ): Hey boss, the last hire took 3 weeks to get the code building on his machine and make his first contribution. Wouldn\'t it be nice if, with minimal effort, we could make that a few minutes? Hey boss, I put quickly together a simple fuzzing setup (\'inputting random data in the app like a monkey and seeing what happens\'), and it manages to crash the app 253 times within a few seconds. I wonder what would happen if people try to do that in production with our app? Hey boss, the last few urgent bug fixes took several people and 2 weeks to be deployed in production because the app can only be built by this one build server with this ancient operating system that has not been supported for 8 years (FreeBSD 9, for the curious) and it kept failing. Oh by the way whenever this server dies we have no way to deploy anymore, like at all. Wouldn\'t it be nice to be able to build our app on any cheap cloud instance? Hey boss, we had a cryptic bug in production affecting users, it took weeks to figure out and fix, and it turns out if was due to undefined behavior (\'a problem in the code that\'s very hard to notice\') corrupting data, and when I run this industry standard linter (\'a program that finds issues in the code\') on our code, it detects the issue instantly. We should run that tool every time we make a change! Hey boss, the yearly audit is coming up and the last one took 7 months to pass because the auditor was not happy with what they saw. I have ideas to make that smoother. Hey boss, there is a security vulnerability in the news right now about being able to decrypt encrypted data and stealing secrets, I think we might be affected, but I don\'t know for sure because the cryptography library we use has been vendored (\'copy-pasted\') by hand with some changes on top that were never reviewed by anyone. We should clean that up and setup something so that we get alerted automatically if there is a vulnerability that affects us. And here\'s what to avoid, again totally, super duper fictional, never-really-happened-to-me examples: We are not using the latest C++ standard, we should halt all work for 2 weeks to upgrade, also I have no idea if something will break because we have no tests I am going to change a lot of things in the project on a separate branch and work on it for months. It\'s definitely getting merged at some point! ( narrator\'s voice: it wasn\'t) We are going to rewrite the project from scratch, it should take a few weeks tops We are going to improve the codebase, but no idea when it will be done or even what we are going to do exactly Ok, let\'s say that now you have buy-in from everyone that matters, let\'s go over the process: Every change is small and incremental. The app works before and works after. Tests pass, linters are happy, nothing was bypassed to apply the change (exceptions do happen but that\'s what they are, exceptional) If an urgent bug fix has to be made, it can be done as usual, nothing is blocked Every change is a measurable improvement and can be explained and demoed to non experts If the whole effort has to be suspended or stopped altogether (because of priorities shifting, budget reasons, etc), it\'s still a net gain overall compared to before starting it (and that gain is in some form measurable ) In my experience, with this approach, you keep everyone happy and can do the improvements that you really need to do. Alright, let\'s get down to business now! Write down the platforms you support This is so important and not many projects do it. Write in the README (you do have a README, right?). It\'s just a list of &lt;architecture&gt;-&lt;operating-system&gt; pair, e.g. x86_64-linux or aarch64-darwin , that your codebase officially supports. This is crucial for getting the build working on every one of them but also and we\'ll see later, removing cruft for platforms you do not support. If you want to get fancy, you can even write down which version of the architecture such as ARMV6 vs ARMv7, etc. That helps answer important questions such as: Can we rely on having hardware support for floats, or SIMD, or SHA256? Do we even care about supporting 32 bits? Are we ever running on a big-endian platform? (The answer is very likely: no, never did, never will - if you do, please email me with the details because that sounds interesting). Can a char be 7 bits? And an important point: This list should absolutely include the developers workstations. Which leads me to my next point: Get the build working on your machine You\'d be amazed at how many C++ codebase in the wild that are a core part of a successful product earning millions and they basically do not compile. Well, if all the stars are aligned they do. But that\'s not what I\'m talking about. I\'m talking about reliably, consistently building on all platforms you support. No fuss, no \'I finally got it building after 3 weeks of hair-pulling\' (this brings back some memories). It just works(tm). A small aparte here. I used to be really into Karate. We are talking 3, 4 training sessions a week, etc. And I distinctly remember one of my teachers telling me (picture a wise Asian sifu - hmm actually my teacher was a bald white guy... picture Steve Ballmer then): You do not yet master this move. Sometimes you do and sometimes you don\'t, so you don\'t. When eating with a spoon, do you miss your mouth one out of five times? And I carried that with me as a Software Engineer. \'The new feature works\' means it works every time. Not four out of five times. And so the build is the same. Experience has shown me that the best way to produce software in a fast and efficient way is to be able to build on your machine, and ideally even run it on your machine. Now if your project is humongous that may be a problem, your system might not even have enough RAM to complete the build. A fallback is to rent a big server somewhere and run your builds here. It\'s not ideal but better than nothing. Another hurdle is the code requiring some platform specific API, for example io_uring on Linux. What can help here is to implement a shim, or build inside a virtual machine on your workstation. Again, not ideal but better than nothing. I have done all of the above in the past and that works but building directly on your machine is still the best option. Get the tests passing on your machine First, if there are no tests, I am sorry. This is going to be really difficult to do any change at all. So go write some tests before doing any change to the code, make them pass, and come back. The easiest way is to capture inputs and outputs of the program running in the real world and write end-to-end tests based on that, the more varied the better. It will ensure there are no regressions when making changes, not that the behavior was correct in the first place, but again, better than nothing. So, now you have a test suite. If some tests fail, disable them for now. Make them pass, even if the whole test suite takes hours to run. We\'ll worry about that later. Write down in the README how to build and test the application Ideally it\'s one command to build and one for testing. At first it\'s fine if it\'s more involved, in that case the respective commands can be put in a build.sh and test.sh that encapsulate the madness. The goal is to have a non C++ expert be able to build the code and run the tests without having to ask you anything. Here some folks would recommend documenting the project layout, the architecture, etc. Since the next step is going to rip out most of it, I\'d say don\'t waste your time now, do that at the end. Find low hanging fruits to speed up the build and tests Emphasis on \'low hanging\'. No change of the build system, no heroic efforts (I keep repeating that in this article but this is so important). Again, in a typical C++ project, you\'d be amazed at how much work the build system is doing without having to do it at all. Try these ideas below and measure if that helps or not: Building and running tests of your dependencies . In a project which was using unittest++ as a test framework, built as a CMake subproject, I discovered that the default behavior was to build the tests of the test framework, and run them, every time! That\'s crazy. Usually there is a CMake variable or such to opt-out of this. Building and running example programs of your dependencies . Same thing as above, the culprit that time was mbedtls . Again, setting a CMake variable to opt-out of that solved it. Building and running the tests of your project by default when it\'s being included as a subproject of another parent project. Yeah the default behavior we just laughed at in our dependencies? It turns out we\'re doing the same to other projects! I am no CMake expert but it seems that there is no standard way to exclude tests in a build. So I recommend adding a build variable called MYPROJECT_TEST unset by default and only build and run tests when it is set. Typically only developers working on the project directly will set it. Same with examples, generating documentation, etc. Building all of a third-party dependency when you only need a small part of it: mbedtls comes to mind as a good citizen here since it exposes many compile-time flags to toggle lots of parts you might not need. Beware of the defaults, and only build what you need! Wrong dependencies listed for a target leading to rebuilding the world when it does not have to: most build systems have a way to output the dependency graph from their point of view and that can really help diagnose these issues. Nothing feels worse than waiting for minutes or hours for a rebuild, when deep inside, you know it should have only rebuilt a few files. Experiment with a faster linker: mold is one that can be dropped in and really help at no cost. However that really depends on how many libraries are being linked, whether that\'s a bottleneck overall, etc. Experiment with a different compiler, if you can: I have seen projects where clang is twice as fast as gcc, and others where there is no difference. Once that\'s done, here are a few things to additionally try, although the gains are typically much smaller or sometimes negative: LTO: off/on/thin Split debug information Make vs Ninja The type of file system in use, and tweaking its settings Once the iteration cycle feels ok, the code gets to go under the microscope. If the build takes ages, it\'s not realistic to want to modify the code. Remove all unnecessary code Dad, I see dead lines of code. (Get the reference? Well, ok then.) I have seen 30%, sometimes more, of a codebase, being completely dead code. That\'s lines of code you pay for every time you compile, you want to make a refactoring, etc. So let\'s rip them out. Here are some ways to go about it: The compiler has a bunch of -Wunused-xxx warnings, e.g. -Wunused-function . They catch some stuff, but not everything. Every single instance of these warnings should be addressed. Usually it\'s as easy as deleting the code, rebuilding and re-running the tests, done. In rare cases it\'s a symptom of a bug where the wrong function was called. So I\'d be somewhat reluctant to fully automate this step. But if you\'re confident in your test suite, go for it. Linters can find unused functions or class fields, e.g. cppcheck . In my experience there are quite a lot of false positives especially regarding virtual functions in the case of inheritance, but the upside is that these tools absolutely find unused things that the compilers did not notice. So, a good excuse for adding a linter to your arsenal, if not to the CI (more on that later). I have seen more exotic techniques were the linker is instructed to put each function in its own section and print every time a section is removed because it\'s detected to be unused at link time, but that results in so much noise e.g. about standard library functions being unused, that I have not found that really practical. Others inspect the generated assembly and compare which functions are present there with the source code, but that does not work for virtual functions. So, maybe worth a shot, depending on your case? Remember the list of supported platforms? Yeah, time to put it to use to kill all the code for unsupported platforms. Code trying to support ancient versions of Solaris on a project that exclusively ran on FreeBSD?  Out of the window it goes. Code trying to provide its own random number generator because maybe the platform we run on does not have one (of course it turned out that was never the case)? To the bin. Hundred of lines of code in case POSIX 2001 is not supported, when we only run on modern Linux and macOS? Nuke it. Checking if the host CPU is big-endian and swapping bytes if it is? Ciao (when was the last time you shipped code for a big-endian CPU? And if yes, how are you finding IBM?). That code introduced years ago for a hypothetical feature that never came? Hasta la vista. And the bonus for doing all of this, is not only that you sped up the build time by a factor of 5 with zero downside, is that, if your boss is a tiny bit technical, they\'ll love seeing PRs deleting thousands of lines of code. And your coworkers as well. Linters Don\'t go overboard with linter rules, add a few basic ones, incorporate them in the development life cycle, incrementally tweak the rules and fix the issues that pop up, and move on. Don\'t try to enable all the rules, it\'s just a rabbit hole of diminishing returns. I have used clang-tidy and cppcheck in the past, they can be helpful, but also incredibly slow and noisy, so be warned. Having no linter is not an option though. The first time you run the linter, it\'ll catch so many real issues that you\'ll wonder why the compiler is not detecting anything even with all the warnings on. Code formatting Wait for the appropriate moment where no branches are active (otherwise people will have horrendous merge conflicts), pick a code style at random, do a one time formatting of the entire codebase (no exceptions), typically with clang-format , commit the configuration, done. Don\'t waste any bit of saliva arguing about the actual code formatting. It only exists to make diffs smaller and avoid arguments, so do not argue about it! Sanitizers Same as linters, it can be a rabbit hole, unfortunately it\'s absolutely required to spot real, production affecting, hard to detect, bugs and to be able to fix them. -fsanitize=address,undefined is a good baseline. They usually do not have false positives so if something gets detected, go fix it. Run the tests with it so that issues get detected there as well. I even heard of people running the production code with some sanitizers enabled, so if your performance budget can allow it, it could be a good idea. If the compiler you (have to) use to ship the production code does not support sanitizers, you can at least use clang or such when developing and running tests. That\'s when the work you did on the build system comes in handy, it should be relatively easy to use different compilers. One thing is for sure: even in the best codebase in the world, with the best coding practices and developers, the second you enable the sanitizers, you absolutely will uncover horrifying bugs and memory leaks that went undetected for years. So do it. Be warned that fixing these can require a lot of work and refactorings.\nEach sanitizer also has options so it could be useful to inspect them if your project is a special snowflake. One last thing: ideally, all third-party dependencies should also be compiled with the sanitizers enabled when running tests, to spot issues in them as well. Add a CI pipeline As Bryan Cantrill once said (quoting from memory), \'I am convinced most firmware just comes out of the home directory of a developer\'s laptop\'. Setting up a CI is quick, free, and automates all the good things we have set up so far (linters, code formatting, tests, etc). And that way we can produce in a pristine environment the production binaries, on every change. If you\'re not doing this already as a developer, I don\'t think you really have entered the 21st century yet. Cherry on the cake: most CI systems allow for running the steps on a matrix of different platforms! So you can demonstrably check that the list of supported platforms is not just theory, it is real. Typically the pipeline just looks like make all test lint fmt so it\'s not rocket science. Just make sure that issues that get reported by the tools (linters, sanitizers, etc) actually fail the pipeline, otherwise no one will notice and fix them. Incremental code improvements Well that\'s known territory so I won\'t say much here. Just that lots of code can often be dramatically simplified. I remember iteratively simplifying a complicated class that manually allocated and (sometimes) deallocated memory, was meant to handle generic things, and so on. All the class did, as it turned out, was allocate a pointer, later check whether the pointer was null or not, and...that\'s it. Yeah that\'s a boolean in my book. True/false, nothing more to it. I feel that\'s the step that\'s the hardest to timebox because each round of simplification opens new avenues to simplify further. Use your best judgment here and stay on the conservative side. Focus on tangible goals such as security, correctness and performance, and stray away from subjective criteria such as \'clean code\'. In my experience, upgrading the C++ standard in use in the project can at times help with code simplifications, for example to replace code that manually increments iterators by a for (auto x : items) loop, but remember it\'s just a means to an end, not an end in itself. If all you need is std::clamp , just write it yourself. Rewrite in a memory safe language? I am doing this right now at work, and that deserves an article of its own. Lots of gotchas there as well. Only do this with a compelling reason. Conclusion Well, there you have it. A tangible, step-by-step plan to get out of the finicky situation that\'s a complex legacy C++ codebase. I have just finished going through that at work on a project, and it\'s become much more bearable to work on it now. I have seen coworkers, who previously would not have come within a 10 mile radius of the codebase, now make meaningful contributions. So it feels great. There are important topics that I wanted to mention but in the end did not, such as the absolute necessity of being able to run the code in a debugger locally, fuzzing, dependency scanning for vulnerabilities, etc. Maybe for the next article! If you go through this on a project, and you found this article helpful, shoot me an email! It\'s nice to know that it helped someone. Addendum: Dependency management This section is very subjective, it\'s just my strong, biased opinion. There\'s a hotly debated topic that I have so far carefully avoided and that\'s dependency management. So in short, in C++ there\'s none. Most people resort to using the system package manager, it\'s easy to notice because their README looks like this: On Ubuntu 20.04: `sudo apt install [100 lines of packages]`\n\nOn macOS: `brew install [100 lines of packages named slightly differently]`\n\nAny other: well you\'re out of luck buddy. I guess you\'ll have to pick a mainstream OS and reinstall \u{af}\\_(\u{30c4})_/\u{af} Etc. I have done it myself. And I think this is a terrible idea. Here\'s why: The installation instructions, as we\'ve seen above, are OS and distribution dependent. Worse, they\'re dependent on the version of the distribution. I remember a project that took months to move from Ubuntu 20.04 to Ubuntu 22.04, because they ship different versions of the packages (if they ship the same packages at all), and so upgrading the distribution also means upgrading the 100 dependencies of your project at the same time. Obviously that\'s a very bad idea. You want to upgrade one dependency at a time, ideally. There\'s always a third-party dependency that has no package and you have to build it from source anyway. The packages are never built with the flags you want. Fedora and Ubuntu have debated for years whether to build packaged with the frame pointer enabled (they finally do since very recently). Remember the section about sanitizers? How are you going to get dependencies with sanitizer enabled? It\'s not going to happen. But there are way more examples: LTO, -march , debug information, etc. Or they were built with a different C++ compiler version from the one you are using and they broke the C++ ABI between the two. You want to easily see the source of the dependency when auditing, developing, debugging, etc, for the version you are currently using . You want to be able to patch a dependency easily if you encounter a bug, and rebuild easily without having to change the build system extensively You never get the exact same version of a package across systems, e.g. when developer Alice is on macOS, Bob on Ubuntu and the production system on FreeBSD. So you have weird discrepancies you cannot reproduce and that\'s annoying. Corollary of the point above: You don\'t know exactly which version(s) you are using across systems and it\'s hard to produce a Bill of Material (BOM) in an automated fashion, which is required (or going to be required very soon? Anyway it\'s a good idea to have it) in some fields. The packages sometimes do not have the version of the library you need (static or dynamic) So you\'re thinking, I know, I will use those fancy new package managers for C++, Conan, vcpkg and the like! Well, not so fast: They require external dependencies so your CI becomes more complex and slower (e.g. figuring out which exact version of Python they require, which surely will be different from the version of Python your project requires) They do not have all versions of a package. Example: Conan and mbedtls , it jumps from version 2.16.12 to 2.23.0 . What happened to the versions in between? Are they flawed and should not be used? Who knows! Security vulnerabilities are not listed anyways for the versions available! Of course I had a project in the past where I had to use version 2.17 ... They might not support some operating systems or architectures you care about (FreeBSD, ARM, etc) I mean, if you have a situation where they work for you, that\'s great, it\'s definitely an improvement over using system packages in my mind. It\'s just that I never encountered (so far) a project where I could make use of them - there was always some blocker. So what do I recommend? Well, the good old git submodules and compiling from source approach. It\'s cumbersome, yes, but also: It\'s dead simple It\'s better than manually vendoring because git has the history and the diff functionalities You know exactly, down to the commit, which version of the dependency is in use Upgrading the version of a single dependency is trivial, just run git checkout It works on every platform You get to choose exactly the compilation flags, compiler, etc to build all the dependencies. And you can even tailor it per dependency! Developers know it already even if they have no C++ experience Fetching the dependencies is secure and the remote source is in git. No one is changing that sneakily. It works recursively (i.e.: transitively, for the dependencies of your dependencies) Compiling each dependency in each submodule can be as simple as add_subdirectory with CMake, or git submodule foreach make by hand. If submodules are really not an option, an alternative is to still compile from source but do it by hand, with one script, that fetches each dependency and builds it. Example in the wild: Neovim. Of course, if your dependency graph visualized in Graphviz looks like a Rorschach test and has to build thousands of dependencies, it is not easily doable, but it might be still possible, using a build system like Buck2, which does hybrid local-remote builds, and reuses build artifacts between builds from different users. If you look at the landscape of package managers for compiled languages (Go, Rust, etc), all of them that I know of compile from source. It\'s the same approach, minus git, plus the automation. Addendum: suggestions from readers I\'ve gathered here some great ideas and feedback from readers (sometimes it\'s the almagamation of multiple comments from different people, and I am paraphrasing from memory so sorry if it\'s not completely accurate): You should put more emphasis on tests (expanding the test suite, the code coverage, etc) - but also: a test suite in C++ is only worth anything when running it under sanitizers, otherwise you get lured into a false sense of safety. 100% agree. Modifying complex foreign code without tests is just not possible in my opinion. And yes, sanitizers will catch so many issues in the tests that you should even consider running your tests suite multiple time in CI with different sanitizers enabled. vcpkg is a good dependency manager for C++ that solves all of your woes. I\'ve never got the chance to use it so I\'ll add it to my toolbox to experiment with. If it matches the requirements I listed, as well as enabling cross-compilation, then yes it\'s absolutely a win over git submodules. Nix can serve as a good dependency manager for C++. I must admit that I was beaten into submission by Nix\'s complexity and slowness. Maybe in a few years when it has matured? You should not invest so much time in refactoring a legacy codebase if all you are going to do is one bug fix a year. Somewhat agree, but it really is a judgement call. In my experience it\'s never one and only one bug fix, whatever management says. And all the good things such as removing dead code, sanitizers etc will be valuable even for the odd bug fix and also lead to noticing more bugs and fixing them. As one replier put it: If you are going to own a codebase, then own it for real. It\'s very risky to remove code, in general you never know for sure if it\'s being used or not, and if someone relies on this exact behavior in the wild. That\'s true, that\'s why I advocate for removing code that is never called using static analysis tools, so that you know for sure . But yes, when in doubt, don\'t. My pet peeve here are virtual methods that are very resistant to static analysis (since the whole point is to pick which exact method to call at runtime), these usually cannot be as easily removed. Also, talk to your sales people, your product managers, heck even your users if you can. Most of the time, if you ask them whether a given feature or platform is in use or not, you\'ll get a swift yes or no reply, and you\'ll know how to proceed. We engineers sometimes forget that a 15 minute talk with people can simplify  so much technical work. Stick all your code in a LLM and start asking it questions : As a currently anti LLM person, I must admit that this idea never crossed my mind. However I think it might be worth a shot, if you can do that in a legally safe way, ideally fully locally, and take everything with a grain a salt. I\'m genuinely curious to see what answers it comes up with! There are tools that analyze the code and produce diagrams, class relationships etc to get an overview of the code : I never used these tools but that\'s a good idea and I will definitely try one in the future Step 0 should be to add the code in a source control system if that\'s not the case already : For sure. I was lucky enough to never encounter that, but heck yeah, even the worst source control system is better than no source control system at all. And I say this after having had to use Visual Source Safe, the one where modifying a file means acquiring an exclusive lock on it, that you then have to release manually. Setting up a CI should be step 1 : Fair point, I can totally see this perspective. I am quicker locally, but fair. Don\'t be a code beauty queen, just do the fixes you need : Amen. If you can drop a platform that\'s barely used to reduce the combinatorial complexity, and that enables you to do major simplifications, go for it : Absolutely. Talk to your sales people and stakeholders and try to convince them. In my case it was ancient FreeBSD versions long out of support, I think we used the security angle to convince everyone to drop them. Reproducible builds : This topic came up and was quite the debate. Honestly I don\'t think achieving a fully reproducible build in a typical C++ codebase is realistic. The compiler and standard library version alone are a problem since they usually are not considered in the build inputs. Achieving a reliable build though is definitely realistic. Docker came up on that topic. Now I have used Docker in anger since 2013 and I don\'t think it brings as much value as people generally think it does. But again - if all you can do is get the code building inside Docker, it\'s better than nothing at all. Git can be instructed to ignore one commit e.g. the one formatting the whole codebase so that git blame still works and the history still makes sense : Fantastic advice that I did not know before, so thanks! I\'ll definitely try that. Use the VCS statistics from the history to identify which parts of the codebase have the most churn and which ones get usually changed together : I never tried that, it\'s an interesting idea, but I also see lots of caveats. Probably worth a try? This article applies not only to C++ but also to legacy codebases in other languages : Thank you! I have the most experience in C++, so that was my point of view, but I\'m glad to hear that. Just skip the C++-specific bits like sanitizers. The book \'Working effectively with Legacy Code\' has good advice : I don\'t think I have ever read it from start to finish, so thanks for the recommendation. I seem to recall I skimmed it and found it very object-oriented specific with lots of OOP design patterns, and that was not helpful to me at the time, but my memory is fuzzy. Generally, touch as little as possible, focus on what adds value (actual value, like, sales dollars). : I agree generally (see the point: don\'t be beauty queen), however in a typical big C++ codebase, the moment you start to examine it under the lens of security, you will find lots and lots a security vulnerabilities that need fixing. And that does not translate in a financial gain - it\'s reducing risk. And I find that extremely valuable. Although, some fields are more sensitive than others. ",
titles:[
{
title:"Get buy-in",
slug:"get-buy-in",
offset:2708,
},
{
title:"Write down the platforms you support",
slug:"write-down-the-platforms-you-support",
offset:7071,
},
{
title:"Get the build working on your machine",
slug:"get-the-build-working-on-your-machine",
offset:8103,
},
{
title:"Get the tests passing on your machine",
slug:"get-the-tests-passing-on-your-machine",
offset:9925,
},
{
title:"Write down in the README how to build and test the application",
slug:"write-down-in-the-readme-how-to-build-and-test-the-application",
offset:10633,
},
{
title:"Find low hanging fruits to speed up the build and tests",
slug:"find-low-hanging-fruits-to-speed-up-the-build-and-tests",
offset:11208,
},
{
title:"Remove all unnecessary code",
slug:"remove-all-unnecessary-code",
offset:14055,
},
{
title:"Linters",
slug:"linters",
offset:16796,
},
{
title:"Code formatting",
slug:"code-formatting",
offset:17392,
},
{
title:"Sanitizers",
slug:"sanitizers",
offset:17838,
},
{
title:"Add a CI pipeline",
slug:"add-a-ci-pipeline",
offset:19236,
},
{
title:"Incremental code improvements",
slug:"incremental-code-improvements",
offset:20176,
},
{
title:"Rewrite in a memory safe language?",
slug:"rewrite-in-a-memory-safe-language",
offset:21328,
},
{
title:"Conclusion",
slug:"conclusion",
offset:21509,
},
{
title:"Addendum: Dependency management",
slug:"addendum-dependency-management",
offset:22295,
},
{
title:"Addendum: suggestions from readers",
slug:"addendum-suggestions-from-readers",
offset:27723,
},
],
},
{
html_file_name:"a_small_trick_to_improve_technical_discussions_by_sharing_code.html",
title:"A small trick to improve technical discussions by sharing code",
text:"This is a big title for a small trick that I\'ve been using daily for years now, in every place I\'ve worked at. Whenever there is a technical discussion, a bug hunt, or any disagreement about the codebase, I think it really helps to look at existing code to anchor the debate in reality and make it concrete. Copy pasting code, taking screenshots, or screen sharing may work at times but I have found a low-tech solution that\'s superior: Sharing a link to a region of code in the codebase. It\'s shorter, easier, and can be used in chats, documentation and PRs.\nIt works for any code, be it existing code on the main branch, or experimental code on a branch: Every web UI of every Version Control System (VCS) worth its salt has that feature, let\'s take Github for example: https://github.com/gaultier/micro-kotlin/blob/master/class_file.h#L773-L775 The hurdle is that every hosting provider has its own URL \'shape\' and it\'s not always documented, so there is a tiny bit of reverse-engineering involved. Compare the previous URL with this one: https://gitlab.com/philigaultier/jvm-bytecode/-/blob/master/class_file.h?ref_type=heads#L125-127 . It\'s slightly different. So to make it easy to share a link to code with coworkers, I\'ve written a tiny script to craft the URL for me, inside my editor. I select a few lines, hit a keystroke, and the URL is now in the clipboard for me to paste anywhere. Since I use Neovim and Lua, this is what I\'ll cover, but I\'m sure any editor can do that. Now that I think of it, there should be an existing extension for this? Back when I started using this trick I remember searching for one and finding nothing. This article could also serve as a gentle introduction to using Lua in Neovim. The code is also directly mappable to Vimscript, Vim9 script or anything really. So first thing first we need to create a user command to invoke this functionality and later map it to a keystroke: vim.api.nvim_create_user_command(\'GitWebUiUrlCopy\', function(arg)\nend,\n{force=true, range=true, nargs=0, desc=\'Copy to clipboard a URL to a git webui for the current line\'}) force=true overrides any previous definition which is handy when iterating over the implementation range=true allows for selecting multiple lines and calling this command on the line range, but it also works when not selecting anything (in normal mode) nargs=0 means that no argument is passed to the command We pass a callback to nvim_create_user_command which will be called when we invoke the command. For now it does nothing but we are going to implement it in a second. arg is an object containing for our purposes the start and end line numbers: local line_start = arg.line1\n  local line_end = arg.line2 And we also need to get the absolute path to the current file: local file_path_abs = vim.fn.expand(\'%:p\') From this point on explanations are git specific, but I\'m sure other VCSes have similar features. Note that since the current directory might be one or several directories deep relative to the root of the git repository, we need to fix this path, because the git web UI expects a path from the root of the git repository. The easiest way to do so is using git ls-files --full-name to convert the absolute path to the path from the root of the repostory. There are many ways in Neovim to call out to a command in a subprocess, here\'s one of them, to get the output of the command: local file_path_abs = vim.fn.expand(\'%:p\')\n  local file_path_rel_cmd = io.popen(\'git ls-files --full-name &quot;\' .. file_path_abs .. \'&quot;\')\n  local file_path_relative_to_git_root = file_path_rel_cmd:read(\'*a\')\n  file_path_rel_cmd.close() We also need to get the git URL of the remote (assuming there is only one, but it\'s easy to expand the logic to handle multiple): local cmd_handle = io.popen(\'git remote get-url origin\')\n  local git_origin = cmd_handle:read(\'*a\')\n  cmd_handle.close()\n  git_origin = string.gsub(git_origin, &quot;%s+$&quot;, &quot;&quot;) And the last bit of information we need is to get the current commit.\nIn the past, I just used the current branch name, however since this is a moving target, it meant that when opening the link, the code might be completely different than what it was when giving out the link. Using a fixed commit is thus better (assuming no one force pushes and messes with the history): local cmd_handle = io.popen(\'git rev-parse HEAD\')\n  local git_commit = cmd_handle:read(\'*a\')\n  cmd_handle.close()\n  git_commit = string.gsub(git_commit, &quot;%s+$&quot;, &quot;&quot;) Now, we can craft the URL by first extracting the interesting parts of the git remote URL and then tacking on at the end all the URL parameters precising the location.\nI assume the git remote URL is a ssh URL here, again it\'s easy to tweak to also handle https URL. Also note that this is the part that\'s hosting provider specific. Since I am mainly using Azure DevOps (ADO) and Github at the moment this is what I\'ll show. In ADO, the git remote URL looks like this: git@ssh.&lt;hostname&gt;:v3/&lt;organization&gt;/&lt;directory&gt;/&lt;project&gt; And the final URL looks like: https://&lt;hostname&gt;/&lt;organization&gt;/&lt;directory&gt;/_git/&lt;project&gt;?&lt;params&gt; In Github, the git remote URL looks like this: git@github.com:&lt;username&gt;/&lt;project&gt;.git And the final URL looks like this: https://github.com/&lt;username&gt;/&lt;project&gt;/blob/&lt;commit_id&gt;/&lt;file_path&gt;?&lt;params&gt; We inspect the git remote url to know in which case we are: local url = \'\'\n  if string.match(git_origin, \'github\') then\n    -- Handle Github\n  elseif string.match(git_origin, \'azure.com\') then\n    -- End is exclusive in that case hence the `+ 1`.\n    line_end = line_end + 1\n\n    -- Handle ADO\n  else\n    print(\'hosting provider not supported\')\n  end We use a Lua pattern to extract the components from the git remote URL using string.gmatch . It weirdly returns an iterator yielding only one result containing our matches, we use a for loop to do so (perhaps there is an easier way in Lua?): Here\'s for Github: for host, user, project in string.gmatch(git_origin, \'git@([^:]+):([^/]+)/([^/]+)%.git\') do\n      url = \'https://\' .. host .. \'/\' .. user .. \'/\' .. project .. \'/blob/\' .. git_commit .. \'/\' .. file_path_relative_to_git_root .. \'#l\' .. line_start .. \'-l\' .. line_end\n      break\n    end And here\'s for ADO: for host, org, dir, project in string.gmatch(git_origin, \'git@ssh%.([^:]+):v3/([^/]+)/([^/]+)/([^\\n]+)\') do\n    url = \'https://\' .. host .. \'/\' .. org .. \'/\' .. dir .. \'/_git/\' .. project .. \'?lineStartColumn=1&amp;lineStyle=plain&amp;_a=contents&amp;version=GC\' .. git_commit .. \'&amp;path=\' .. file_path_relative_to_git_root .. \'&amp;line=\' .. line_start .. \'&amp;lineEnd=\' .. line_end\n    break\n  end Finally we stick the result in the system clipboard, and we can even open the url in the default browser (I have only tested that logic on Linux but it should work on other OSes): -- Copy to clipboard.\n  vim.fn.setreg(\'+\', url)\n\n  -- Open URL in the default browser.\n  local os_name = vim.loop.os_uname().sysname\n  if os_name == \'Linux\' or os_name == \'FreeBSD\' or os_name == \'OpenBSD\' or os_name == \'NetBSD\' then\n    os.execute(\'xdg-open &quot;\' .. url .. \'&quot;\')\n  elseif os_name == \'Darwin\' then\n    os.execute(\'open &quot;\' .. url .. \'&quot;\')\n  elseif os_name == \'Windows\' then\n    os.execute(\'start &quot;\' .. url .. \'&quot;\')\n  else\n    print(\'Unknown os: \' .. os_name)\n  end We can now map the command to our favorite keystroke, for me space + x, for both normal mode ( n ) and visual mode ( v ): vim.keymap.set({\'v\', \'n\'}, \'&lt;leader&gt;x\', \':GitWebUiUrlCopy&lt;CR&gt;\') And that\'s it, just 60 lines of Lua, and easy to extend to support even more hosting providers. Addendum: the full code The full code vim.keymap.set({\'v\', \'n\'}, \'&lt;leader&gt;x\', \':GitWebUiUrlCopy&lt;CR&gt;\')\nvim.api.nvim_create_user_command(\'GitWebUiUrlCopy\', function(arg)\n  local file_path_abs = vim.fn.expand(\'%:p\')\n  local file_path_rel_cmd = io.popen(\'git ls-files --full-name &quot;\' .. file_path_abs .. \'&quot;\')\n  local file_path_relative_to_git_root = file_path_rel_cmd:read(\'*a\')\n  file_path_rel_cmd.close()\n\n  local line_start = arg.line1\n  local line_end = arg.line2\n\n  local cmd_handle = io.popen(\'git remote get-url origin\')\n  local git_origin = cmd_handle:read(\'*a\')\n  cmd_handle.close()\n  git_origin = string.gsub(git_origin, &quot;%s+$&quot;, &quot;&quot;)\n\n  local cmd_handle = io.popen(\'git rev-parse HEAD\')\n  local git_commit = cmd_handle:read(\'*a\')\n  cmd_handle.close()\n  git_commit = string.gsub(git_commit, &quot;%s+$&quot;, &quot;&quot;)\n\n  local url = \'\'\n  if string.match(git_origin, \'github\') then\n    for host, user, project in string.gmatch(git_origin, \'git@([^:]+):([^/]+)/([^/]+)%.git\') do\n      url = \'https://\' .. host .. \'/\' .. user .. \'/\' .. project .. \'/blob/\' .. git_commit .. \'/\' .. file_path_relative_to_git_root .. \'#L\' .. line_start .. \'-L\' .. line_end\n      break\n    end\n  elseif string.match(git_origin, \'azure.com\') then\n    -- End is exclusive in that case hence the `+ 1`.\n    line_end = line_end + 1\n\n    for host, org, dir, project in string.gmatch(git_origin, \'git@ssh%.([^:]+):v3/([^/]+)/([^/]+)/([^\\n]+)\') do\n      url = \'https://\' .. host .. \'/\' .. org .. \'/\' .. dir .. \'/_git/\' .. project .. \'?lineStartColumn=1&amp;lineStyle=plain&amp;_a=contents&amp;version=GC\' .. git_commit .. \'&amp;path=\' .. file_path_relative_to_git_root .. \'&amp;line=\' .. line_start .. \'&amp;lineEnd=\' .. line_end\n      break\n    end\n  else\n    print(\'Hosting provider not supported\')\n  end\n\n  -- Copy to clipboard.\n  vim.fn.setreg(\'+\', url)\n\n  -- Open URL in the default browser.\n  local os_name = vim.loop.os_uname().sysname\n  if os_name == \'Linux\' or os_name == \'FreeBSD\' or os_name == \'OpenBSD\' or os_name == \'NetBSD\' then\n    os.execute(\'xdg-open &quot;\' .. url .. \'&quot;\')\n  elseif os_name == \'Darwin\' then\n    os.execute(\'open &quot;\' .. url .. \'&quot;\')\n  elseif os_name == \'Windows\' then\n    os.execute(\'start &quot;\' .. url .. \'&quot;\')\n  else\n    print(\'Unknown os: \' .. os_name)\n  end\nend,\n{force=true, range=true, nargs=0, desc=\'Copy to clipboard a URL to a git webui for the current line\'}) ",
titles:[
{
title:"Addendum: the full code",
slug:"addendum-the-full-code",
offset:7736,
},
],
},
{
html_file_name:"how_to_rewrite_a_cpp_codebase_successfully.html",
title:"How to rewrite a C++ codebase successfully",
text:"Discussions: /r/programming , /r/rust , Lobsters Not your typical \'Rewrite it in Rust\' article. I recently wrote about inheriting a legacy C++ codebase . At some point, although I cannot pinpoint exactly when, a few things became clear to me: No one in the team but me is able - or feels confident enough - to make a change in this codebase This is a crucial project for the company and will live for years if not decades The code is pretty bad on all the criteria we care about: correctness, maintainability, security, you name it. I don\'t blame the original developers, they were understaffed and it was written as a prototype (the famous case of the prototype which becomes the production code). No hiring of C++ developers is planned or at least in the current budget (also because that\'s the only C++ project we have and we have many other projects to maintain and extend) So it was apparent to me that sticking with C++ was a dead end. It\'s simply a conflict of values and priorities: C++ values many things that are not that important in this project, such as performance above all; and it does not give any guarantees about things that are crucial to us, such as memory and temporal safety (special mention to integer under/overflows. Have fun reviewing every single instance of arithmetic operations to check if it can under/overflow). We bought a race car but what we needed was a family-friendly 5 seater, that\'s our mistake. The only solution would be to train everyone in the team on C++ and dedicate a significant amount of time rewriting the most problematic parts of the codebase to perhaps reach a good enough state, and even then, we\'d have little confidence our code is robust against nation-state attacks. It\'s a judgment call in the end, but that seemed to be more effort than \'simply\' introducing a new language and doing a rewrite. I don\'t actually like the term \'rewrite\'. Folks on the internet will eagerly repeat that rewrites are a bad idea, will undoubtedly fail, and are a sign of hubris and naivety. I have experienced such rewrites, from scratch, and yes that does not end well. However, I claim, because I\'ve done it, and many others before me, that an incremental rewrite can be successful, and is absolutely worth it. It\'s all about how it is being done, so here\'s how I proceeded and I hope it can be applied in other cases, and people find it useful. I think it\'s a good case study because whilst not a big codebase, it is a complex codebase, and it\'s used in production on 10+ different operating systems and architectures, including by external customers. This is not a toy. So join me on this journey, here\'s the guide to rewrite a C++ codebase successfully. And also what not do! The project This project is a library that exposes a C API but the implementation is C++, and it vendors C libraries (e.g. mbedtls) which we build from source. The final artifacts are a libfoo.a static library and a libfoo.h C header. It is used to talk to applets on a smart card like your credit card, ID, passport or driving license (yes, smart cards are nowadays everywhere - you probably carry several on you right now), since they use a bizarre interesting communication protocol. The library also implements a home-grown protocol on top of the well-specified smart card protocol, encryption, and business logic. It is meant to be part of an user-facing application running on smartphones and Point of Sales terminals, as well as in servers running in a datacenter or in the cloud. This library is used in: Android applications, through JNI Go back-end services running in Kubernetes, through CGO iOS applications, through Swift FFI C and C++ applications running on very small 32 bits ARM boards similar to the first Raspberry Pi Additionally, developers are using macOS (x64 and arm64) and Linux so the library needs to build and run on these platforms. Since external customers also integrate their applications with our library and we do not control these environments, and because some developer machines and servers use glibc and others musl,  we also need to work with either the glibc and the musl C libraries, as well as clang and gcc, and expose a C89-ish API, to maximize compatibility. Alright, now that the stage is set, let\'s go through the steps of rewriting this project. Improve the existing codebase That\'s basically all the steps in Inheriting a legacy C++ codebase . We need to start the rewrite with a codebase that builds and runs on every platform we support, with tests passing, and a clear README explaining how to setup the project locally. This is a small investment (a few days to a few weeks depending on the scale of the codebase) that will pay massive dividends in the future. But I think the most important point is to trim all the unused code which is typically the majority of the codebase! No one wants to spend time and effort on rewriting completely unused code. Additionally, if you fail to convince your team and the stakeholders to do the rewrite, you at least have improved the codebase you are now stuck with. So it\'s time well spent either way. Get buy-in Same as in my previous article: Buy-in from teammates and stakeholders is probably the most important thing to get, and maintain. It\'s a big investment in time and thus money we are talking about, it can only work with everyone on board. Here I think the way to go is showing the naked truth and staying very factual, in terms managers and non-technical people can understand. This is roughly what I presented: The bus factor for this project is 1 (me) Tool X shows that there are memory leaks at the rate of Y MiB/hour which means the application using our library will be OOM killed after around Z minutes/hours. Quick and dirty fuzzing manages to make the library crash 133 times in 10 seconds Linter X detects hundreds of real issues we need to fix All of these points make it really likely a hacker can exploit our library to gain Remote Code Execution (RCE) or steal secrets Essentially, it\'s a matter of genuinely presenting the alternative of rewriting being cheaper in terms of time and effort compared to improving the project with pure C++. If your teammates and boss are reality-based, it should be a straightforward decision. We use at my day job basically a RFC process to introduce a major change. That\'s great because it forces the person pushing for a change to document the current issues, the possible solutions, and allowing for a rational discussion to take place in the team. And documenting the whole process in a shared document (that allows comments) is very valuable because when people ask about it months later, you can just share the link to it. After the problematic situation has been presented, I think at least 3 different solutions should be presented and compared (including sticking with pure C++), and seriously consider each option. I find it important here to be as little emotionally invested as possible even if one option is your favorite, and to be ready to work for possibly months on your least favorite option, if it happens to be chosen by the collective. Ideally, if time permits, a small prototype for the preferred solution should be done, to confirm or infirm early that it can work, and to eliminate doubts. It\'s a much more compelling argument to say: &quot;Of course it will work, here is prototype I made, let\'s look at it together!&quot; compared to &quot;I hope it will work, but who knows, oh well I guess we\'ll see 3 months in...&quot;. After much debate, we settled on Rust as the new programming language being introduced into the codebase. It\'s important to note that I am not a Rust die hard fan. I appreciate the language but it\'s not perfect (see the FFI section later), it has issues, it\'s just that it solves all the issues we have in this project, especially considering the big focus on security (since we deal with payments),  the relative similarity with the company tech stack (Go), and the willingness of the team to learn it and review code in it. After all, the goal is also to gain additional developers, and stop being the only person who can even touch this code. I also seriously considered Go, but after doing a prototype, I was doubtful the many limitations of CGO would allow us to achieve the rewrite. Other teammates also had concerns on how the performance and battery usage would look like on low-end Android and Linux devices, especially 32 bits, having essentially two garbage collectors running concurrently, the JVM one and the Go one. Keeping buy-in Keeping buy-in after initially getting it is not a given, since software always takes longer than expected and unexpected hurdles happen all the time. Here, showing the progress through regular demos (weekly or biweekly is a good frequency) is great for stakeholders especially non-technical ones. And it can potentially motivate fellow developers to also learn the new language and help you out. Additionally, showing how long-standing issues in the old code get automatically solved by the new code, e.g. memory leaks, or fuzzing crashes in one function, are a great sign for stakeholders of the quality improving and the value of the on-going effort. Be prepared to repeat many many times the decision process that led to the rewrite to your boss, your boss\'s boss, the odd product manager who\'s not technical, the salesperson supporting the external customers, etc. It\'s important to nail the elevator\'s pitch. That applies also to teammates, who might be unsure the new programming language \'carries its weight\'. It helps to regularly ask them how they feel about the language, the on-going-effort, the roadmap, etc. Also, pairing with them, so that ideally, everyone in the team feels confident working on this project alone. Preparations to introduce the new language Before adding the first line of code in the new language, I created a Git tag last-before-rust . The commit right after introduced some code in Rust. This proved invaluable, because when rewriting the legacy code, I found tens of bugs lying around, and I think that\'s very typical. Also, this rewriting effort requires time, during which other team members or external customers may report bugs they just found. Every time such a bug appeared, I switched to this Git tag, and tried to reproduce the bug. Almost every time, the bug was already present before the rewrite. That\'s a very important information (for me, it was a relief!) for solving the bug, and also for stakeholders. That\'s the difference in their eye between: We are improving the product by fixing long existing bugs; or: we are introducing new bugs with our risky changes and we should maybe stop the effort completely because it\'s harming the product. Furthermore, I think the first commit introducing the new code should add dummy code and focus on making the build system and CI work seamlessly on every supported platform. This is not appealing work but it\'s necessary. Also, having instructions in the README explaining a bit what each tool does ( cargo , rustup , clippy , etc) is very valuable and will ease beginners into contributing in the new language. Incremental rewrite Along with stakeholder buy-in, the most important point in the article is that only an incremental rewrite can succeed, in my opinion. Rewriting from scratch is bound to fail, I think. At least I have never seen it succeed, and have seen it fail many times. What does it mean, very pragmatically? Well it\'s just a few rules of thumb: A small component is picked to be rewritten, the smallest, the better. Ideally it is as small as one function, or one class. The new implementation is written in the same Git (or whatever CVS you use) repository as the existing code, alongside it. It\'s a \'bug for bug\' implementation which means it does the exact same thing as the old implementation, even if the old seems sometimes non-sensical. In some cases, what the old code tries to do is so broken and insecure, that we have to do something different in the new code, but that should be rare. Tests for the new implementation are written and pass (so that we know the new implementation is likely correct) Each site calling the function/class is switched to using the new implementation. After each replacement, the test suite is run and passes (so that we know that nothing broke at the scale of the project; a kind of regression testing). The change is committed. That way is something breaks, we know exactly which change is the culprit. A small PR is opened, reviewed and merged. Since our changes are anyways incremental, it\'s up to us to decide that the current diff is of the right size for a PR. We can make the PR as big or small as we want. We can even make a PR with only the new implementation that\'s not yet used at all. Once the old function/class is not used anymore by any code, it can be \'garbage-collected\' i.e. safely removed. This can even be its own PR depending on the size. Rinse and repeat until all of the old code has been replaced There are of course thornier cases, but that\'s the gist of it. What\'s crucial is that each commit on the main branch builds and runs fine. At not point the codebase is ever broken, does not build, or is in an unknown state. It\'s actually not much different from the way I do a refactor in a codebase with just one programming language. What\'s very important to avoid are big PRs that are thousands lines long and nobody wants to review them, or long running branches that effectively create a multiverse inside the codebase. It\'s the same as regular software development, really. Here are a few additional tips I recommend doing: Starting from the leaves of the call graph is much easier than from the root. For example, if foo calls bar which calls baz , first rewriting baz then bar then foo is straightforward, but the reverse is usually not true. Thus, mapping out at the start from a high-level what are the existing components and which component calls out to which other component is invaluable in that regard, but also for establishing a rough roadmap for the rewrite, and reporting on the progress (&quot;3 components have been rewritten, 2 left to do!&quot;). Port the code comments from the old code to the new code if they make sense and add value. In my experience, a few are knowledge gems and should be kept, and most are completely useless noise. If you can use automated tools (search and replace, or tools operating at the AST level) to change every call site to use the new implementation, it\'ll make your reviewers very happy, and save you hours and hours of debugging because of a copy-paste mistake Since Rust and C++ can basically only communicate through a C API (I am aware of experimental projects to make them talk directly but we did not use those - we ultimately want 100% Rust code exposing a C API, just like the old C++ did), it means that each Rust function must be accompanied by a corresponding C function signature, so that C++ can call it as a C function. I recommend automating this process with cbindgen . I have encountered some limitations with it but it\'s very useful, especially to keep the implementation (in Rust) and the API (in C) in sync, or if your teammates are not comfortable with C. Automate when you can, for example I added the cbindgen code generation step to CMake so that rebuilding the C++ project would automatically run cbindgen as well as cargo build for the right target in the right mode (debug or release) for the right platforms ( --target=... ). DevUX matters! When rewriting a function/class, port the tests for this function/class to the new implementation to avoid reducing the code coverage each time Make the old and the new test suites fast so that the iteration time is short When a divergence is detected (a difference in output or side effects between the old and the new implementation), observe with tests or within the debugger the output of the old implementation (that\'s where the initial Git tag comes handy, and working with small commits) in detail so that you can correct the new implementation. Some people even develop big test suites verifying that the output of the old and the new implementation are exactly the same. Since it\'s a bug-for-bug rewrite, what the new implementation does may seem weird or unnecessarily convoluted but shall be kept (at least as a first pass). However, how it does it in the new code should be up to the best software engineering standards, that means tests, fuzzing, documentation, etc. Thread lightly, what can tank the project is being too bold when rewriting code and by doing so, introducing bugs or subtly changing the behavior which will cause breakage down the line. It\'s better to be conservative here. Pick a prefix for all structs and functions in the C API exposed by the Rust code, even if it\'s just RUST_xxx , so that they are immediately identifiable and greppable. Just like libcurl has the prefix curl_xxx . Finally, there is one hidden advantage of doing an incremental rewrite. A from-scratch rewrite is all or nothing, if it does not fully complete and replace the old implementation, it\'s useless and wasteful. However, an incremental rewrite is immediately useful, may be paused and continued a number of times, and even if the funding gets cut short and it never fully completes, it\'s still a clear improvement over the starting point. Fuzzing I am a fan a fuzzing, it\'s great. Almost every time I fuzz some code, I find a corner case I did not think about, especially when doing parsing. I added fuzzing to the project so that every new Rust function is fuzzed. I initially used AFL but then turned to cargo-fuzz , and I\'ll explain why. Fuzzing is only useful if code coverage is high . The worst that can happen is to dedicate serious time to setup fuzzing, to only discover at the end that the same few branches are always taken during fuzzing. Coverage can only be improved if developers can easily see exactly which branches are being executed during fuzzing. And I could not find an easy way with AFL to get a hold on that data. Using cargo-fuzz and various LLVM tools, I wrote a small shell script to visualize exactly which branches are taken during fuzzing as well as the code coverage in percents for each file and for the project as a whole (right now it\'s at around 90%). To get to a high coverage, the quality of the corpus data is paramount, since fuzzing works by doing small mutations of this corpus and observing which branches are taken as a result. I realized that the existing tests in C++ had lots of useful data in them, e.g.: const std::vector&lt;char&gt; input = {0x32, 0x01, 0x49, ...}; // &lt;= This is the interesting data.\nassert(foo(input) == ...); So I had the idea of extracting all the input = ... data from the tests to build a good fuzzing corpus. My first go at it was a hand-written quick and dirty C++ lexer in Rust. It worked but it was clunky. Right after I finished it, I thought: why don\'t I use tree-sitter to properly parse C++ in Rust? And so I did, and it turned out great, just 300 lines of Rust walking through each TestXXX.cpp file in the repository and using tree-sitter to extract each pattern. I used the query language of tree-sitter to do so: let query = tree_sitter::Query::new(\n    tree_sitter_cpp::language(),\n    &quot;(initializer_list (number_literal)+) @capture&quot;,\n) The tree-sitter website thankfully has a playground where I could experiment and tweak the query and see the results live. As time went on and more and more C++ tests were migrated to Rust tests, it was very easy to extend this small Rust program that builds the corpus data, to also scan the Rust tests! A typical Rust test would look like this: const INPUT: [u8; 4] = [0x01, 0x02, 0x03, 0x04]; // &lt;= This is the interesting data.\nassert_eq!(foo(&amp;INPUT), ...); And the query to extract the interesting data would be: let query = tree_sitter::Query::new(\n    tree_sitter_rust::language(),\n    // TODO: Maybe make this query more specific with:\n    // `(let_declaration value: (array_expression (integer_literal)+)) @capture`.\n    // But in a few cases, the byte array is defined with `const`, not `let`.\n    &quot;(array_expression (integer_literal)+) @capture&quot;,\n) However I discovered that not all data was successfully extracted. What about this code: const BAR : u8 = 0x42;\nconst INPUT: [u8; 4] = [BAR, 0x02, 0x03, 0x04]; // &lt;= This is the interesting data.\nassert_eq!(foo(&amp;INPUT), ...); We have a constant BAR which trips up tree-sitter, because it only sees a literal (i.e. 3 letters: \'B\', \'A\' and \'R\') and does not know its value. The way I solved this issue was to do two passes: once to collect all constants along with their values in a map, and then a second pass to find all arrays in tests: let query = tree_sitter::Query::new(\n    tree_sitter_rust::language(),\n    &quot;(const_item value: (integer_literal)) @capture &quot;,\n) So that we can then resolve the literals to their numeric value. That\'s how I implemented a compiler for the Kotlin programming language in the past and it worked great. Maybe there are more advanced approaches but this one is dead-simple and fast so it\'s good enough for us. I am pretty happy with how this turned out, scanning all C++ and Rust files to find interesting test data in them to build the corpus. I think this was key to move from the initial 20% code coverage with fuzzing (using a few hard-coded corpus files) to 90%. It\'s fast too. Also, it means the corpus gets better each time we had a test (be it in C++ or Rust), for free. Does it mean that the corpus will grow to an extreme size? Well, worry not, because LLVM comes with a fuzzing corpus minimizer: # Minimize the fuzzing corpus (in place).\n$ cargo +nightly fuzz cmin [...] For each file in the corpus, it feeds it as input to our code, observes which branches are taken, and if a new set of branches is taken, this file remains (or perhaps gets minimized even more, not sure how smart this tool is). Otherwise it is deemed a duplicate and is trimmed. So: We generate the corpus with our program Minimize it Run the fuzzing for however long we wish. It runs in CI for every commit and developers can also run it locally. When fuzzing is complete, we print the code coverage statistics Finally, we still have the option to add manually crafted files to this corpus if we wish. For example after hitting a bug in the wild, and fixing it, we can add a reproducer file to the corpus as a kind of regression test. Pure Rust vs interop (FFI) Writing Rust has been a joy, even for more junior developers in the team. Pure Rust code was pretty much 100% correct on the first try. However we had to use unsafe {} blocks in the FFI layer. We segregated all the FFI code to one file, and converted the C FFI structs to Rust idiomatic structs as soon as possible, so that the bulk of the Rust code can be idiomatic and safe. But that means this FFI code is the most likely part of the Rust code to have bugs. To get some confidence in its correctness, we write Rust tests using the C FFI functions (as if we were a C consumer of the library) running under Miri which acts as valgrind essentially, simulating a CPU and checking that our code is memory safe. Tests run perhaps 5 to 10 times as slow as without Miri but this has proven invaluable since it detected many bugs ranging from alignment issues to memory leaks and use-after-free issues. We run tests under Miri in CI to make sure each commit is reasonably safe. So beware: introducing Rust to a C or C++ codebase may actually introduce new memory safety issues, usually all located in the FFI code. Thankfully that\'s a better situation to be in than to have to inspect all of the codebase when a memory issue is detected. C FFI in Rust is cumbersome and error-prone The root cause for all these issues is that the C API that C++ and Rust use to call each other is very limited in its expressiveness w.r.t ownership, as well as many Rust types not being marked #[repr(C)] , even types you would expect to, such as Option , Vec or &amp;[u8] . That means that you have to define your own equivalent types: #[repr(C)]\n// An option type that can be used from C\npub struct OptionC&lt;T&gt; {\n    pub has_value: bool,\n    pub value: T,\n}\n\n\n#[repr(C)]\n// Akin to `&amp;[u8]`, for C.\npub struct ByteSliceView {\n    pub ptr: *const u8,\n    pub len: usize,\n}\n\n/// Owning Array i.e. `Vec&lt;T&gt;` in Rust or `std::vector&lt;T&gt;` in C++.\n#[repr(C)]\npub struct OwningArrayC&lt;T&gt; {\n    pub data: *mut T,\n    pub len: usize,\n    pub cap: usize,\n}\n\n/// # Safety\n/// Only call from C.\n#[no_mangle]\npub extern &quot;C&quot; fn make_owning_array_u8(len: usize) -&gt; OwningArrayC&lt;u8&gt; {\n    vec![0; len].into()\n} Apparently, Rust developers do not want to commit to a particular ABI for these types, to avoid missing out on some future optimizations. So it means that every Rust struct now needs the equivalent &quot;FFI friendly&quot; struct along with conversion functions (usually implemented as .into() for convenience): struct Foo&lt;\'a&gt; {\n    x: Option&lt;usize&gt;,\n    y: &amp;\'a [u8],\n    z: Vec&lt;u8&gt;,\n}\n\n\n#[repr(C)]\nstruct FooC {\n    x: OptionC&lt;usize&gt;,\n    y: ByteSliceView,\n    z: OwningArrayC&lt;u8&gt;,\n} Which is cumbersome but still fine, especially since Rust has powerful macros (which I investigated using but did not eventually). However, since Rust also does not have great idiomatic support for custom allocators, we stuck with the standard memory allocator, which meant that each struct with heap-allocated fields has to have a deallocation function: #[no_mangle]\npub extern &quot;C&quot; fn foo_free(foo: &amp;FooC) {\n    ...\n} And the C or C++ calling code would have to do: FooC foo{};\nif (foo_parse(&amp;foo, bytes) == SUCCESS) {\n    // do something with foo...\n    ...\n\n    foo_free(foo);\n} To simplify this, I introduced a defer construct to C++ (thanks Gingerbill!): FooC foo{};\ndefer({foo_free(foo);});\n\nif (foo_parse(&amp;foo, bytes) == SUCCESS) {\n    // do something with foo...\n    ...\n} Which feels right at home for Go developers, and is an improvement over the style in use in the old C++ code where it was fully manual calls to new/delete. Still, it\'s more work than what you\'d have to do in pure idiomatic Rust or C++ code (or even C code with arenas for that matter). In Zig or Odin, I would probably have used arenas to avoid that, or a general allocator with defer . An example of a real bug at the FFI boundary More perniciously, it\'s easy to introduce memory unsafety at the FFI boundary. Here is a real bug I introduced, can you spot it? I elided all the error handling to make it easier to spot: #[repr(C)]\nstruct BarC {\n    x: ByteSliceView,\n}\n\n#[no_mangle]\nunsafe extern &quot;C&quot; fn bar_parse(input: *const u8, input_len: usize, bar_c: &amp;mut BarC) {\n    let input: &amp;[u8] = unsafe { std::slice::from_raw_parts(input, input_len) };\n\n    let bar: Bar = Bar {\n        x: [input[0], input[1]],\n    };\n\n    *bar_c = BarC {\n        x: ByteSliceView {\n            ptr: bar.x.as_ptr(),\n            len: bar.x.len(),\n        },\n    };\n} clippy did not notice anything. address-sanitizer did not notice anything. However, both miri and valgrind did, and fuzzing crashed (which was not easy to troubleshoot but at least pinpointed to a problem). So...found it? Still nothing? Well, let\'s be good developers and add a test for it: #[test]\nfn bar() {\n    // This mimicks how C/C++ code would call our function.\n    let mut bar_c = MaybeUninit::&lt;BarC&gt;::uninit();\n    let input = [0, 1, 2];\n    unsafe {\n        bar_parse(\n            input.as_ptr(),\n            input.len(),\n            bar_c.as_mut_ptr().as_mut().unwrap(),\n        );\n    }\n\n    let bar_c = unsafe { bar_c.assume_init_ref() };\n    let x: &amp;[u8] = (&amp;bar_c.x).into();\n    assert_eq!(x, [0, 1].as_slice());\n} If you\'re lucky, cargo test would fail at the last assertion saying that the value is not what we expected, but in my case it passed every time, and so the bug stayed undetected for a while. That\'s because we unknowingly introduced undefined behavior, and as such, how or if it manifests is impossible to tell. Let\'s run the test with Miri: running 1 test\ntest api::tests::bar ... error: Undefined Behavior: out-of-bounds pointer use: alloc195648 has been freed, so this pointer is dangling\n    --&gt; src/tlv.rs:321:18\n     |\n321  |         unsafe { &amp;*core::ptr::slice_from_raw_parts(item.ptr, item.len) }\n     |                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ out-of-bounds pointer use: alloc195648 has been freed, so this pointer is dangling\n     |\n     = help: this indicates a bug in the program: it performed an invalid operation, and caused Undefined Behavior\n     = help: see https://doc.rust-lang.org/nightly/reference/behavior-considered-undefined.html for further information\nhelp: alloc195648 was allocated here:\n    --&gt; src/api.rs:1396:9\n     |\n1396 |     let bar: Bar = Bar {\n     |         ^^^\nhelp: alloc195648 was deallocated here:\n    --&gt; src/api.rs:1406:1\n     |\n1406 | } Miri is great, I tell you. The issue here is that we essentially return a pointer to local variable ( x ) from inside the function, so the pointer is dangling. Alternatively we can call our function from C/C++ and run that under valgrind: int main() {\n  BarC bar{};\n  const uint8_t input[] = {0, 1, 2, 3};\n  bar_parse(input, sizeof(input), &amp;bar);\n  assert(bar.x.ptr[0] == 0);\n  assert(bar.x.ptr[1] == 1);\n} And I get: ==805913== Conditional jump or move depends on uninitialised value(s)\n==805913==    at 0x127C34: main (src/example.cpp:13)\n==805913== \n==805913== Conditional jump or move depends on uninitialised value(s)\n==805913==    at 0x127C69: main (src/example.cpp:14) Which is not very informative, but better than nothing. Miri \'s output is much more actionable. So in conclusion, Rust\'s FFI capabilities work but are tedious are error-prone in my opinion, and so require extra care and testing with Miri/fuzzing, with high code coverage of the FFI functions. It\'s not enough to only test the pure (non FFI) Rust code. Another example of a real bug at the FFI boundary When I started this rewrite, I was under the impression that the Rust standard library uses the C memory allocator (basically, malloc ) under the covers when it needs to allocate some memory. However, I quickly discovered that it is not (anymore?) the case, Rust uses its own allocator - at least on Linux where there is no C library shipping with the kernel.\nMiri again is the MVP here since it detected the issue of mixing the C and Rust allocations which prompted this section. As Bryan Cantrill once said: &quot;glibc on Linux, it\'s just, like, your opinion dude&quot;. Meaning, glibc is just one option, among many, since Linux is just the kernel and does not ship with a libC. So the Rust standard library cannot expect a given C library on every Linux system, like it would be on macOS or the BSDs or Illumos. All of that to say: Rust implements its own memory allocator. The consequence of this, is that allocating memory on the C/C++ side, and freeing it on the Rust side, is undefined behavior: it amounts to freeing a pointer that was never allocated by this allocator. And vice-versa, allocating a pointer from Rust and freeing it from C. That has dire consequences since most memory allocators do not detect this in release mode. You might free completely unrelated memory leading to use-after-free later, or corrupt the memory allocator structures. It\'s bad. Here\'s a simplified example of code that triggered this issue: #[repr(C)]\npub struct FooC {\n    foo: u8,\n    bar: *mut usize,\n}\n\n#[no_mangle]\npub extern &quot;C&quot; fn parse_foo(in_bytes: *const u8, in_bytes_len: usize, foo: &amp;mut FooC) {\n    let in_bytes: &amp;[u8] = unsafe { &amp;*core::ptr::slice_from_raw_parts(in_bytes, in_bytes_len) };\n\n    // Parse `foo` from `in_bytes` but `bar` is sometimes not present in the payload.\n    // In that case it is set manually by the calling code.\n    *foo = FooC {\n        foo: in_bytes[0],\n        bar: if in_bytes_len == 1 {\n            core::ptr::null_mut()\n        } else {\n            let x = Box::new(in_bytes[1] as usize);\n            Box::into_raw(x)\n        },\n    }\n}\n\n#[no_mangle]\npub extern &quot;C&quot; fn free_foo(foo: &amp;mut FooC) {\n    if !foo.bar.is_null() {\n        unsafe {\n            let _ = Box::from_raw(foo.bar);\n        }\n    }\n} And the calling code: FooC foo{};\n\nconst uint8_t data[] = { 1 };\nparse_foo(data, sizeof(data), &amp;foo);\nif (foo.bar == nullptr) {\n  foo.bar = new size_t{99999};\n}\n\nfree_foo(&amp;foo); This is undefined behavior if the array is of size 1, since in that case the Rust allocator will free a pointer allocated by the C allocator, and address sanitizer catches it: SUMMARY: AddressSanitizer: alloc-dealloc-mismatch /home/runner/work/llvm-project/llvm-project/final/llvm-project/compiler-rt/lib/asan/asan_malloc_linux.cpp:52:3 in free However, it is only detected with sanitizers on and if a test (or fuzzing) triggers this case. Or by Miri if a Rust test covers this function. So I recommend sticking to one \'side\', be it C/C++ or Rust, of the FFI boundary, to allocate and free all the memory used in FFI structures. Rust has an edge here since the long-term goal is to have 100% of Rust so it will have to allocate all the memory anyway in the end. Depending on the existing code style, it might be hard to ensure that the C/C++ allocator is not used at all for structures used in FFI, due to abstractions and hidden memory allocations. One possible solution (which I did not implement but considered) is making FFI structures a simple opaque pointer (or \'handle\') so that the caller has to use FFI functions to allocate and free this structure. That also means implementing getter/setters for certain fields since the structures are now opaque. It maximizes the ABI compatibility, since the caller cannot rely on a given struct size, alignment, or fields. However that entails more work and more functions in the API. libcurl is an example of such an approach, libuv is an example of a library which did not do this initially, but plans to move to this approach in future versions, which would be a breaking change for clients. So to summarize, Miri is so essential that I don\'t know whether it\'s viable to write Rust code with lots of FFI (and thus lots of unsafe blocks) without it. If Miri did not exist, I would seriously consider using only arenas or reconsider the use of Rust. Cross-compilation Rust has great cross-compilation support; C++ not so much. Nonetheless I managed to coerced CMake into cross-compiling to every platform we support from my Linux laptop. After using Docker for more than 10 years I am firmly against using Docker for that, it\'s just clunky and slow and not a good fit. Also we already have to cross-compile to the mobile platforms anyway so why not make that work for all platforms? That way, I can even cross-compile tests and example programs in C or C++ using the library and run them inside qemu to make sure all platforms work as expected. I took inspiration from the CMake code in the Android project, which has to cross-compile for many architectures. Did you know that Android supports x86 (which is 32 bits), x86_64, arm (which is 32 bits), aarch64 (sometimes called arm64), and more? In short, you instruct CMake to cross-compile by supplying on the command-line the variables CMAKE_SYSTEM_PROCESSOR and CMAKE_SYSTEM_NAME , which are the equivalent of GOARCH and GOOS if you are familiar with Go. E.g.: $ cmake -B .build -S src -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DCMAKE_SYSTEM_NAME=Linux -DCMAKE_SYSTEM_PROCESSOR=arm On the Rust side, you tell cargo to cross-compile by supplying the --target command-line argument, e.g.: --target=x86_64-unknown-linux-musl . This works by virtue of installing the pre-compiled toolchain for this platform with rustup first: $ rustup target add x86_64-unknown-linux-musl So now we have to convert in CMake CMAKE_SYSTEM_ARCHITECTURE and CMAKE_SYSTEM_NAME into a target triple that clang and cargo can understand. Of course you have to do all the hard work yourself. This is complicated by lots of factors like Apple using the architecture name arm64 instead of aarch64 , iOS peculiarities, soft vs hard float, arm having multiple variants (v6, v7, v8, etc), and so on. Your mileage may vary. We opt-in into using musl with a CMake command line option, on Linux. Here it is in all its glory: # We need to craft the target triple to make it work when cross-compiling.\n# NOTE: If an architecture supports both soft-float and hard-float, we pick hard-float (`hf`).\n# since we do not target any real hardware with soft-float.\n# Linux has two main libcs, glibc (the default) and musl (opt-in with `FMW_LIBC_MUSL=1`), useful for Alpine.\nif (CMAKE_SYSTEM_NAME STREQUAL &quot;Linux&quot; AND CMAKE_SYSTEM_PROCESSOR STREQUAL &quot;x86_64&quot; AND NOT DEFINED FMW_LIBC_MUSL)\n    set(TARGET_TRIPLE &quot;x86_64-unknown-linux-gnu&quot;)\nelseif (CMAKE_SYSTEM_NAME STREQUAL &quot;Linux&quot; AND CMAKE_SYSTEM_PROCESSOR STREQUAL &quot;x86_64&quot; AND &quot;${FMW_LIBC_MUSL}&quot; EQUAL 1)\n    set(TARGET_TRIPLE &quot;x86_64-unknown-linux-musl&quot;)\nelseif (CMAKE_SYSTEM_NAME STREQUAL &quot;Linux&quot; AND CMAKE_SYSTEM_PROCESSOR STREQUAL &quot;arm&quot; AND NOT DEFINED FMW_LIBC_MUSL)\n    set(TARGET_TRIPLE &quot;arm-unknown-linux-gnueabihf&quot;)\nelseif (CMAKE_SYSTEM_NAME STREQUAL &quot;Linux&quot; AND CMAKE_SYSTEM_PROCESSOR STREQUAL &quot;arm&quot; AND &quot;${FMW_LIBC_MUSL}&quot; EQUAL 1)\n    set(TARGET_TRIPLE &quot;arm-unknown-linux-musleabihf&quot;)\nelseif (CMAKE_SYSTEM_NAME STREQUAL &quot;Linux&quot; AND CMAKE_SYSTEM_PROCESSOR STREQUAL &quot;aarch64&quot; AND NOT DEFINED FMW_LIBC_MUSL)\n    set(TARGET_TRIPLE &quot;aarch64-unknown-linux-gnu&quot;)\nelseif (CMAKE_SYSTEM_NAME STREQUAL &quot;Linux&quot; AND CMAKE_SYSTEM_PROCESSOR STREQUAL &quot;aarch64&quot; AND &quot;${FMW_LIBC_MUSL}&quot; EQUAL 1)\n    set(TARGET_TRIPLE &quot;aarch64-unknown-linux-musl&quot;)\nelseif (CMAKE_SYSTEM_NAME STREQUAL &quot;Linux&quot; AND CMAKE_SYSTEM_PROCESSOR STREQUAL &quot;armv7&quot;)\n    set(TARGET_TRIPLE &quot;armv7-unknown-linux-gnueabihf&quot;)\nelseif (CMAKE_SYSTEM_NAME STREQUAL &quot;Darwin&quot; AND CMAKE_SYSTEM_PROCESSOR STREQUAL &quot;aarch64&quot;)\n    set(TARGET_TRIPLE &quot;aarch64-apple-darwin&quot;)\nelseif (CMAKE_SYSTEM_NAME STREQUAL &quot;Darwin&quot; AND CMAKE_SYSTEM_PROCESSOR STREQUAL &quot;arm64&quot;)\n    set(TARGET_TRIPLE &quot;aarch64-apple-darwin&quot;)\nelseif (CMAKE_SYSTEM_NAME STREQUAL &quot;Darwin&quot; AND CMAKE_SYSTEM_PROCESSOR STREQUAL &quot;x86_64&quot;)\n    set(TARGET_TRIPLE &quot;x86_64-apple-darwin&quot;)\nelseif (CMAKE_SYSTEM_NAME STREQUAL &quot;iOS&quot; AND CMAKE_SYSTEM_PROCESSOR STREQUAL &quot;x86_64&quot;)\n    set(TARGET_TRIPLE &quot;x86_64-apple-ios&quot;)\n    execute_process(COMMAND xcrun --sdk iphonesimulator --show-sdk-path OUTPUT_VARIABLE CMAKE_OSX_SYSROOT)\n    string(REPLACE &quot;\\n&quot; &quot;&quot; CMAKE_OSX_SYSROOT ${CMAKE_OSX_SYSROOT})\nelseif (CMAKE_SYSTEM_NAME STREQUAL &quot;iOS&quot; AND CMAKE_SYSTEM_PROCESSOR STREQUAL &quot;aarch64&quot;)\n    set(TARGET_TRIPLE &quot;aarch64-apple-ios&quot;)\n    execute_process(COMMAND xcrun --sdk iphoneos --show-sdk-path OUTPUT_VARIABLE CMAKE_OSX_SYSROOT)\n    string(REPLACE &quot;\\n&quot; &quot;&quot; CMAKE_OSX_SYSROOT ${CMAKE_OSX_SYSROOT})\nelseif (CMAKE_SYSTEM_NAME STREQUAL &quot;iOS&quot; AND CMAKE_SYSTEM_PROCESSOR STREQUAL &quot;arm64&quot;)\n    set(TARGET_TRIPLE &quot;aarch64-apple-ios&quot;)\n    execute_process(COMMAND xcrun --sdk iphoneos --show-sdk-path OUTPUT_VARIABLE CMAKE_OSX_SYSROOT)\n    string(REPLACE &quot;\\n&quot; &quot;&quot; CMAKE_OSX_SYSROOT ${CMAKE_OSX_SYSROOT})\nelseif (CMAKE_SYSTEM_NAME STREQUAL &quot;Android&quot; AND CMAKE_SYSTEM_PROCESSOR STREQUAL &quot;arm&quot;)\n    set(TARGET_TRIPLE &quot;arm-linux-androideabi&quot;)\nelseif (CMAKE_SYSTEM_NAME STREQUAL &quot;Android&quot; AND CMAKE_SYSTEM_PROCESSOR STREQUAL &quot;armv7&quot;)\n    set(TARGET_TRIPLE &quot;armv7-linux-androideabi&quot;)\nelseif (CMAKE_SYSTEM_NAME STREQUAL &quot;Android&quot; AND CMAKE_SYSTEM_PROCESSOR STREQUAL &quot;armv7-a&quot;)\n    set(TARGET_TRIPLE &quot;armv7-linux-androideabi&quot;)\nelseif (CMAKE_SYSTEM_NAME STREQUAL &quot;Android&quot; AND CMAKE_SYSTEM_PROCESSOR STREQUAL &quot;aarch64&quot;)\n    set(TARGET_TRIPLE &quot;aarch64-linux-android&quot;)\nelseif (CMAKE_SYSTEM_NAME STREQUAL &quot;Android&quot; AND CMAKE_SYSTEM_PROCESSOR STREQUAL &quot;i686&quot;)\n    set(TARGET_TRIPLE &quot;i686-linux-android&quot;)\nelseif (CMAKE_SYSTEM_NAME STREQUAL &quot;Android&quot; AND CMAKE_SYSTEM_PROCESSOR STREQUAL &quot;x86_64&quot;)\n    set(TARGET_TRIPLE &quot;x86_64-linux-android&quot;)\nelse()\n    message(FATAL_ERROR &quot;Invalid OS/Architecture, not supported: CMAKE_SYSTEM_NAME=${CMAKE_SYSTEM_NAME} CMAKE_SYSTEM_PROCESSOR=${CMAKE_SYSTEM_PROCESSOR}&quot;)\nendif()\n\nmessage(STATUS &quot;Target triple: ${TARGET_TRIPLE}&quot;)\n\n# If we are cross compiling manually (e.g to Linux arm), `CMAKE_C_COMPILER_TARGET` and `CMAKE_CXX_COMPILER_TARGET` are unset and we need to set them manually.\n# But if we are cross compiling through a separate build system e.g. to Android or iOS, they will set these variables and we should not override them.\nif ( NOT DEFINED CMAKE_C_COMPILER_TARGET )\n   set(CMAKE_C_COMPILER_TARGET ${TARGET_TRIPLE})\nendif()\nif ( NOT DEFINED CMAKE_CXX_COMPILER_TARGET )\n   set(CMAKE_CXX_COMPILER_TARGET ${TARGET_TRIPLE})\nendif() There was a lot of trial and error as you can guess. Also, gcc is not directly supported for cross-compilation in this approach because gcc does not support a --target option like clang does, since it\'s not a cross-compiler. You have to download the variant you need e.g. gcc-9-i686-linux-gnu to compile for x86, and set CMAKE_C_COMPILER and CMAKE_CXX_COMPILER to gcc-9-i686-linux-gnu . However, in that case you are not setting CMAKE_SYSTEM_NAME and CMAKE_SYSTEM_PROCESSOR since it\'s in theory not cross-compiling, so cargo will not have its --target option filled, so it won\'t work for the Rust code. I advise sticking with clang in this setup. Still, when not cross-compiling, gcc works fine. Finally, I wrote a Lua script to cross-compile for every platform we support to make sure I did not break anything. I resorted to using the Zig toolchain (not the language) to be able to statically link with musl or cross-compile from Linux to iOS which I could not achieve with pure clang. However this is only my local setup, we do not use the Zig toolchain when building the production artifacts (e.g. the iOS build is done in a macOS virtual machine, not from a Linux machine). This is very useful also if you have several compile-time feature flags and want to build in different configurations for all platforms, e.g. enable/disable logs at compile time: local android_sdk = arg[1]\nif android_sdk == nil or android_sdk == &quot;&quot; then\n  print(&quot;Missing Android SDK as argv[1] e.g. \'~/Android/Sdk/ndk/21.4.7075529\'.&quot;)\n  os.exit(1)\nend\n\nlocal build_root = arg[2]\nif build_root == nil then\n  build_root = &quot;/tmp/&quot;\nend\n\nlocal rustup_targets = {\n  &quot;aarch64-apple-darwin&quot;,\n  &quot;aarch64-linux-android&quot;,\n  &quot;aarch64-unknown-linux-gnu&quot;,\n  &quot;aarch64-unknown-linux-musl&quot;,\n  &quot;arm-linux-androideabi&quot;,\n  &quot;arm-unknown-linux-gnueabihf&quot;,\n  &quot;arm-unknown-linux-musleabihf&quot;,\n  &quot;armv7-linux-androideabi&quot;,\n  &quot;armv7-unknown-linux-gnueabi&quot;,\n  &quot;armv7-unknown-linux-gnueabihf&quot;,\n  &quot;armv7-unknown-linux-musleabi&quot;,\n  &quot;armv7-unknown-linux-musleabihf&quot;,\n  &quot;i686-linux-android&quot;,\n  &quot;x86_64-apple-darwin&quot;,\n  &quot;x86_64-linux-android&quot;,\n  &quot;x86_64-unknown-linux-gnu&quot;,\n  &quot;x86_64-unknown-linux-musl&quot;,\n}\n\nfor i = 1,#rustup_targets do\n  local target = rustup_targets[i]\n  os.execute(&quot;rustup target install &quot; .. target)\nend\n\n\nlocal targets = {\n  {os=&quot;Linux&quot;, arch=&quot;x86_64&quot;, cc=&quot;clang&quot;, cxx=&quot;clang++&quot;, cmakeArgs=&quot;&quot;},\n  {os=&quot;Linux&quot;, arch=&quot;aarch64&quot;, cc=&quot;clang&quot;, cxx=&quot;clang++&quot;, cmakeArgs=&quot;&quot;},\n  {os=&quot;Linux&quot;, arch=&quot;arm&quot;, cc=&quot;clang&quot;, cxx=&quot;clang++&quot;, cmakeArgs=&quot;&quot;},\n  {os=&quot;Linux&quot;, arch=&quot;armv7&quot;, cc=&quot;clang&quot;, cxx=&quot;clang++&quot;, cmakeArgs=&quot;&quot;},\n  {os=&quot;Linux&quot;, arch=&quot;arm&quot;, cc=&quot;zig&quot;, cxx=&quot;zig&quot;, cmakeArgs=&quot;-DCMAKE_C_COMPILER_ARG1=cc -DCMAKE_CXX_COMPILER_ARG1=c++ -DFMW_LIBC_MUSL=1 -DCMAKE_C_COMPILER_TARGET=arm-linux-musleabihf -DCMAKE_CXX_COMPILER_TARGET=arm-linux-musleabihf&quot;},\n  {os=&quot;Linux&quot;, arch=&quot;aarch64&quot;, cc=&quot;zig&quot;, cxx=&quot;zig&quot;, cmakeArgs=&quot;-DCMAKE_C_COMPILER_ARG1=cc -DCMAKE_CXX_COMPILER_ARG1=c++ -DFMW_LIBC_MUSL=1 -DCMAKE_C_COMPILER_TARGET=aarch64-linux-musl -DCMAKE_CXX_COMPILER_TARGET=aarch64-linux-musl&quot;},\n  {os=&quot;Linux&quot;, arch=&quot;x86_64&quot;, cc=&quot;zig&quot;, cxx=&quot;zig&quot;, cmakeArgs=&quot;-DCMAKE_C_COMPILER_ARG1=cc -DCMAKE_CXX_COMPILER_ARG1=c++ -DFMW_LIBC_MUSL=1 -DCMAKE_C_COMPILER_TARGET=x86_64-linux-musl -DCMAKE_CXX_COMPILER_TARGET=x86_64-linux-musl&quot;},\n  {os=&quot;Darwin&quot;, arch=&quot;x86_64&quot;, cc=&quot;zig&quot;, cxx=&quot;zig&quot;, cmakeArgs=&quot;-DCMAKE_C_COMPILER_ARG1=cc -DCMAKE_CXX_COMPILER_ARG1=c++ -DFMW_LIBC_MUSL=1 -DCMAKE_C_COMPILER_TARGET=x86_64-macos-none -DCMAKE_CXX_COMPILER_TARGET=x86_64-macos-none&quot;},\n  {os=&quot;Darwin&quot;, arch=&quot;arm64&quot;, cc=&quot;zig&quot;, cxx=&quot;zig&quot;, cmakeArgs=&quot;-DCMAKE_C_COMPILER_ARG1=cc -DCMAKE_CXX_COMPILER_ARG1=c++ -DFMW_LIBC_MUSL=1 -DCMAKE_C_COMPILER_TARGET=aarch64-macos-none -DCMAKE_CXX_COMPILER_TARGET=aarch64-macos-none&quot;},\n  {os=&quot;Android&quot;, arch=&quot;armv7-a&quot;, cc=&quot;clang&quot;, cxx=&quot;clang++&quot;, cmakeArgs=&quot;-DCMAKE_ANDROID_NDK=\'&quot; .. android_sdk .. &quot;\'&quot;},\n  {os=&quot;Android&quot;, arch=&quot;aarch64&quot;, cc=&quot;clang&quot;, cxx=&quot;clang++&quot;, cmakeArgs=&quot;-DCMAKE_ANDROID_NDK=\'&quot; .. android_sdk .. &quot;\'&quot;},\n  {os=&quot;Android&quot;, arch=&quot;i686&quot;, cc=&quot;clang&quot;, cxx=&quot;clang++&quot;, cmakeArgs=&quot;-DCMAKE_ANDROID_NDK=\'&quot; .. android_sdk .. &quot;\'&quot;},\n  {os=&quot;Android&quot;, arch=&quot;x86_64&quot;, cc=&quot;clang&quot;, cxx=&quot;clang++&quot;, cmakeArgs=&quot;-DCMAKE_ANDROID_NDK=\'&quot; .. android_sdk .. &quot;\'&quot;},\n}\n\nfor i = 1,#targets do\n  local target = targets[i]\n  local build_dir = &quot;.build-&quot; .. target.os .. &quot;-&quot; .. target.arch .. &quot;-&quot; .. target.cc .. &quot;-&quot; .. target.cxx .. &quot;-&quot; .. target.cmakeArgs\n  build_dir = string.gsub(build_dir, &quot;%s+&quot;, &quot;_&quot;)\n  build_dir = string.gsub(build_dir, &quot;^./+&quot;, &quot;_&quot;)\n  build_dir = build_root .. &quot;/&quot; .. build_dir\n  print(build_dir)\n\n  local cmd_handle = io.popen(&quot;command -v llvm-ar&quot;)\n  local llvm_ar = cmd_handle:read(\'*a\')\n  cmd_handle:close()\n  llvm_ar = string.gsub(llvm_ar, &quot;%s+$&quot;, &quot;&quot;)\n\n  local cmd_handle = io.popen(&quot;command -v llvm-ranlib&quot;)\n  local llvm_ranlib = cmd_handle:read(\'*a\')\n  cmd_handle:close()\n  llvm_ranlib = string.gsub(llvm_ranlib, &quot;%s+$&quot;, &quot;&quot;)\n\n  local build_cmd = &quot;cmake -DCMAKE_BUILD_TYPE=RelWithDebInfo -B \'&quot; .. build_dir .. &quot;\' -DCMAKE_AR=&quot; .. llvm_ar .. &quot; -DCMAKE_RANLIB=&quot; .. llvm_ranlib .. &quot; -DCMAKE_SYSTEM_NAME=&quot; .. target.os .. &quot; -DCMAKE_SYSTEM_PROCESSOR=&quot; .. target.arch .. &quot; -DCMAKE_C_COMPILER=&quot; .. target.cc .. &quot; -DCMAKE_CXX_COMPILER=&quot; .. target.cxx .. &quot; &quot; ..  target.cmakeArgs .. &quot; -S src/. -G Ninja&quot;\n  print(build_cmd)\n  os.execute(build_cmd)\n\n  -- Work-around for getting rid of mbedtls linker flags specific to Apple\'s LLVM fork that are actually not needed.\n  if target.os == &quot;Darwin&quot; then\n    os.execute(&quot;sed -i \'&quot; .. build_dir .. &quot;/CMakeFiles/rules.ninja\' -e \'s/ -no_warning_for_no_symbols -c//g\'&quot;)\n  end\n\n  os.execute(&quot;ninja -C \'&quot; .. build_dir .. &quot;\'&quot;)\nend I look forward to only having Rust code and deleting all of this convoluted stuff. That\'s something that people do not mention often when saying that modern C++ is good enough and secure enough. Well, first I disagree with this statement, but more broadly, the C++ toolchain to cross-compile sucks. You only have clang that can cross-compile in theory but in practice you have to resort to the Zig toolchain to automate cross-compiling the standard library etc. Also, developers not deeply familiar with either C or C++ do not want to touch all this CMake/Autotools with a ten-foot pole. And I understand them. Stockholm syndrome notwithstanding, these are pretty slow, convoluted, niche programming languages and no one wants to actively learn and use them unless they have to. Once you are used to simply typing go build or cargo build , you really start to ask yourself if those weird things are worth anyone\'s time. Conclusion The rewrite is not yet fully done, but we have already more Rust code than C++ code, and it\'s moving along nicely, at our own pace (it\'s not by far the only project we have on our lap). Once all C++ code is removed, we will do a final pass to remove the CMake stuff and build directly via cargo . We\'ll see if that works when integrating with other build systems e.g. Bazel for Android or Xcode for iOS. Developers who learned Rust are overall very happy with it and did not have too many fights with the borrow checker, with one notable exception of trying to migrate a C struct that used an intrusive linked list (ah, the dreaded linked list v. borrow checker!). My suggestion was to simply use a Vec in Rust since the linked list was not really justified here, and the problem was solved. Adding unit tests was trivial in Rust compared to C++ and as a result people would write a lot more of them. Built-in support for tests is expected in 2024 by developers. I don\'t think one single C++ test was written during this rewrite, now that I think of it. Everyone was really satisfied with the tooling, even though having to first do rustup target add ... before cross-compiling tripped up a few people, since in Go that\'s done automatically behind the scenes (I think one difference is that Go compiles everything from source and so does not need to download pre-compiled blobs?). Everyone also had an easy time with their text editor/IDE, Rust is ubiquitous enough now that every editor will have support for it. All the tooling we needed to scan dependencies for vulnerabilities, linting, etc was present and polished. Shootout to osv-scanner from Google, which allowed us to scan both the Rust and C++ dependencies in the same project (and it evens supports Go). As expected, developers migrating C++ code to Rust code had a breeze with the Rust code and almost every time asked for assistance when dealing with the C++ code. C++ is just too complex a language for most developers, especially compared to its alternatives. CMake/Make/Ninja proved surprisingly difficult for developers not accustomed to them, but I mentioned that already. I think half of my time during this rewrite was actually spent coercing all the various build systems (Bazel/Xcode/CMake/cargo/Go) on the various platforms into working well together. If there is no one in the team who\'s really familiar with build systems, I think this is going to be a real challenge. So, I hope this article alleviated your concerns about rewriting your C++ codebase. It can absolutely be done, just pick the right programming language for you and your context, do it incrementally, don\'t overpromise, establish a rough roadmap with milestones, regularly show progress to stakeholders (even if it\'s just you, it helps staying motivated!), and make sure the team is on-board and enjoying the process. You know, like any other software project, really! ",
titles:[
{
title:"The project",
slug:"the-project",
offset:2720,
},
{
title:"Improve the existing codebase",
slug:"improve-the-existing-codebase",
offset:4314,
},
{
title:"Get buy-in",
slug:"get-buy-in",
offset:5114,
},
{
title:"Keeping buy-in",
slug:"keeping-buy-in",
offset:8551,
},
{
title:"Preparations to introduce the new language",
slug:"preparations-to-introduce-the-new-language",
offset:9798,
},
{
title:"Incremental rewrite",
slug:"incremental-rewrite",
offset:11173,
},
{
title:"Fuzzing",
slug:"fuzzing",
offset:17422,
},
{
title:"Pure Rust vs interop (FFI)",
slug:"pure-rust-vs-interop-ffi",
offset:22560,
},
{
title:"C FFI in Rust is cumbersome and error-prone",
slug:"c-ffi-in-rust-is-cumbersome-and-error-prone",
offset:23819,
},
{
title:"An example of a real bug at the FFI boundary",
slug:"an-example-of-a-real-bug-at-the-ffi-boundary",
offset:26511,
},
{
title:"Another example of a real bug at the FFI boundary",
slug:"another-example-of-a-real-bug-at-the-ffi-boundary",
offset:30190,
},
{
title:"Cross-compilation",
slug:"cross-compilation",
offset:34603,
},
{
title:"Conclusion",
slug:"conclusion",
offset:49386,
},
],
},
{
html_file_name:"write_a_video_game_from_scratch_like_1987.html",
title:"Let\'s write a video game from scratch like it\'s 1987",
text:"Discussions: Hacker News , /r/programming In a previous article I\'ve done the \'Hello, world!\' of GUIs in assembly: A black window with a white text, using X11 without any libraries, just talking directly over a socket. In a later article I\'ve done the same with Wayland in C, displaying a static image. I showed that this is not complex and results in a very lean and small application. Recently, I stumbled upon this Hacker News post : Microsoft\'s official Minesweeper app has ads, pay-to-win, and is hundreds of MBs And I thought it would be fun to make with the same principles a full-fledged GUI application: the cult video game Minesweeper. Will it be hundred of megabytes when we finish? How much work is it really? Can a hobbyist make this in a few hours?         Your browser doesn\'t support this video. Here is a link to the video instead. Screencast Press enter to reset and press any mouse button to uncover the cell under the mouse cursor. Here is a Youtube link in case the video does not play (I tried lots of things so that it plays on iOS to no avail). The result is a ~300 KiB statically linked executable, that requires no libraries, and uses a constant ~1 MiB of resident heap memory (allocated at the start, to hold the assets). That\'s roughly a thousand times smaller in size than Microsoft\'s. And it only is a few hundred lines of code. The advantage of this approach is that the application is tiny and stand-alone: statically linked with the few bits of libC it uses (and that\'s it), it can be trivially compiled on every Unix, and copied around, and it will work on every machine (with the same OS/architecture that is). Even on ancient Linuxes from 20 years ago. I remember playing this game as a kid (must have been on Windows 98). It was a lot of fun! I don\'t exactly remember the rules though so it\'s a best approximation. If you spot an error, please open a Github issue ! And the source code repository for the game is here . What we\'re making The 11th version of the X protocol was born in 1987 and has not changed since. Since it predates GPUs by a decade or so, its model does not really fit the hardware of today. Still, it\'s everywhere. Any Unix has a X server, even macOS with XQuartz, and now Windows supports running GUI Linux applications inside WSL! X11 has never been so ubiquitous. The protocol is relatively simple and the entry bar is low: we only need to create a socket and we\'re off the races. And for 2D applications, there\'s no need to be a Vulkan wizard or even interact with the GPU. Hell, it will work even without any GPU! Everyone writing GUIs these days use a giant pile of libraries, starting with the overly complicated venerable libX11 and libxcb libraries, to Qt and SDL. Here are the steps we need to take: Open a window Upload image data (the one sprite with all the assets) Draw parts of the sprite to the window React to keyboard/mouse events And that\'s it. Spoiler alert: every step is 1-3 X11 messages that we need to craft and send. The only messages that we receive are the keyboard and mouse events. It\'s really not much at all! We will implement this in the Odin programming language which I really enjoy. But if you want to follow along with C or anything really, go for it. All we need is to be able to open a Unix socket, send and receive data on it, and load an image into memory. We will use PNG for that, since Odin has in its standard library support for PNGs, but we could also very easily use a simple format like PPM (like I did in the linked Wayland article) that is trivial to parse. Since Odin has support for both in its standard library, it does not really matter, and I stuck with PNG since it\'s more space-efficient. Finally, if you\'re into writing X11 applications even with libraries, lots of things in X11 are undocumented or underdocumented, and this article can be a good learning resource. As a bonus, you can also follow along with pure Wayland, using my previous Wayland article. Or perhaps you simply enjoy, like me, peeking behind the curtain to understand the magician\'s tricks. It almost always ends up with: &quot;That\'s it? That\'s all there is to it?&quot;. Authentication In previous articles, we connected to the X server without any authentication. Let\'s be a bit more refined: we now also support the X authentication protocol. That\'s because when running under Wayland with XWayland in some desktop environments like Gnome, we have to use authentication. This requires our application to read a 16 bytes long token that\'s present in a file in the user\'s home directory, and include it in the handshake we send to the X server. This mechanism is called MIT-MAGIC-COOKIE-1 . The catch is that this file contains multiple tokens for various authentication mechanisms, and network hosts. Remember, X11 is designed to work over the network. However we only care here about the entry for localhost. So we need to parse a little bit. It\'s basically what libXau does. From its docs: The .Xauthority file is a binary file consisting of a sequence of entries\nin the following format:\n\t2 bytes\t\tFamily value (second byte is as in protocol HOST)\n\t2 bytes\t\taddress length (always MSB first)\n\tA bytes\t\thost address (as in protocol HOST)\n\t2 bytes\t\tdisplay &quot;number&quot; length (always MSB first)\n\tS bytes\t\tdisplay &quot;number&quot; string\n\t2 bytes\t\tname length (always MSB first)\n\tN bytes\t\tauthorization name string\n\t2 bytes\t\tdata length (always MSB first)\n\tD bytes\t\tauthorization data string First let\'s define some types and constants: AUTH_ENTRY_FAMILY_LOCAL: u16 : 1\nAUTH_ENTRY_MAGIC_COOKIE: string : &quot;MIT-MAGIC-COOKIE-1&quot;\n\nAuthToken :: [16]u8\n\nAuthEntry :: struct {\n\tfamily:    u16,\n\tauth_name: []u8,\n\tauth_data: []u8,\n} We only define fields we are interested in. Let\'s now parse each entry accordingly: read_x11_auth_entry :: proc(buffer: ^bytes.Buffer) -&gt; (AuthEntry, bool) {\n\tentry := AuthEntry{}\n\n\t{\n\t\tn_read, err := bytes.buffer_read(buffer, mem.ptr_to_bytes(&amp;entry.family))\n\t\tif err == .EOF {return {}, false}\n\n\t\tassert(err == .None)\n\t\tassert(n_read == size_of(entry.family))\n\t}\n\n\taddress_len: u16 = 0\n\t{\n\t\tn_read, err := bytes.buffer_read(buffer, mem.ptr_to_bytes(&amp;address_len))\n\t\tassert(err == .None)\n\n\t\taddress_len = bits.byte_swap(address_len)\n\t\tassert(n_read == size_of(address_len))\n\t}\n\n\taddress := make([]u8, address_len)\n\t{\n\t\tn_read, err := bytes.buffer_read(buffer, address)\n\t\tassert(err == .None)\n\t\tassert(n_read == cast(int)address_len)\n\t}\n\n\tdisplay_number_len: u16 = 0\n\t{\n\t\tn_read, err := bytes.buffer_read(buffer, mem.ptr_to_bytes(&amp;display_number_len))\n\t\tassert(err == .None)\n\n\t\tdisplay_number_len = bits.byte_swap(display_number_len)\n\t\tassert(n_read == size_of(display_number_len))\n\t}\n\n\tdisplay_number := make([]u8, display_number_len)\n\t{\n\t\tn_read, err := bytes.buffer_read(buffer, display_number)\n\t\tassert(err == .None)\n\t\tassert(n_read == cast(int)display_number_len)\n\t}\n\n\tauth_name_len: u16 = 0\n\t{\n\t\tn_read, err := bytes.buffer_read(buffer, mem.ptr_to_bytes(&amp;auth_name_len))\n\t\tassert(err == .None)\n\n\t\tauth_name_len = bits.byte_swap(auth_name_len)\n\t\tassert(n_read == size_of(auth_name_len))\n\t}\n\n\tentry.auth_name = make([]u8, auth_name_len)\n\t{\n\t\tn_read, err := bytes.buffer_read(buffer, entry.auth_name)\n\t\tassert(err == .None)\n\t\tassert(n_read == cast(int)auth_name_len)\n\t}\n\n\tauth_data_len: u16 = 0\n\t{\n\t\tn_read, err := bytes.buffer_read(buffer, mem.ptr_to_bytes(&amp;auth_data_len))\n\t\tassert(err == .None)\n\n\t\tauth_data_len = bits.byte_swap(auth_data_len)\n\t\tassert(n_read == size_of(auth_data_len))\n\t}\n\n\tentry.auth_data = make([]u8, auth_data_len)\n\t{\n\t\tn_read, err := bytes.buffer_read(buffer, entry.auth_data)\n\t\tassert(err == .None)\n\t\tassert(n_read == cast(int)auth_data_len)\n\t}\n\n\treturn entry, true\n} Now we can sift through the different entries in the file to find the one we are after: load_x11_auth_token :: proc(allocator := context.allocator) -&gt; (token: AuthToken, ok: bool) {\n\tcontext.allocator = allocator\n\tdefer free_all(allocator)\n\n\tfilename_env := os.get_env(&quot;XAUTHORITY&quot;)\n\n\tfilename :=\n\t\tlen(filename_env) != 0 \\\n\t\t? filename_env \\\n\t\t: filepath.join([]string{os.get_env(&quot;HOME&quot;), &quot;.Xauthority&quot;})\n\n\tdata := os.read_entire_file_from_filename(filename) or_return\n\n\tbuffer := bytes.Buffer{}\n\tbytes.buffer_init(&amp;buffer, data[:])\n\n\n\tfor {\n\t\tauth_entry := read_x11_auth_entry(&amp;buffer) or_break\n\n\t\tif auth_entry.family == AUTH_ENTRY_FAMILY_LOCAL &amp;&amp;\n\t\t   slice.equal(auth_entry.auth_name, transmute([]u8)AUTH_ENTRY_MAGIC_COOKIE) &amp;&amp;\n\t\t   len(auth_entry.auth_data) == size_of(AuthToken) {\n\n\t\t\tmem.copy_non_overlapping(\n\t\t\t\traw_data(&amp;token),\n\t\t\t\traw_data(auth_entry.auth_data),\n\t\t\t\tsize_of(AuthToken),\n\t\t\t)\n\t\t\treturn token, true\n\t\t}\n\t}\n\n    // Did not find a fitting token.\n\treturn {}, false\n} Odin has a nice shorthand to return early on errors: or_return , which is the equivalent of ? in Rust or try in Zig. Same thing with or_break . And we use it in this manner in main : main :: proc() {\n\tauth_token, _ := load_x11_auth_token(context.temp_allocator)\n} If we did not find a fitting token, no matter, we will simply carry on with an empty one. One interesting thing: in Odin, similarly to Zig, allocators are passed to functions wishing to allocate memory. Contrary to Zig though, Odin has a mechanism to make that less tedious (and more implicit as a result) by essentially passing the allocator as the last function argument which is optional. Odin is nice enough to also provide us two allocators that we can use right away: A general purpose allocator, and a temporary allocator that uses an arena. Since authentication entries can be large, we have to allocate - the stack is only so big. It would be unfortunate to stack overflow because a hostname is a tiny bit too long in this file. Some readers have pointed out that it is likely it would all fit on the stack here, but this was also a perfect opportunity to describe Odin\'s approach to memory management. However, we do not want to retain the parsed entries from the file in memory after finding the 16 bytes token, so we defer free_all(allocator) . This is much better than going through each entry and freeing individually each field. We simply free the whole arena in one swoop (but the backing memory remains around to be reused later). Furthermore, using this arena places an upper bound (a few MiBs) on the allocations we can do. So if one entry in the file is huge, or malformed, we verifyingly cannot allocate many GiBs of memory. This is good news, because otherwise, the OS will start swapping like crazy and start killing random programs. In my experience it usually kills the window/desktop manager which kills all open windows. Very efficient from the OS perspective, and awful from the user perspective. So it\'s always good to place an upper bound on all resources including heap memory usage of your program. All in all I find Odin\'s approach very elegant. I usually want the ability to use a different allocator in a given function, but also if I don\'t care, it will do the right thing and use the standard allocator. Opening a window This part is almost exactly the same as the first linked article so I\'ll speed run this. First we open a UNIX domain socket: connect_x11_socket :: proc() -&gt; os.Socket {\n\tSockaddrUn :: struct #packed {\n\t\tsa_family: os.ADDRESS_FAMILY,\n\t\tsa_data:   [108]u8,\n\t}\n\n\tsocket, err := os.socket(os.AF_UNIX, os.SOCK_STREAM, 0)\n\tassert(err == os.ERROR_NONE)\n\n\tpossible_socket_paths := [2]string{&quot;/tmp/.X11-unix/X0&quot;, &quot;/tmp/.X11-unix/X1&quot;}\n\tfor &amp;socket_path in possible_socket_paths {\n\t\taddr := SockaddrUn {\n\t\t\tsa_family = cast(u16)os.AF_UNIX,\n\t\t}\n\t\tmem.copy_non_overlapping(&amp;addr.sa_data, raw_data(socket_path), len(socket_path))\n\n\t\terr = os.connect(socket, cast(^os.SOCKADDR)&amp;addr, size_of(addr))\n\t\tif (err == os.ERROR_NONE) {return socket}\n\t}\n\n\tos.exit(1)\n} We try a few possible paths for the socket, that can vary a bit from distribution to distribution. We now can send the handshake, and receive general information from the server. Let\'s define some structs for that per the X11 protocol: Screen :: struct #packed {\n\tid:             u32,\n\tcolormap:       u32,\n\twhite:          u32,\n\tblack:          u32,\n\tinput_mask:     u32,\n\twidth:          u16,\n\theight:         u16,\n\twidth_mm:       u16,\n\theight_mm:      u16,\n\tmaps_min:       u16,\n\tmaps_max:       u16,\n\troot_visual_id: u32,\n\tbacking_store:  u8,\n\tsave_unders:    u8,\n\troot_depth:     u8,\n\tdepths_count:   u8,\n}\n\nConnectionInformation :: struct {\n\troot_screen:      Screen,\n\tresource_id_base: u32,\n\tresource_id_mask: u32,\n} The structs are #packed to match the network protocol format, otherwise the compiler may insert padding between fields. One thing to know about X11: Everything we send has to be padded to a multiple of 4 bytes. We define a helper to do that by using the formula ((i32)x + 3) &amp; -4 along with a unit test for good measure: round_up_4 :: #force_inline proc(x: u32) -&gt; u32 {\n\tmask: i32 = -4\n\treturn transmute(u32)((transmute(i32)x + 3) &amp; mask)\n}\n\n@(test)\ntest_round_up_4 :: proc(_: ^testing.T) {\n\tassert(round_up_4(0) == 0)\n\tassert(round_up_4(1) == 4)\n\tassert(round_up_4(2) == 4)\n\tassert(round_up_4(3) == 4)\n\tassert(round_up_4(4) == 4)\n\tassert(round_up_4(5) == 8)\n\tassert(round_up_4(6) == 8)\n\tassert(round_up_4(7) == 8)\n\tassert(round_up_4(8) == 8)\n} We can now send the handshake with the authentication token inside. We leverage the writev system call to send multiple separate buffers of different lengths in one call. We skip over most of the information the server sends us, since we only are after a few fields: x11_handshake :: proc(socket: os.Socket, auth_token: ^AuthToken) -&gt; ConnectionInformation {\n\tRequest :: struct #packed {\n\t\tendianness:             u8,\n\t\tpad1:                   u8,\n\t\tmajor_version:          u16,\n\t\tminor_version:          u16,\n\t\tauthorization_len:      u16,\n\t\tauthorization_data_len: u16,\n\t\tpad2:                   u16,\n\t}\n\n\trequest := Request {\n\t\tendianness             = \'l\',\n\t\tmajor_version          = 11,\n\t\tauthorization_len      = len(AUTH_ENTRY_MAGIC_COOKIE),\n\t\tauthorization_data_len = size_of(AuthToken),\n\t}\n\n\n\t{\n\t\tpadding := [2]u8{0, 0}\n\t\tn_sent, err := linux.writev(\n\t\t\tcast(linux.Fd)socket,\n\t\t\t[]linux.IO_Vec {\n\t\t\t\t{base = &amp;request, len = size_of(Request)},\n\t\t\t\t{base = raw_data(AUTH_ENTRY_MAGIC_COOKIE), len = len(AUTH_ENTRY_MAGIC_COOKIE)},\n\t\t\t\t{base = raw_data(padding[:]), len = len(padding)},\n\t\t\t\t{base = raw_data(auth_token[:]), len = len(auth_token)},\n\t\t\t},\n\t\t)\n\t\tassert(err == .NONE)\n\t\tassert(\n\t\t\tn_sent ==\n\t\t\tsize_of(Request) + len(AUTH_ENTRY_MAGIC_COOKIE) + len(padding) + len(auth_token),\n\t\t)\n\t}\n\n\tStaticResponse :: struct #packed {\n\t\tsuccess:       u8,\n\t\tpad1:          u8,\n\t\tmajor_version: u16,\n\t\tminor_version: u16,\n\t\tlength:        u16,\n\t}\n\n\tstatic_response := StaticResponse{}\n\t{\n\t\tn_recv, err := os.recv(socket, mem.ptr_to_bytes(&amp;static_response), 0)\n\t\tassert(err == os.ERROR_NONE)\n\t\tassert(n_recv == size_of(StaticResponse))\n\t\tassert(static_response.success == 1)\n\t}\n\n\n\trecv_buf: [1 &lt;&lt; 15]u8 = {}\n\t{\n\t\tassert(len(recv_buf) &gt;= cast(u32)static_response.length * 4)\n\n\t\tn_recv, err := os.recv(socket, recv_buf[:], 0)\n\t\tassert(err == os.ERROR_NONE)\n\t\tassert(n_recv == cast(u32)static_response.length * 4)\n\t}\n\n\n\tDynamicResponse :: struct #packed {\n\t\trelease_number:              u32,\n\t\tresource_id_base:            u32,\n\t\tresource_id_mask:            u32,\n\t\tmotion_buffer_size:          u32,\n\t\tvendor_length:               u16,\n\t\tmaximum_request_length:      u16,\n\t\tscreens_in_root_count:       u8,\n\t\tformats_count:               u8,\n\t\timage_byte_order:            u8,\n\t\tbitmap_format_bit_order:     u8,\n\t\tbitmap_format_scanline_unit: u8,\n\t\tbitmap_format_scanline_pad:  u8,\n\t\tmin_keycode:                 u8,\n\t\tmax_keycode:                 u8,\n\t\tpad2:                        u32,\n\t}\n\n\tread_buffer := bytes.Buffer{}\n\tbytes.buffer_init(&amp;read_buffer, recv_buf[:])\n\n\tdynamic_response := DynamicResponse{}\n\t{\n\t\tn_read, err := bytes.buffer_read(&amp;read_buffer, mem.ptr_to_bytes(&amp;dynamic_response))\n\t\tassert(err == .None)\n\t\tassert(n_read == size_of(DynamicResponse))\n\t}\n\n\n\t// Skip over the vendor information.\n\tbytes.buffer_next(&amp;read_buffer, cast(int)round_up_4(cast(u32)dynamic_response.vendor_length))\n\t// Skip over the format information (each 8 bytes long).\n\tbytes.buffer_next(&amp;read_buffer, 8 * cast(int)dynamic_response.formats_count)\n\n\tscreen := Screen{}\n\t{\n\t\tn_read, err := bytes.buffer_read(&amp;read_buffer, mem.ptr_to_bytes(&amp;screen))\n\t\tassert(err == .None)\n\t\tassert(n_read == size_of(screen))\n\t}\n\n\treturn(\n\t\tConnectionInformation {\n\t\t\tresource_id_base = dynamic_response.resource_id_base,\n\t\t\tresource_id_mask = dynamic_response.resource_id_mask,\n\t\t\troot_screen = screen,\n\t\t} \\\n\t)\n} Our main now becomes: main :: proc() {\n\tauth_token, _ := load_x11_auth_token(context.temp_allocator)\n\tsocket := connect_x11_socket()\n\tconnection_information := x11_handshake(socket, &amp;auth_token)\n} The next step is to create a graphical context. When creating a new entity, we generate an id for it, and send that in the create request. Afterwards, we can refer to the entity by this id: next_x11_id :: proc(current_id: u32, info: ConnectionInformation) -&gt; u32 {\n\treturn 1 + ((info.resource_id_mask &amp; (current_id)) | info.resource_id_base)\n} Time to create a graphical context: x11_create_graphical_context :: proc(socket: os.Socket, gc_id: u32, root_id: u32) {\n\topcode: u8 : 55\n\tFLAG_GC_BG: u32 : 8\n\tBITMASK: u32 : FLAG_GC_BG\n\tVALUE1: u32 : 0x00_00_ff_00\n\n\tRequest :: struct #packed {\n\t\topcode:   u8,\n\t\tpad1:     u8,\n\t\tlength:   u16,\n\t\tid:       u32,\n\t\tdrawable: u32,\n\t\tbitmask:  u32,\n\t\tvalue1:   u32,\n\t}\n\trequest := Request {\n\t\topcode   = opcode,\n\t\tlength   = 5,\n\t\tid       = gc_id,\n\t\tdrawable = root_id,\n\t\tbitmask  = BITMASK,\n\t\tvalue1   = VALUE1,\n\t}\n\n\t{\n\t\tn_sent, err := os.send(socket, mem.ptr_to_bytes(&amp;request), 0)\n\t\tassert(err == os.ERROR_NONE)\n\t\tassert(n_sent == size_of(Request))\n\t}\n} Finally we create a window. We subscribe to a few events as well: Exposure : when our window becomes visible KEY_PRESS : when a keyboard key is pressed KEY_RELEASE : when a keyboard key is released BUTTON_PRESS : when a mouse button is pressed BUTTON_RELEASE : when a mouse button is released We also pick an arbitrary background color, yellow. It does not matter because we will always cover every part of the window with our assets. x11_create_window :: proc(\n\tsocket: os.Socket,\n\twindow_id: u32,\n\tparent_id: u32,\n\tx: u16,\n\ty: u16,\n\twidth: u16,\n\theight: u16,\n\troot_visual_id: u32,\n) {\n\tFLAG_WIN_BG_PIXEL: u32 : 2\n\tFLAG_WIN_EVENT: u32 : 0x800\n\tFLAG_COUNT: u16 : 2\n\tEVENT_FLAG_EXPOSURE: u32 = 0x80_00\n\tEVENT_FLAG_KEY_PRESS: u32 = 0x1\n\tEVENT_FLAG_KEY_RELEASE: u32 = 0x2\n\tEVENT_FLAG_BUTTON_PRESS: u32 = 0x4\n\tEVENT_FLAG_BUTTON_RELEASE: u32 = 0x8\n\tflags: u32 : FLAG_WIN_BG_PIXEL | FLAG_WIN_EVENT\n\tdepth: u8 : 24\n\tborder_width: u16 : 0\n\tCLASS_INPUT_OUTPUT: u16 : 1\n\topcode: u8 : 1\n\tBACKGROUND_PIXEL_COLOR: u32 : 0x00_ff_ff_00\n\n\tRequest :: struct #packed {\n\t\topcode:         u8,\n\t\tdepth:          u8,\n\t\trequest_length: u16,\n\t\twindow_id:      u32,\n\t\tparent_id:      u32,\n\t\tx:              u16,\n\t\ty:              u16,\n\t\twidth:          u16,\n\t\theight:         u16,\n\t\tborder_width:   u16,\n\t\tclass:          u16,\n\t\troot_visual_id: u32,\n\t\tbitmask:        u32,\n\t\tvalue1:         u32,\n\t\tvalue2:         u32,\n\t}\n\trequest := Request {\n\t\topcode         = opcode,\n\t\tdepth          = depth,\n\t\trequest_length = 8 + FLAG_COUNT,\n\t\twindow_id      = window_id,\n\t\tparent_id      = parent_id,\n\t\tx              = x,\n\t\ty              = y,\n\t\twidth          = width,\n\t\theight         = height,\n\t\tborder_width   = border_width,\n\t\tclass          = CLASS_INPUT_OUTPUT,\n\t\troot_visual_id = root_visual_id,\n\t\tbitmask        = flags,\n\t\tvalue1         = BACKGROUND_PIXEL_COLOR,\n\t\tvalue2         = EVENT_FLAG_EXPOSURE | EVENT_FLAG_BUTTON_RELEASE | EVENT_FLAG_BUTTON_PRESS | EVENT_FLAG_KEY_PRESS | EVENT_FLAG_KEY_RELEASE,\n\t}\n\n\t{\n\t\tn_sent, err := os.send(socket, mem.ptr_to_bytes(&amp;request), 0)\n\t\tassert(err == os.ERROR_NONE)\n\t\tassert(n_sent == size_of(Request))\n\t}\n} We decide that our game will have 16 rows and 16 columns, and each asset is 16x16 pixels. main is now: ENTITIES_ROW_COUNT :: 16\nENTITIES_COLUMN_COUNT :: 16\nENTITIES_WIDTH :: 16\nENTITIES_HEIGHT :: 16\n\nmain :: proc() {\n\tauth_token, _ := load_x11_auth_token(context.temp_allocator)\n\tsocket := connect_x11_socket()\n\tconnection_information := x11_handshake(socket, &amp;auth_token)\n\n\tgc_id := next_x11_id(0, connection_information)\n\tx11_create_graphical_context(socket, gc_id, connection_information.root_screen.id)\n\n\twindow_id := next_x11_id(gc_id, connection_information)\n\tx11_create_window(\n\t\tsocket,\n\t\twindow_id,\n\t\tconnection_information.root_screen.id,\n\t\t200,\n\t\t200,\n\t\tENTITIES_COLUMN_COUNT * ENTITIES_WIDTH,\n\t\tENTITIES_ROW_COUNT * ENTITIES_HEIGHT,\n\t\tconnection_information.root_screen.root_visual_id,\n\t)\n} Note that the window dimensions are a hint, they might now be respected, for example in a tiling window manager. We do not handle this case here since the assets are fixed size. If you have followed along, you will now see... nothing. That\'s because we need to tell X11 to show our window with the map_window call: x11_map_window :: proc(socket: os.Socket, window_id: u32) {\n\topcode: u8 : 8\n\n\tRequest :: struct #packed {\n\t\topcode:         u8,\n\t\tpad1:           u8,\n\t\trequest_length: u16,\n\t\twindow_id:      u32,\n\t}\n\trequest := Request {\n\t\topcode         = opcode,\n\t\trequest_length = 2,\n\t\twindow_id      = window_id,\n\t}\n\t{\n\t\tn_sent, err := os.send(socket, mem.ptr_to_bytes(&amp;request), 0)\n\t\tassert(err == os.ERROR_NONE)\n\t\tassert(n_sent == size_of(Request))\n\t}\n\n} We now see: Time to start programming the game itself! Loading assets What\'s a game without nice looking pictures stolen from somewhere on the internet ? Here is our sprite, the one image containing all our assets: Odin has a nice feature to embed the image file in our executable which makes redistribution a breeze and startup a bit faster, so we\'ll do that: png_data := #load(&quot;sprite.png&quot;)\n\tsprite, err := png.load_from_bytes(png_data, {})\n\tassert(err == nil) Now here is the catch: The X11 image format is different from the one in the sprite so we have to swap the bytes around: sprite_data := make([]u8, sprite.height * sprite.width * 4)\n\n\t// Convert the image format from the sprite (RGB) into the X11 image format (BGRX).\n\tfor i := 0; i &lt; sprite.height * sprite.width - 3; i += 1 {\n\t\tsprite_data[i * 4 + 0] = sprite.pixels.buf[i * 3 + 2] // R -&gt; B\n\t\tsprite_data[i * 4 + 1] = sprite.pixels.buf[i * 3 + 1] // G -&gt; G\n\t\tsprite_data[i * 4 + 2] = sprite.pixels.buf[i * 3 + 0] // B -&gt; R\n\t\tsprite_data[i * 4 + 3] = 0 // pad\n\t} The A component is actually unused since we do not have transparency. Now that our image is in (client) memory, how to make it available to the server? Which, again, in the X11 model, might be running on a totally different machine across the world! X11 has 3 useful calls for images: CreatePixmap and PutImage . A Pixmap is an off-screen image buffer. PutImage uploads image data either to a pixmap or to the window directly (a \'drawable\' in X11 parlance). CopyArea copies one rectangle in one drawable to another drawable. In my humble opinion, these are complete misnomers. CreatePixmap should have been called CreateOffscreenImageBuffer and PutImage should have been UploadImageData . CopyArea : you\'re fine buddy, carry on. We cannot simply use PutImage here since that would show the whole sprite on the screen (there are no fields to specify that only part of the image should be displayed). We could show only parts of it, with separate PutImage calls for each entity, but that would mean uploading the image data to the server each time. What we want is to upload the image data once, off-screen, with one PutImage call, and then copy parts of it onto the window. Here is the dance we need to do: CreatePixmap PutImage to upload the image data to the pixmap - at that point nothing is shown on the window, everything is still off-screen For each entity in our game, issue a cheap CopyArea call which copies parts of the pixmap onto the window - now it\'s visible! The X server can actually upload the image data to the GPU on a PutImage call (this is implementation dependent). After that, CopyArea calls can be translated by the X server to GPU commands to copy the image data from one GPU buffer to another: that\'s really performant! The image data is only uploaded once to the GPU and then resides there for the remainder of the program. Unfortunately, the X standard does not enforce that (it says: &quot;may or may not [...]&quot;), but that\'s a useful model to have in mind. Another useful model is to think of what happens when the X server is running across the network: We only want to send the image data once because that\'s time-consuming, and afterwards issue cheap CopyArea commands that are only a few bytes each. Ok, let\'s implement that then: x11_create_pixmap :: proc(\n\tsocket: os.Socket,\n\twindow_id: u32,\n\tpixmap_id: u32,\n\twidth: u16,\n\theight: u16,\n\tdepth: u8,\n) {\n\topcode: u8 : 53\n\n\tRequest :: struct #packed {\n\t\topcode:         u8,\n\t\tdepth:          u8,\n\t\trequest_length: u16,\n\t\tpixmap_id:      u32,\n\t\tdrawable_id:    u32,\n\t\twidth:          u16,\n\t\theight:         u16,\n\t}\n\n\trequest := Request {\n\t\topcode         = opcode,\n\t\tdepth          = depth,\n\t\trequest_length = 4,\n\t\tpixmap_id      = pixmap_id,\n\t\tdrawable_id    = window_id,\n\t\twidth          = width,\n\t\theight         = height,\n\t}\n\n\t{\n\t\tn_sent, err := os.send(socket, mem.ptr_to_bytes(&amp;request), 0)\n\t\tassert(err == os.ERROR_NONE)\n\t\tassert(n_sent == size_of(Request))\n\t}\n}\n\nx11_put_image :: proc(\n\tsocket: os.Socket,\n\tdrawable_id: u32,\n\tgc_id: u32,\n\twidth: u16,\n\theight: u16,\n\tdst_x: u16,\n\tdst_y: u16,\n\tdepth: u8,\n\tdata: []u8,\n) {\n\topcode: u8 : 72\n\n\tRequest :: struct #packed {\n\t\topcode:         u8,\n\t\tformat:         u8,\n\t\trequest_length: u16,\n\t\tdrawable_id:    u32,\n\t\tgc_id:          u32,\n\t\twidth:          u16,\n\t\theight:         u16,\n\t\tdst_x:          u16,\n\t\tdst_y:          u16,\n\t\tleft_pad:       u8,\n\t\tdepth:          u8,\n\t\tpad1:           u16,\n\t}\n\n\tdata_length_padded := round_up_4(cast(u32)len(data))\n\n\trequest := Request {\n\t\topcode         = opcode,\n\t\tformat         = 2, // ZPixmap\n\t\trequest_length = cast(u16)(6 + data_length_padded / 4),\n\t\tdrawable_id    = drawable_id,\n\t\tgc_id          = gc_id,\n\t\twidth          = width,\n\t\theight         = height,\n\t\tdst_x          = dst_x,\n\t\tdst_y          = dst_y,\n\t\tdepth          = depth,\n\t}\n\t{\n\t\tpadding_len := data_length_padded - cast(u32)len(data)\n\n\t\tn_sent, err := linux.writev(\n\t\t\tcast(linux.Fd)socket,\n\t\t\t[]linux.IO_Vec {\n\t\t\t\t{base = &amp;request, len = size_of(Request)},\n\t\t\t\t{base = raw_data(data), len = len(data)},\n\t\t\t\t{base = raw_data(data), len = cast(uint)padding_len},\n\t\t\t},\n\t\t)\n\t\tassert(err == .NONE)\n\t\tassert(n_sent == size_of(Request) + len(data) + cast(int)padding_len)\n\t}\n}\n\nx11_copy_area :: proc(\n\tsocket: os.Socket,\n\tsrc_id: u32,\n\tdst_id: u32,\n\tgc_id: u32,\n\tsrc_x: u16,\n\tsrc_y: u16,\n\tdst_x: u16,\n\tdst_y: u16,\n\twidth: u16,\n\theight: u16,\n) {\n\topcode: u8 : 62\n\tRequest :: struct #packed {\n\t\topcode:         u8,\n\t\tpad1:           u8,\n\t\trequest_length: u16,\n\t\tsrc_id:         u32,\n\t\tdst_id:         u32,\n\t\tgc_id:          u32,\n\t\tsrc_x:          u16,\n\t\tsrc_y:          u16,\n\t\tdst_x:          u16,\n\t\tdst_y:          u16,\n\t\twidth:          u16,\n\t\theight:         u16,\n\t}\n\n\trequest := Request {\n\t\topcode         = opcode,\n\t\trequest_length = 7,\n\t\tsrc_id         = src_id,\n\t\tdst_id         = dst_id,\n\t\tgc_id          = gc_id,\n\t\tsrc_x          = src_x,\n\t\tsrc_y          = src_y,\n\t\tdst_x          = dst_x,\n\t\tdst_y          = dst_y,\n\t\twidth          = width,\n\t\theight         = height,\n\t}\n\t{\n\t\tn_sent, err := os.send(socket, mem.ptr_to_bytes(&amp;request), 0)\n\t\tassert(err == os.ERROR_NONE)\n\t\tassert(n_sent == size_of(Request))\n\t}\n} Let\'s try in main : img_depth: u8 = 24\n\tpixmap_id := next_x11_id(window_id, connection_information)\n\tx11_create_pixmap(\n\t\tsocket,\n\t\twindow_id,\n\t\tpixmap_id,\n\t\tcast(u16)sprite.width,\n\t\tcast(u16)sprite.height,\n\t\timg_depth,\n\t)\n\n\tx11_put_image(\n\t\tsocket,\n\t\tpixmap_id,\n\t\tgc_id,\n\t\tsprite_width,\n\t\tsprite_height,\n\t\t0,\n\t\t0,\n\t\timg_depth,\n\t\tsprite_data,\n\t)\n\n    // Let\'s render two different assets: an exploded mine and an idle mine.\n\tx11_copy_area(\n\t\tsocket,\n\t\tpixmap_id,\n\t\twindow_id,\n\t\tgc_id,\n\t\t32, // X coordinate on the sprite sheet.\n\t\t40, // Y coordinate on the sprite sheet.\n\t\t0, // X coordinate on the window.\n\t\t0, // Y coordinate on the window.\n\t\t16, // Width.\n\t\t16, // Height.\n\t)\n\tx11_copy_area(\n\t\tsocket,\n\t\tpixmap_id,\n\t\twindow_id,\n\t\tgc_id,\n\t\t64,\n\t\t40,\n\t\t16,\n\t\t0,\n\t\t16,\n\t\t16,\n\t) Result: We are now ready to focus on the game entities. The game entities We have a few different entities we want to show, each is a 16x16 section of the sprite sheet. Let\'s define their coordinates to be readable: Position :: struct {\n\tx: u16,\n\ty: u16,\n}\n\nEntity_kind :: enum {\n\tCovered,\n\tUncovered_0,\n\tUncovered_1,\n\tUncovered_2,\n\tUncovered_3,\n\tUncovered_4,\n\tUncovered_5,\n\tUncovered_6,\n\tUncovered_7,\n\tUncovered_8,\n\tMine_exploded,\n\tMine_idle,\n}\n\nASSET_COORDINATES: [Entity_kind]Position = {\n\t.Uncovered_0 = {x = 0 * 16, y = 22},\n\t.Uncovered_1 = {x = 1 * 16, y = 22},\n\t.Uncovered_2 = {x = 2 * 16, y = 22},\n\t.Uncovered_3 = {x = 3 * 16, y = 22},\n\t.Uncovered_4 = {x = 4 * 16, y = 22},\n\t.Uncovered_5 = {x = 5 * 16, y = 22},\n\t.Uncovered_6 = {x = 6 * 16, y = 22},\n\t.Uncovered_7 = {x = 7 * 16, y = 22},\n\t.Uncovered_8 = {x = 8 * 16, y = 22},\n\t.Covered = {x = 0, y = 38},\n\t.Mine_exploded = {x = 32, y = 40},\n\t.Mine_idle = {x = 64, y = 40},\n} And we\'ll group everything we need in one struct called Scene : Scene :: struct {\n\twindow_id:              u32,\n\tgc_id:                  u32,\n\tsprite_pixmap_id:       u32,\n\tdisplayed_entities:     [ENTITIES_ROW_COUNT * ENTITIES_COLUMN_COUNT]Entity_kind,\n\tmines:                  [ENTITIES_ROW_COUNT * ENTITIES_COLUMN_COUNT]bool,\n} The first interesting field is displayed_entities which keeps track of which assets are shown. For example, a mine is either covered, uncovered and exploded if the player clicked on it, or uncovered and idle if the player won). The second one is mines which simply keeps track of where mines are. It could be a bitfield to optimize space but I did not bother. In main we create a new scene and plant mines randomly. scene := Scene {\n\t\twindow_id              = window_id,\n\t\tgc_id                  = gc_id,\n\t\tsprite_pixmap_id       = pixmap_id,\n\t}\n\treset(&amp;scene) We put this logic in the reset helper so that the player can easily restart the game with one keystroke: reset :: proc(scene: ^Scene) {\n\tfor &amp;entity in scene.displayed_entities {\n\t\tentity = .Covered\n\t}\n\n\tfor &amp;mine in scene.mines {\n\t\tmine = rand.choice([]bool{true, false, false, false})\n\t}\n} Here I used a 1/4 chance that a cell has a mine. We are now ready to render our (static for now) scene: render :: proc(socket: os.Socket, scene: ^Scene) {\n\tfor entity, i in scene.displayed_entities {\n\t\trect := ASSET_COORDINATES[entity]\n\t\trow, column := idx_to_row_column(i)\n\n\t\tx11_copy_area(\n\t\t\tsocket,\n\t\t\tscene.sprite_pixmap_id,\n\t\t\tscene.window_id,\n\t\t\tscene.gc_id,\n\t\t\trect.x,\n\t\t\trect.y,\n\t\t\tcast(u16)column * ENTITIES_WIDTH,\n\t\t\tcast(u16)row * ENTITIES_HEIGHT,\n\t\t\tENTITIES_WIDTH,\n\t\t\tENTITIES_HEIGHT,\n\t\t)\n\t}\n} And here is what we get: The next step is to respond to events. Reacting to keyboard and mouse events This is very straightforward. Since the only messages we expect are for keyboard and mouse events, with a fixed size of 32 bytes, we simply read 32 bytes exactly in a blocking fashion. The first byte indicates which kind of event it is: wait_for_x11_events :: proc(socket: os.Socket, scene: ^Scene) {\n\tGenericEvent :: struct #packed {\n\t\tcode: u8,\n\t\tpad:  [31]u8,\n\t}\n\tassert(size_of(GenericEvent) == 32)\n\n\tKeyReleaseEvent :: struct #packed {\n\t\tcode:            u8,\n\t\tdetail:          u8,\n\t\tsequence_number: u16,\n\t\ttime:            u32,\n\t\troot_id:         u32,\n\t\tevent:           u32,\n\t\tchild_id:        u32,\n\t\troot_x:          u16,\n\t\troot_y:          u16,\n\t\tevent_x:         u16,\n\t\tevent_y:         u16,\n\t\tstate:           u16,\n\t\tsame_screen:     bool,\n\t\tpad1:            u8,\n\t}\n\tassert(size_of(KeyReleaseEvent) == 32)\n\n\tButtonReleaseEvent :: struct #packed {\n\t\tcode:        u8,\n\t\tdetail:      u8,\n\t\tseq_number:  u16,\n\t\ttimestamp:   u32,\n\t\troot:        u32,\n\t\tevent:       u32,\n\t\tchild:       u32,\n\t\troot_x:      u16,\n\t\troot_y:      u16,\n\t\tevent_x:     u16,\n\t\tevent_y:     u16,\n\t\tstate:       u16,\n\t\tsame_screen: bool,\n\t\tpad1:        u8,\n\t}\n\tassert(size_of(ButtonReleaseEvent) == 32)\n\n\tEVENT_EXPOSURE: u8 : 0xc\n\tEVENT_KEY_RELEASE: u8 : 0x3\n\tEVENT_BUTTON_RELEASE: u8 : 0x5\n\n\tKEYCODE_ENTER: u8 : 36\n\n\tfor {\n\t\tgeneric_event := GenericEvent{}\n\t\tn_recv, err := os.recv(socket, mem.ptr_to_bytes(&amp;generic_event), 0)\n\t\tif err == os.EPIPE || n_recv == 0 {\n\t\t\tos.exit(0) // The end.\n\t\t}\n\n\t\tassert(err == os.ERROR_NONE)\n\t\tassert(n_recv == size_of(GenericEvent))\n\n\t\tswitch generic_event.code {\n\t\tcase EVENT_EXPOSURE:\n\t\t\trender(socket, scene)\n\n\t\tcase EVENT_KEY_RELEASE:\n\t\t\tevent := transmute(KeyReleaseEvent)generic_event\n\t\t\tif event.detail == KEYCODE_ENTER {\n\t\t\t\treset(scene)\n\t\t\t\trender(socket, scene)\n\t\t\t}\n\n\t\tcase EVENT_BUTTON_RELEASE:\n\t\t\tevent := transmute(ButtonReleaseEvent)generic_event\n\t\t\ton_cell_clicked(event.event_x, event.event_y, scene)\n\t\t\trender(socket, scene)\n\t\t}\n\t}\n} If the event is Exposed , we simply render (that\'s our first render when the window becomes visible - or if the window was minimized and then made visible again). If the event is the Enter key, we reset the state of the game and render. X11 differentiates between physical and logical keys on the keyboard but that does not matter here (or I would argue in most games: we are interested in the physical location of the key, not what the user mapped it to). If the event is (pressing and) releasing a mouse button, we run the game logic to uncover a cell and render. That\'s it! Game logic: uncover a cell The last thing to do is implementing the game rules. From my faint memory, when uncovering a cell, we have two cases: If it\'s a mine, we lost If it\'s not a mine, we uncover this cell and neighboring cells, in a flood fill fashion. We only uncover non-mines of course. An uncovered cell shows how many neighboring mines are around with a number (0 is simply empty, no number is shown). The one thing that tripped me is that we inspect all 8 neighboring cells to count mines, but when doing the flood fill, we only visit the 4 neighboring cells: up, right, down, left - not the diagonal neighbors. Otherwise the flood fill ends up uncovering all cells in the game at once. First, we need to translate the mouse position in the window to a cell index/row/column in our grid: row_column_to_idx :: #force_inline proc(row: int, column: int) -&gt; int {\n\treturn cast(int)row * ENTITIES_COLUMN_COUNT + cast(int)column\n}\n\nlocate_entity_by_coordinate :: proc(win_x: u16, win_y: u16) -&gt; (idx: int, row: int, column: int) {\n\tcolumn = cast(int)win_x / ENTITIES_WIDTH\n\trow = cast(int)win_y / ENTITIES_HEIGHT\n\n\tidx = row_column_to_idx(row, column)\n\n\treturn idx, row, column\n} Then the game logic: on_cell_clicked :: proc(x: u16, y: u16, scene: ^Scene) {\n\tidx, row, column := locate_entity_by_coordinate(x, y)\n\n\tmined := scene.mines[idx]\n\n\tif mined {\n\t\tscene.displayed_entities[idx] = .Mine_exploded\n\t\t// Lose.\n\t\tuncover_all_cells(&amp;scene.displayed_entities, &amp;scene.mines, .Mine_exploded)\n\t} else {\n\t\tvisited := [ENTITIES_COLUMN_COUNT * ENTITIES_ROW_COUNT]bool{}\n\t\tuncover_cells_flood_fill(row, column, &amp;scene.displayed_entities, &amp;scene.mines, &amp;visited)\n\n\t\t// Win.\n\t\tif count_remaining_goals(scene.displayed_entities, scene.mines) == 0 {\n\t\t\tuncover_all_cells(&amp;scene.displayed_entities, &amp;scene.mines, .Mine_idle)\n\t\t}\n\t}\n} The objective is to uncover all cells without mines. We could keep a counter around and decrement it each time, but I wanted to make it idiot-proof, so I simply scan the grid to count how many uncovered cells without a mine underneath remain (in count_remaining_goals ). No risk that way to have a desync between the game state and what is shown on the screen, because we did not decrement the counter in one edge case. uncover_all_cells unconditionally reveals the whole grid when the player won or lost. We just need to show the mines exploded when they lost, and idle when they won. uncover_cells_flood_fill is the interesting one. We use recursion, and to avoid visiting the same cells multiple times and potentially getting into infinite recursion, we track which cells were visited: uncover_cells_flood_fill :: proc(\n\trow: int,\n\tcolumn: int,\n\tdisplayed_entities: ^[ENTITIES_COLUMN_COUNT * ENTITIES_ROW_COUNT]Entity_kind,\n\tmines: ^[ENTITIES_ROW_COUNT * ENTITIES_COLUMN_COUNT]bool,\n\tvisited: ^[ENTITIES_COLUMN_COUNT * ENTITIES_ROW_COUNT]bool,\n) {\n\ti := row_column_to_idx(row, column)\n\tif visited[i] {return}\n\n\tvisited[i] = true\n\n\t// Do not uncover covered mines.\n\tif mines[i] {return}\n\n\tif displayed_entities[i] != .Covered {return}\n\n\t// Uncover cell.\n\n\tmines_around_count := count_mines_around_cell(row, column, mines[:])\n\tassert(mines_around_count &lt;= 8)\n\n\tdisplayed_entities[i] =\n\tcast(Entity_kind)(cast(int)Entity_kind.Uncovered_0 + mines_around_count)\n\n\t// Uncover neighbors.\n\n\t// Up.\n\tif !(row == 0) {\n\t\tuncover_cells_flood_fill(row - 1, column, displayed_entities, mines, visited)\n\t}\n\n\t// Right\n\tif !(column == (ENTITIES_COLUMN_COUNT - 1)) {\n\t\tuncover_cells_flood_fill(row, column + 1, displayed_entities, mines, visited)\n\t}\n\n\t// Bottom.\n\tif !(row == (ENTITIES_ROW_COUNT - 1)) {\n\t\tuncover_cells_flood_fill(row + 1, column, displayed_entities, mines, visited)\n\t}\n\n\t// Left.\n\tif !(column == 0) {\n\t\tuncover_cells_flood_fill(row, column - 1, displayed_entities, mines, visited)\n\t}\n} There are a few helpers here and there that are simple, but otherwise... that\'s it, that\'s the end. We\'re done! All under 1000 lines of code without any tricks or clever things. Conclusion X11 is old and crufty, but also gets out of the way. Once a few utility functions to open the window, receive events, etc have been implemented, it can be forgotten and we can focus all our attention on the game. That\'s very valuable. How many libraries, frameworks and development environments can say the same? I also enjoy that it works with any programming language, any tech stack. Don\'t need no bindings, no FFI, just send some bytes over the socket. You can even do that in Bash (don\'t tempt me!). I did not implement a few accessory things from the original game, like planting a flag on a cell you suspect has a mine. Feel free to do this at home, it\'s not much work. Finally, give Odin a try, it\'s great! It\'s this weird mix of a sane C with a Go-ish syntax and a good standard library. I hope that you had as much fun as I did! Addendum: the full code The full code package main\n\nimport &quot;core:bytes&quot;\nimport &quot;core:image/png&quot;\nimport &quot;core:math/bits&quot;\nimport &quot;core:math/rand&quot;\nimport &quot;core:mem&quot;\nimport &quot;core:os&quot;\nimport &quot;core:path/filepath&quot;\nimport &quot;core:slice&quot;\nimport &quot;core:sys/linux&quot;\nimport &quot;core:testing&quot;\n\nTILE_WIDTH :: 16\nTILE_HEIGHT :: 16\n\nPosition :: struct {\n\tx: u16,\n\ty: u16,\n}\n\nEntity_kind :: enum {\n\tCovered,\n\tUncovered_0,\n\tUncovered_1,\n\tUncovered_2,\n\tUncovered_3,\n\tUncovered_4,\n\tUncovered_5,\n\tUncovered_6,\n\tUncovered_7,\n\tUncovered_8,\n\tMine_exploded,\n\tMine_idle,\n}\n\nASSET_COORDINATES: [Entity_kind]Position = {\n\t.Uncovered_0 = {x = 0 * 16, y = 22},\n\t.Uncovered_1 = {x = 1 * 16, y = 22},\n\t.Uncovered_2 = {x = 2 * 16, y = 22},\n\t.Uncovered_3 = {x = 3 * 16, y = 22},\n\t.Uncovered_4 = {x = 4 * 16, y = 22},\n\t.Uncovered_5 = {x = 5 * 16, y = 22},\n\t.Uncovered_6 = {x = 6 * 16, y = 22},\n\t.Uncovered_7 = {x = 7 * 16, y = 22},\n\t.Uncovered_8 = {x = 8 * 16, y = 22},\n\t.Covered = {x = 0, y = 38},\n\t.Mine_exploded = {x = 32, y = 40},\n\t.Mine_idle = {x = 64, y = 40},\n}\n\nAuthToken :: [16]u8\n\nAuthEntry :: struct {\n\tfamily:    u16,\n\tauth_name: []u8,\n\tauth_data: []u8,\n}\n\nScreen :: struct #packed {\n\tid:             u32,\n\tcolormap:       u32,\n\twhite:          u32,\n\tblack:          u32,\n\tinput_mask:     u32,\n\twidth:          u16,\n\theight:         u16,\n\twidth_mm:       u16,\n\theight_mm:      u16,\n\tmaps_min:       u16,\n\tmaps_max:       u16,\n\troot_visual_id: u32,\n\tbacking_store:  u8,\n\tsave_unders:    u8,\n\troot_depth:     u8,\n\tdepths_count:   u8,\n}\n\nConnectionInformation :: struct {\n\troot_screen:      Screen,\n\tresource_id_base: u32,\n\tresource_id_mask: u32,\n}\n\n\nAUTH_ENTRY_FAMILY_LOCAL: u16 : 1\nAUTH_ENTRY_MAGIC_COOKIE: string : &quot;MIT-MAGIC-COOKIE-1&quot;\n\nround_up_4 :: #force_inline proc(x: u32) -&gt; u32 {\n\tmask: i32 = -4\n\treturn transmute(u32)((transmute(i32)x + 3) &amp; mask)\n}\n\nread_x11_auth_entry :: proc(buffer: ^bytes.Buffer) -&gt; (AuthEntry, bool) {\n\tentry := AuthEntry{}\n\n\t{\n\t\tn_read, err := bytes.buffer_read(buffer, mem.ptr_to_bytes(&amp;entry.family))\n\t\tif err == .EOF {return {}, false}\n\n\t\tassert(err == .None)\n\t\tassert(n_read == size_of(entry.family))\n\t}\n\n\taddress_len: u16 = 0\n\t{\n\t\tn_read, err := bytes.buffer_read(buffer, mem.ptr_to_bytes(&amp;address_len))\n\t\tassert(err == .None)\n\n\t\taddress_len = bits.byte_swap(address_len)\n\t\tassert(n_read == size_of(address_len))\n\t}\n\n\taddress := make([]u8, address_len)\n\t{\n\t\tn_read, err := bytes.buffer_read(buffer, address)\n\t\tassert(err == .None)\n\t\tassert(n_read == cast(int)address_len)\n\t}\n\n\tdisplay_number_len: u16 = 0\n\t{\n\t\tn_read, err := bytes.buffer_read(buffer, mem.ptr_to_bytes(&amp;display_number_len))\n\t\tassert(err == .None)\n\n\t\tdisplay_number_len = bits.byte_swap(display_number_len)\n\t\tassert(n_read == size_of(display_number_len))\n\t}\n\n\tdisplay_number := make([]u8, display_number_len)\n\t{\n\t\tn_read, err := bytes.buffer_read(buffer, display_number)\n\t\tassert(err == .None)\n\t\tassert(n_read == cast(int)display_number_len)\n\t}\n\n\tauth_name_len: u16 = 0\n\t{\n\t\tn_read, err := bytes.buffer_read(buffer, mem.ptr_to_bytes(&amp;auth_name_len))\n\t\tassert(err == .None)\n\n\t\tauth_name_len = bits.byte_swap(auth_name_len)\n\t\tassert(n_read == size_of(auth_name_len))\n\t}\n\n\tentry.auth_name = make([]u8, auth_name_len)\n\t{\n\t\tn_read, err := bytes.buffer_read(buffer, entry.auth_name)\n\t\tassert(err == .None)\n\t\tassert(n_read == cast(int)auth_name_len)\n\t}\n\n\tauth_data_len: u16 = 0\n\t{\n\t\tn_read, err := bytes.buffer_read(buffer, mem.ptr_to_bytes(&amp;auth_data_len))\n\t\tassert(err == .None)\n\n\t\tauth_data_len = bits.byte_swap(auth_data_len)\n\t\tassert(n_read == size_of(auth_data_len))\n\t}\n\n\tentry.auth_data = make([]u8, auth_data_len)\n\t{\n\t\tn_read, err := bytes.buffer_read(buffer, entry.auth_data)\n\t\tassert(err == .None)\n\t\tassert(n_read == cast(int)auth_data_len)\n\t}\n\n\n\treturn entry, true\n}\n\nload_x11_auth_token :: proc(allocator := context.allocator) -&gt; (token: AuthToken, ok: bool) {\n\tcontext.allocator = allocator\n\tdefer free_all(allocator)\n\n\tfilename_env := os.get_env(&quot;XAUTHORITY&quot;)\n\n\tfilename :=\n\t\tlen(filename_env) != 0 \\\n\t\t? filename_env \\\n\t\t: filepath.join([]string{os.get_env(&quot;HOME&quot;), &quot;.Xauthority&quot;})\n\n\tdata := os.read_entire_file_from_filename(filename) or_return\n\n\tbuffer := bytes.Buffer{}\n\tbytes.buffer_init(&amp;buffer, data[:])\n\n\n\tfor {\n\t\tauth_entry := read_x11_auth_entry(&amp;buffer) or_break\n\n\t\tif auth_entry.family == AUTH_ENTRY_FAMILY_LOCAL &amp;&amp;\n\t\t   slice.equal(auth_entry.auth_name, transmute([]u8)AUTH_ENTRY_MAGIC_COOKIE) &amp;&amp;\n\t\t   len(auth_entry.auth_data) == size_of(AuthToken) {\n\n\t\t\tmem.copy_non_overlapping(\n\t\t\t\traw_data(&amp;token),\n\t\t\t\traw_data(auth_entry.auth_data),\n\t\t\t\tsize_of(AuthToken),\n\t\t\t)\n\t\t\treturn token, true\n\t\t}\n\t}\n\n\t// Did not find a fitting token.\n\treturn {}, false\n}\n\nconnect_x11_socket :: proc() -&gt; os.Socket {\n\tSockaddrUn :: struct #packed {\n\t\tsa_family: os.ADDRESS_FAMILY,\n\t\tsa_data:   [108]u8,\n\t}\n\n\tsocket, err := os.socket(os.AF_UNIX, os.SOCK_STREAM, 0)\n\tassert(err == os.ERROR_NONE)\n\n\tpossible_socket_paths := [2]string{&quot;/tmp/.X11-unix/X0&quot;, &quot;/tmp/.X11-unix/X1&quot;}\n\tfor &amp;socket_path in possible_socket_paths {\n\t\taddr := SockaddrUn {\n\t\t\tsa_family = cast(u16)os.AF_UNIX,\n\t\t}\n\t\tmem.copy_non_overlapping(&amp;addr.sa_data, raw_data(socket_path), len(socket_path))\n\n\t\terr = os.connect(socket, cast(^os.SOCKADDR)&amp;addr, size_of(addr))\n\t\tif (err == os.ERROR_NONE) {return socket}\n\t}\n\n\tos.exit(1)\n}\n\n\nx11_handshake :: proc(socket: os.Socket, auth_token: ^AuthToken) -&gt; ConnectionInformation {\n\n\tRequest :: struct #packed {\n\t\tendianness:             u8,\n\t\tpad1:                   u8,\n\t\tmajor_version:          u16,\n\t\tminor_version:          u16,\n\t\tauthorization_len:      u16,\n\t\tauthorization_data_len: u16,\n\t\tpad2:                   u16,\n\t}\n\n\trequest := Request {\n\t\tendianness             = \'l\',\n\t\tmajor_version          = 11,\n\t\tauthorization_len      = len(AUTH_ENTRY_MAGIC_COOKIE),\n\t\tauthorization_data_len = size_of(AuthToken),\n\t}\n\n\n\t{\n\t\tpadding := [2]u8{0, 0}\n\t\tn_sent, err := linux.writev(\n\t\t\tcast(linux.Fd)socket,\n\t\t\t[]linux.IO_Vec {\n\t\t\t\t{base = &amp;request, len = size_of(Request)},\n\t\t\t\t{base = raw_data(AUTH_ENTRY_MAGIC_COOKIE), len = len(AUTH_ENTRY_MAGIC_COOKIE)},\n\t\t\t\t{base = raw_data(padding[:]), len = len(padding)},\n\t\t\t\t{base = raw_data(auth_token[:]), len = len(auth_token)},\n\t\t\t},\n\t\t)\n\t\tassert(err == .NONE)\n\t\tassert(\n\t\t\tn_sent ==\n\t\t\tsize_of(Request) + len(AUTH_ENTRY_MAGIC_COOKIE) + len(padding) + len(auth_token),\n\t\t)\n\t}\n\n\tStaticResponse :: struct #packed {\n\t\tsuccess:       u8,\n\t\tpad1:          u8,\n\t\tmajor_version: u16,\n\t\tminor_version: u16,\n\t\tlength:        u16,\n\t}\n\n\tstatic_response := StaticResponse{}\n\t{\n\t\tn_recv, err := os.recv(socket, mem.ptr_to_bytes(&amp;static_response), 0)\n\t\tassert(err == os.ERROR_NONE)\n\t\tassert(n_recv == size_of(StaticResponse))\n\t\tassert(static_response.success == 1)\n\t}\n\n\n\trecv_buf: [1 &lt;&lt; 15]u8 = {}\n\t{\n\t\tassert(len(recv_buf) &gt;= cast(u32)static_response.length * 4)\n\n\t\tn_recv, err := os.recv(socket, recv_buf[:], 0)\n\t\tassert(err == os.ERROR_NONE)\n\t\tassert(n_recv == cast(u32)static_response.length * 4)\n\t}\n\n\n\tDynamicResponse :: struct #packed {\n\t\trelease_number:              u32,\n\t\tresource_id_base:            u32,\n\t\tresource_id_mask:            u32,\n\t\tmotion_buffer_size:          u32,\n\t\tvendor_length:               u16,\n\t\tmaximum_request_length:      u16,\n\t\tscreens_in_root_count:       u8,\n\t\tformats_count:               u8,\n\t\timage_byte_order:            u8,\n\t\tbitmap_format_bit_order:     u8,\n\t\tbitmap_format_scanline_unit: u8,\n\t\tbitmap_format_scanline_pad:  u8,\n\t\tmin_keycode:                 u8,\n\t\tmax_keycode:                 u8,\n\t\tpad2:                        u32,\n\t}\n\n\tread_buffer := bytes.Buffer{}\n\tbytes.buffer_init(&amp;read_buffer, recv_buf[:])\n\n\tdynamic_response := DynamicResponse{}\n\t{\n\t\tn_read, err := bytes.buffer_read(&amp;read_buffer, mem.ptr_to_bytes(&amp;dynamic_response))\n\t\tassert(err == .None)\n\t\tassert(n_read == size_of(DynamicResponse))\n\t}\n\n\n\t// Skip over the vendor information.\n\tbytes.buffer_next(&amp;read_buffer, cast(int)round_up_4(cast(u32)dynamic_response.vendor_length))\n\t// Skip over the format information (each 8 bytes long).\n\tbytes.buffer_next(&amp;read_buffer, 8 * cast(int)dynamic_response.formats_count)\n\n\tscreen := Screen{}\n\t{\n\t\tn_read, err := bytes.buffer_read(&amp;read_buffer, mem.ptr_to_bytes(&amp;screen))\n\t\tassert(err == .None)\n\t\tassert(n_read == size_of(screen))\n\t}\n\n\treturn (ConnectionInformation {\n\t\t\t\tresource_id_base = dynamic_response.resource_id_base,\n\t\t\t\tresource_id_mask = dynamic_response.resource_id_mask,\n\t\t\t\troot_screen = screen,\n\t\t\t})\n}\n\nnext_x11_id :: proc(current_id: u32, info: ConnectionInformation) -&gt; u32 {\n\treturn 1 + ((info.resource_id_mask &amp; (current_id)) | info.resource_id_base)\n}\n\nx11_create_graphical_context :: proc(socket: os.Socket, gc_id: u32, root_id: u32) {\n\topcode: u8 : 55\n\tFLAG_GC_BG: u32 : 8\n\tBITMASK: u32 : FLAG_GC_BG\n\tVALUE1: u32 : 0x00_00_ff_00\n\n\tRequest :: struct #packed {\n\t\topcode:   u8,\n\t\tpad1:     u8,\n\t\tlength:   u16,\n\t\tid:       u32,\n\t\tdrawable: u32,\n\t\tbitmask:  u32,\n\t\tvalue1:   u32,\n\t}\n\trequest := Request {\n\t\topcode   = opcode,\n\t\tlength   = 5,\n\t\tid       = gc_id,\n\t\tdrawable = root_id,\n\t\tbitmask  = BITMASK,\n\t\tvalue1   = VALUE1,\n\t}\n\n\t{\n\t\tn_sent, err := os.send(socket, mem.ptr_to_bytes(&amp;request), 0)\n\t\tassert(err == os.ERROR_NONE)\n\t\tassert(n_sent == size_of(Request))\n\t}\n}\n\nx11_create_window :: proc(\n\tsocket: os.Socket,\n\twindow_id: u32,\n\tparent_id: u32,\n\tx: u16,\n\ty: u16,\n\twidth: u16,\n\theight: u16,\n\troot_visual_id: u32,\n) {\n\tFLAG_WIN_BG_PIXEL: u32 : 2\n\tFLAG_WIN_EVENT: u32 : 0x800\n\tFLAG_COUNT: u16 : 2\n\tEVENT_FLAG_EXPOSURE: u32 = 0x80_00\n\tEVENT_FLAG_KEY_PRESS: u32 = 0x1\n\tEVENT_FLAG_KEY_RELEASE: u32 = 0x2\n\tEVENT_FLAG_BUTTON_PRESS: u32 = 0x4\n\tEVENT_FLAG_BUTTON_RELEASE: u32 = 0x8\n\tflags: u32 : FLAG_WIN_BG_PIXEL | FLAG_WIN_EVENT\n\tdepth: u8 : 24\n\tborder_width: u16 : 0\n\tCLASS_INPUT_OUTPUT: u16 : 1\n\topcode: u8 : 1\n\tBACKGROUND_PIXEL_COLOR: u32 : 0x00_ff_ff_00\n\n\tRequest :: struct #packed {\n\t\topcode:         u8,\n\t\tdepth:          u8,\n\t\trequest_length: u16,\n\t\twindow_id:      u32,\n\t\tparent_id:      u32,\n\t\tx:              u16,\n\t\ty:              u16,\n\t\twidth:          u16,\n\t\theight:         u16,\n\t\tborder_width:   u16,\n\t\tclass:          u16,\n\t\troot_visual_id: u32,\n\t\tbitmask:        u32,\n\t\tvalue1:         u32,\n\t\tvalue2:         u32,\n\t}\n\trequest := Request {\n\t\topcode         = opcode,\n\t\tdepth          = depth,\n\t\trequest_length = 8 + FLAG_COUNT,\n\t\twindow_id      = window_id,\n\t\tparent_id      = parent_id,\n\t\tx              = x,\n\t\ty              = y,\n\t\twidth          = width,\n\t\theight         = height,\n\t\tborder_width   = border_width,\n\t\tclass          = CLASS_INPUT_OUTPUT,\n\t\troot_visual_id = root_visual_id,\n\t\tbitmask        = flags,\n\t\tvalue1         = BACKGROUND_PIXEL_COLOR,\n\t\tvalue2         = EVENT_FLAG_EXPOSURE | EVENT_FLAG_BUTTON_RELEASE | EVENT_FLAG_BUTTON_PRESS \n            | EVENT_FLAG_KEY_PRESS | EVENT_FLAG_KEY_RELEASE,\n\t}\n\n\t{\n\t\tn_sent, err := os.send(socket, mem.ptr_to_bytes(&amp;request), 0)\n\t\tassert(err == os.ERROR_NONE)\n\t\tassert(n_sent == size_of(Request))\n\t}\n}\n\nx11_map_window :: proc(socket: os.Socket, window_id: u32) {\n\topcode: u8 : 8\n\n\tRequest :: struct #packed {\n\t\topcode:         u8,\n\t\tpad1:           u8,\n\t\trequest_length: u16,\n\t\twindow_id:      u32,\n\t}\n\trequest := Request {\n\t\topcode         = opcode,\n\t\trequest_length = 2,\n\t\twindow_id      = window_id,\n\t}\n\t{\n\t\tn_sent, err := os.send(socket, mem.ptr_to_bytes(&amp;request), 0)\n\t\tassert(err == os.ERROR_NONE)\n\t\tassert(n_sent == size_of(Request))\n\t}\n\n}\n\nx11_put_image :: proc(\n\tsocket: os.Socket,\n\tdrawable_id: u32,\n\tgc_id: u32,\n\twidth: u16,\n\theight: u16,\n\tdst_x: u16,\n\tdst_y: u16,\n\tdepth: u8,\n\tdata: []u8,\n) {\n\topcode: u8 : 72\n\n\tRequest :: struct #packed {\n\t\topcode:         u8,\n\t\tformat:         u8,\n\t\trequest_length: u16,\n\t\tdrawable_id:    u32,\n\t\tgc_id:          u32,\n\t\twidth:          u16,\n\t\theight:         u16,\n\t\tdst_x:          u16,\n\t\tdst_y:          u16,\n\t\tleft_pad:       u8,\n\t\tdepth:          u8,\n\t\tpad1:           u16,\n\t}\n\n\tdata_length_padded := round_up_4(cast(u32)len(data))\n\n\trequest := Request {\n\t\topcode         = opcode,\n\t\tformat         = 2, // ZPixmap\n\t\trequest_length = cast(u16)(6 + data_length_padded / 4),\n\t\tdrawable_id    = drawable_id,\n\t\tgc_id          = gc_id,\n\t\twidth          = width,\n\t\theight         = height,\n\t\tdst_x          = dst_x,\n\t\tdst_y          = dst_y,\n\t\tdepth          = depth,\n\t}\n\t{\n\t\tpadding_len := data_length_padded - cast(u32)len(data)\n\n\t\tn_sent, err := linux.writev(\n\t\t\tcast(linux.Fd)socket,\n\t\t\t[]linux.IO_Vec {\n\t\t\t\t{base = &amp;request, len = size_of(Request)},\n\t\t\t\t{base = raw_data(data), len = len(data)},\n\t\t\t\t{base = raw_data(data), len = cast(uint)padding_len},\n\t\t\t},\n\t\t)\n\t\tassert(err == .NONE)\n\t\tassert(n_sent == size_of(Request) + len(data) + cast(int)padding_len)\n\t}\n}\n\nrender :: proc(socket: os.Socket, scene: ^Scene) {\n\tfor entity, i in scene.displayed_entities {\n\t\trect := ASSET_COORDINATES[entity]\n\t\trow, column := idx_to_row_column(i)\n\n\t\tx11_copy_area(\n\t\t\tsocket,\n\t\t\tscene.sprite_pixmap_id,\n\t\t\tscene.window_id,\n\t\t\tscene.gc_id,\n\t\t\trect.x,\n\t\t\trect.y,\n\t\t\tcast(u16)column * ENTITIES_WIDTH,\n\t\t\tcast(u16)row * ENTITIES_HEIGHT,\n\t\t\tENTITIES_WIDTH,\n\t\t\tENTITIES_HEIGHT,\n\t\t)\n\t}\n}\n\nENTITIES_ROW_COUNT :: 16\nENTITIES_COLUMN_COUNT :: 16\nENTITIES_WIDTH :: 16\nENTITIES_HEIGHT :: 16\n\nScene :: struct {\n\twindow_id:          u32,\n\tgc_id:              u32,\n\tsprite_pixmap_id:   u32,\n\tdisplayed_entities: [ENTITIES_ROW_COUNT * ENTITIES_COLUMN_COUNT]Entity_kind,\n\t// TODO: Bitfield?\n\tmines:              [ENTITIES_ROW_COUNT * ENTITIES_COLUMN_COUNT]bool,\n}\n\nwait_for_x11_events :: proc(socket: os.Socket, scene: ^Scene) {\n\tGenericEvent :: struct #packed {\n\t\tcode: u8,\n\t\tpad:  [31]u8,\n\t}\n\tassert(size_of(GenericEvent) == 32)\n\n\tKeyReleaseEvent :: struct #packed {\n\t\tcode:            u8,\n\t\tdetail:          u8,\n\t\tsequence_number: u16,\n\t\ttime:            u32,\n\t\troot_id:         u32,\n\t\tevent:           u32,\n\t\tchild_id:        u32,\n\t\troot_x:          u16,\n\t\troot_y:          u16,\n\t\tevent_x:         u16,\n\t\tevent_y:         u16,\n\t\tstate:           u16,\n\t\tsame_screen:     bool,\n\t\tpad1:            u8,\n\t}\n\tassert(size_of(KeyReleaseEvent) == 32)\n\n\tButtonReleaseEvent :: struct #packed {\n\t\tcode:        u8,\n\t\tdetail:      u8,\n\t\tseq_number:  u16,\n\t\ttimestamp:   u32,\n\t\troot:        u32,\n\t\tevent:       u32,\n\t\tchild:       u32,\n\t\troot_x:      u16,\n\t\troot_y:      u16,\n\t\tevent_x:     u16,\n\t\tevent_y:     u16,\n\t\tstate:       u16,\n\t\tsame_screen: bool,\n\t\tpad1:        u8,\n\t}\n\tassert(size_of(ButtonReleaseEvent) == 32)\n\n\tEVENT_EXPOSURE: u8 : 0xc\n\tEVENT_KEY_RELEASE: u8 : 0x3\n\tEVENT_BUTTON_RELEASE: u8 : 0x5\n\n\tKEYCODE_ENTER: u8 : 36\n\n\tfor {\n\t\tgeneric_event := GenericEvent{}\n\t\tn_recv, err := os.recv(socket, mem.ptr_to_bytes(&amp;generic_event), 0)\n\t\tif err == os.EPIPE || n_recv == 0 {\n\t\t\tos.exit(0) // The end.\n\t\t}\n\n\t\tassert(err == os.ERROR_NONE)\n\t\tassert(n_recv == size_of(GenericEvent))\n\n\t\tswitch generic_event.code {\n\t\tcase EVENT_EXPOSURE:\n\t\t\trender(socket, scene)\n\n\t\tcase EVENT_KEY_RELEASE:\n\t\t\tevent := transmute(KeyReleaseEvent)generic_event\n\t\t\tif event.detail == KEYCODE_ENTER {\n\t\t\t\treset(scene)\n\t\t\t\trender(socket, scene)\n\t\t\t}\n\n\t\tcase EVENT_BUTTON_RELEASE:\n\t\t\tevent := transmute(ButtonReleaseEvent)generic_event\n\t\t\ton_cell_clicked(event.event_x, event.event_y, scene)\n\t\t\trender(socket, scene)\n\t\t}\n\t}\n}\n\nreset :: proc(scene: ^Scene) {\n\tfor &amp;entity in scene.displayed_entities {\n\t\tentity = .Covered\n\t}\n\n\tfor &amp;mine in scene.mines {\n\t\tmine = rand.choice([]bool{true, false, false, false})\n\t}\n}\n\nx11_copy_area :: proc(\n\tsocket: os.Socket,\n\tsrc_id: u32,\n\tdst_id: u32,\n\tgc_id: u32,\n\tsrc_x: u16,\n\tsrc_y: u16,\n\tdst_x: u16,\n\tdst_y: u16,\n\twidth: u16,\n\theight: u16,\n) {\n\topcode: u8 : 62\n\tRequest :: struct #packed {\n\t\topcode:         u8,\n\t\tpad1:           u8,\n\t\trequest_length: u16,\n\t\tsrc_id:         u32,\n\t\tdst_id:         u32,\n\t\tgc_id:          u32,\n\t\tsrc_x:          u16,\n\t\tsrc_y:          u16,\n\t\tdst_x:          u16,\n\t\tdst_y:          u16,\n\t\twidth:          u16,\n\t\theight:         u16,\n\t}\n\n\trequest := Request {\n\t\topcode         = opcode,\n\t\trequest_length = 7,\n\t\tsrc_id         = src_id,\n\t\tdst_id         = dst_id,\n\t\tgc_id          = gc_id,\n\t\tsrc_x          = src_x,\n\t\tsrc_y          = src_y,\n\t\tdst_x          = dst_x,\n\t\tdst_y          = dst_y,\n\t\twidth          = width,\n\t\theight         = height,\n\t}\n\t{\n\t\tn_sent, err := os.send(socket, mem.ptr_to_bytes(&amp;request), 0)\n\t\tassert(err == os.ERROR_NONE)\n\t\tassert(n_sent == size_of(Request))\n\t}\n}\n\non_cell_clicked :: proc(x: u16, y: u16, scene: ^Scene) {\n\tidx, row, column := locate_entity_by_coordinate(x, y)\n\n\tmined := scene.mines[idx]\n\n\tif mined {\n\t\tscene.displayed_entities[idx] = .Mine_exploded\n\t\t// Lose.\n\t\tuncover_all_cells(&amp;scene.displayed_entities, &amp;scene.mines, .Mine_exploded)\n\t} else {\n\t\tvisited := [ENTITIES_COLUMN_COUNT * ENTITIES_ROW_COUNT]bool{}\n\t\tuncover_cells_flood_fill(row, column, &amp;scene.displayed_entities, &amp;scene.mines, &amp;visited)\n\n\t\t// Win.\n\t\tif count_remaining_goals(scene.displayed_entities, scene.mines) == 0 {\n\t\t\tuncover_all_cells(&amp;scene.displayed_entities, &amp;scene.mines, .Mine_idle)\n\t\t}\n\t}\n}\n\ncount_remaining_goals :: proc(\n\tdisplayed_entities: [ENTITIES_COLUMN_COUNT * ENTITIES_ROW_COUNT]Entity_kind,\n\tmines: [ENTITIES_COLUMN_COUNT * ENTITIES_ROW_COUNT]bool,\n) -&gt; int {\n\n\tcovered := 0\n\n\tfor entity in displayed_entities {\n\t\tcovered += cast(int)(entity == .Covered)\n\t}\n\n\tmines_count := 0\n\n\tfor mine in mines {\n\t\tmines_count += cast(int)mine\n\t}\n\n\treturn covered - mines_count\n}\n\nuncover_all_cells :: proc(\n\tdisplayed_entities: ^[ENTITIES_COLUMN_COUNT * ENTITIES_ROW_COUNT]Entity_kind,\n\tmines: ^[ENTITIES_ROW_COUNT * ENTITIES_COLUMN_COUNT]bool,\n\tshown_mine: Entity_kind,\n) {\n\tfor &amp;entity, i in displayed_entities {\n\t\tif mines[i] {\n\t\t\tentity = shown_mine\n\t\t} else {\n\t\t\trow, column := idx_to_row_column(i)\n\t\t\tmines_around_count := count_mines_around_cell(row, column, mines[:])\n\t\t\tassert(mines_around_count &lt;= 8)\n\n\t\t\tentity = cast(Entity_kind)(cast(int)Entity_kind.Uncovered_0 + mines_around_count)\n\t\t}\n\t}\n}\n\nuncover_cells_flood_fill :: proc(\n\trow: int,\n\tcolumn: int,\n\tdisplayed_entities: ^[ENTITIES_COLUMN_COUNT * ENTITIES_ROW_COUNT]Entity_kind,\n\tmines: ^[ENTITIES_ROW_COUNT * ENTITIES_COLUMN_COUNT]bool,\n\tvisited: ^[ENTITIES_COLUMN_COUNT * ENTITIES_ROW_COUNT]bool,\n) {\n\ti := row_column_to_idx(row, column)\n\tif visited[i] {return}\n\n\tvisited[i] = true\n\n\t// Do not uncover covered mines.\n\tif mines[i] {return}\n\n\tif displayed_entities[i] != .Covered {return}\n\n\t// Uncover cell.\n\n\tmines_around_count := count_mines_around_cell(row, column, mines[:])\n\tassert(mines_around_count &lt;= 8)\n\n\tdisplayed_entities[i] =\n\tcast(Entity_kind)(cast(int)Entity_kind.Uncovered_0 + mines_around_count)\n\n\t// Uncover neighbors.\n\n\t// Up.\n\tif !(row == 0) {\n\t\tuncover_cells_flood_fill(row - 1, column, displayed_entities, mines, visited)\n\t}\n\n\t// Right\n\tif !(column == (ENTITIES_COLUMN_COUNT - 1)) {\n\t\tuncover_cells_flood_fill(row, column + 1, displayed_entities, mines, visited)\n\t}\n\n\t// Bottom.\n\tif !(row == (ENTITIES_ROW_COUNT - 1)) {\n\t\tuncover_cells_flood_fill(row + 1, column, displayed_entities, mines, visited)\n\t}\n\n\t// Left.\n\tif !(column == 0) {\n\t\tuncover_cells_flood_fill(row, column - 1, displayed_entities, mines, visited)\n\t}\n}\n\nidx_to_row_column :: #force_inline proc(i: int) -&gt; (int, int) {\n\tcolumn := i % ENTITIES_COLUMN_COUNT\n\trow := i / ENTITIES_ROW_COUNT\n\n\treturn row, column\n}\n\nrow_column_to_idx :: #force_inline proc(row: int, column: int) -&gt; int {\n\treturn cast(int)row * ENTITIES_COLUMN_COUNT + cast(int)column\n}\n\ncount_mines_around_cell :: proc(row: int, column: int, displayed_entities: []bool) -&gt; int {\n\t// TODO: Pad the border to elide all bound checks?\n\n\tup_left :=\n\t\trow == 0 || column == 0 \\\n\t\t? false \\\n\t\t: displayed_entities[row_column_to_idx(row - 1, column - 1)]\n\tup := row == 0 ? false : displayed_entities[row_column_to_idx(row - 1, column)]\n\tup_right :=\n\t\trow == 0 || column == (ENTITIES_COLUMN_COUNT - 1) \\\n\t\t? false \\\n\t\t: displayed_entities[row_column_to_idx(row - 1, column + 1)]\n\tright :=\n\t\tcolumn == (ENTITIES_COLUMN_COUNT - 1) \\\n\t\t? false \\\n\t\t: displayed_entities[row_column_to_idx(row, column + 1)]\n\tbottom_right :=\n\t\trow == (ENTITIES_ROW_COUNT - 1) || column == (ENTITIES_COLUMN_COUNT - 1) \\\n\t\t? false \\\n\t\t: displayed_entities[row_column_to_idx(row + 1, column + 1)]\n\tbottom :=\n\t\trow == (ENTITIES_ROW_COUNT - 1) \\\n\t\t? false \\\n\t\t: displayed_entities[row_column_to_idx(row + 1, column)]\n\tbottom_left :=\n\t\tcolumn == 0 || row == (ENTITIES_COLUMN_COUNT - 1) \\\n\t\t? false \\\n\t\t: displayed_entities[row_column_to_idx(row + 1, column - 1)]\n\tleft := column == 0 ? false : displayed_entities[row_column_to_idx(row, column - 1)]\n\n\n\treturn(\n\t\tcast(int)up_left +\n\t\tcast(int)up +\n\t\tcast(int)up_right +\n\t\tcast(int)right +\n\t\tcast(int)bottom_right +\n\t\tcast(int)bottom +\n\t\tcast(int)bottom_left +\n\t\tcast(int)left \\\n\t)\n}\n\nlocate_entity_by_coordinate :: proc(win_x: u16, win_y: u16) -&gt; (idx: int, row: int, column: int) {\n\tcolumn = cast(int)win_x / ENTITIES_WIDTH\n\trow = cast(int)win_y / ENTITIES_HEIGHT\n\n\tidx = row_column_to_idx(row, column)\n\n\treturn idx, row, column\n}\n\nx11_create_pixmap :: proc(\n\tsocket: os.Socket,\n\twindow_id: u32,\n\tpixmap_id: u32,\n\twidth: u16,\n\theight: u16,\n\tdepth: u8,\n) {\n\topcode: u8 : 53\n\n\tRequest :: struct #packed {\n\t\topcode:         u8,\n\t\tdepth:          u8,\n\t\trequest_length: u16,\n\t\tpixmap_id:      u32,\n\t\tdrawable_id:    u32,\n\t\twidth:          u16,\n\t\theight:         u16,\n\t}\n\n\trequest := Request {\n\t\topcode         = opcode,\n\t\tdepth          = depth,\n\t\trequest_length = 4,\n\t\tpixmap_id      = pixmap_id,\n\t\tdrawable_id    = window_id,\n\t\twidth          = width,\n\t\theight         = height,\n\t}\n\n\t{\n\t\tn_sent, err := os.send(socket, mem.ptr_to_bytes(&amp;request), 0)\n\t\tassert(err == os.ERROR_NONE)\n\t\tassert(n_sent == size_of(Request))\n\t}\n}\n\nmain :: proc() {\n\tpng_data := #load(&quot;sprite.png&quot;)\n\tsprite, err := png.load_from_bytes(png_data, {})\n\tassert(err == nil)\n\tsprite_data := make([]u8, sprite.height * sprite.width * 4)\n\n\t// Convert the image format from the sprite (RGB) into the X11 image format (BGRX).\n\tfor i := 0; i &lt; sprite.height * sprite.width - 3; i += 1 {\n\t\tsprite_data[i * 4 + 0] = sprite.pixels.buf[i * 3 + 2] // R -&gt; B\n\t\tsprite_data[i * 4 + 1] = sprite.pixels.buf[i * 3 + 1] // G -&gt; G\n\t\tsprite_data[i * 4 + 2] = sprite.pixels.buf[i * 3 + 0] // B -&gt; R\n\t\tsprite_data[i * 4 + 3] = 0 // pad\n\t}\n\n\tauth_token, _ := load_x11_auth_token(context.temp_allocator)\n\n\tsocket := connect_x11_socket()\n\tconnection_information := x11_handshake(socket, &amp;auth_token)\n\n\tgc_id := next_x11_id(0, connection_information)\n\tx11_create_graphical_context(socket, gc_id, connection_information.root_screen.id)\n\n\twindow_id := next_x11_id(gc_id, connection_information)\n\tx11_create_window(\n\t\tsocket,\n\t\twindow_id,\n\t\tconnection_information.root_screen.id,\n\t\t200,\n\t\t200,\n\t\tENTITIES_COLUMN_COUNT * ENTITIES_WIDTH,\n\t\tENTITIES_ROW_COUNT * ENTITIES_HEIGHT,\n\t\tconnection_information.root_screen.root_visual_id,\n\t)\n\n\timg_depth: u8 = 24\n\tpixmap_id := next_x11_id(window_id, connection_information)\n\tx11_create_pixmap(\n\t\tsocket,\n\t\twindow_id,\n\t\tpixmap_id,\n\t\tcast(u16)sprite.width,\n\t\tcast(u16)sprite.height,\n\t\timg_depth,\n\t)\n\tscene := Scene {\n\t\twindow_id        = window_id,\n\t\tgc_id            = gc_id,\n\t\tsprite_pixmap_id = pixmap_id,\n\t}\n\treset(&amp;scene)\n\n\tx11_put_image(\n\t\tsocket,\n\t\tscene.sprite_pixmap_id,\n\t\tscene.gc_id,\n\t\tcast(u16)sprite.width,\n\t\tcast(u16)sprite.height,\n\t\t0,\n\t\t0,\n\t\timg_depth,\n\t\tsprite_data,\n\t)\n\n\tx11_map_window(socket, window_id)\n\n\twait_for_x11_events(socket, &amp;scene)\n}\n\n\n@(test)\ntest_round_up_4 :: proc(_: ^testing.T) {\n\tassert(round_up_4(0) == 0)\n\tassert(round_up_4(1) == 4)\n\tassert(round_up_4(2) == 4)\n\tassert(round_up_4(3) == 4)\n\tassert(round_up_4(4) == 4)\n\tassert(round_up_4(5) == 8)\n\tassert(round_up_4(6) == 8)\n\tassert(round_up_4(7) == 8)\n\tassert(round_up_4(8) == 8)\n}\n\n@(test)\ntest_count_mines_around_cell :: proc(_: ^testing.T) {\n\t{\n\t\tmines := [ENTITIES_ROW_COUNT * ENTITIES_COLUMN_COUNT]bool{}\n\t\tmines[row_column_to_idx(0, 0)] = true\n\t\tmines[row_column_to_idx(0, 1)] = true\n\t\tmines[row_column_to_idx(0, 2)] = true\n\t\tmines[row_column_to_idx(1, 2)] = true\n\t\tmines[row_column_to_idx(2, 2)] = true\n\t\tmines[row_column_to_idx(2, 1)] = true\n\t\tmines[row_column_to_idx(2, 0)] = true\n\t\tmines[row_column_to_idx(1, 0)] = true\n\n\t\tassert(count_mines_around_cell(1, 1, mines[:]) == 8)\n\t}\n} ",
titles:[
{
title:"What we\'re making",
slug:"what-we-re-making",
offset:1957,
},
{
title:"Authentication",
slug:"authentication",
offset:4159,
},
{
title:"Opening a window",
slug:"opening-a-window",
offset:11110,
},
{
title:"Loading assets",
slug:"loading-assets",
offset:21786,
},
{
title:"The game entities",
slug:"the-game-entities",
offset:28790,
},
{
title:"Reacting to keyboard and mouse events",
slug:"reacting-to-keyboard-and-mouse-events",
offset:31435,
},
{
title:"Game logic: uncover a cell",
slug:"game-logic-uncover-a-cell",
offset:34023,
},
{
title:"Conclusion",
slug:"conclusion",
offset:38055,
},
{
title:"Addendum: the full code",
slug:"addendum-the-full-code",
offset:38905,
},
],
},
{
html_file_name:"odin_and_musl.html",
title:"Odin and musl: Cross-compiling Odin programs for the Raspberry Pi Zero",
text:"Odin programming language is becoming my favorite tool as a Software Engineer. It\'s a fantastic programming language, mostly because it is dead simple. I have purchased some time ago a Raspberry Pi Zero 2, and I found myself wanting to write command-line Odin programs for it. Here it is in all its beauty: Here\'s the story of how I did it. If you do not work with Odin but do work a lot with cross-compilation, like I do at work, all of these techniques will be, I believe, very valuable anyway. Note: ARM64 is sometimes also called AARCH64 interchangeably. Note 2: The Rapsberry Pi Zero 1 is based on ARM (32 bits). The Raspberry Pi Zero 2 is based on ARM64 (64 bits). If you have a Raspberry Pi Zero 1, this article still applies, just adjust the target when cross-compiling. Inciting incident The thing is, I work on an Intel Linux laptop and the Zero is a Linux ARM 64 bits piece of hardware. It\'s also a relatively cheap component with only 512 MiB of RAM and a slow CPU (compared to a modern developer workstation), and based on a very slow SD card, so it\'s not fast to install the required tools and to build source code on it. Cross-compilation is much easier and faster. Odin can cross-compile to it with -target=linux_arm64 , so that\'s great, let\'s try it: $ odin build src -target=linux_arm64\n[...]\n/usr/bin/ld: /home/pg/my-code/odin-music-chords-placements/src.o: error adding symbols: file in wrong format\nclang: error: linker command failed with exit code 1 (use -v to see invocation) Oh no...The key part is: file in wrong format . That\'s because behind the scenes, the Odin compiler builds our code into an ARM64 object file, which is great. But then it tries to link this object file with libc, which on this computer is a x86_64 library, and that won\'t work. We can confirm this theory by asking Odin to print the linking command: $ odin build src -target=linux_arm64 -print-linker-flags\nclang -Wno-unused-command-line-argument [...]  -lm -lc   -L/       -no-pie And we see it links libc with -lc , meaning it links our program with the local libc it finds on my machine which is a different architecture than our target. Confrontation What we want is to link our object file with the correct libc, meaning one that has been built for ARM64. Moreover, we\'d like to build our program statically with libc so that we can simply copy the one executable to the Raspberry Pi Zero and it is fully self-contained. We completely side-step issues of different glibc versions not being compatible with each other. Enter musl, a C library for Linux that supports many platforms including ARM64, and static compilation. That\'s exactly what we need! A big difference between Odin and Zig is that Zig is a full cross-compilation toolchain: it comes with the source code of musl , and has put in a ton of work to cross-compile it to the target the user desires. So to make our use-case work with Odin, without Odin the toolchain supporting what Zig supports, what we need to do is cross-compile our code to an ARM64 object file but without linking it yet. Then we link it manually to musl libc that has been built for ARM64. We could download this musl artifact from the internet but it\'s both more educational, empowering, and secure, to build it ourselves. So let\'s do this, it\'s not too much work. To build musl, we can either use clang since it is a cross-compiler by default, or a GCC toolchain that has been made to target ARM64. Most Linux distributions provide such a compiler as a package typically called gcc-aarch64-xxx e.g. sudo apt-get install gcc-aarch64-linux-gnu or sudo dnf install gcc-aarch64-linux-gnu . So let\'s now build a static musl for ARM64, following the official instructions. We just need to this once: $ git clone --recurse --depth 1 https://git.musl-libc.org/git/musl\n$ cd musl\n\n# With Clang:\n$ CFLAGS=\'--target=aarch64-unknown-linux-musl\' RANLIB=llvm-ranlib AR=llvm-ar CC=clang ./configure --target=aarch64 --disable-shared\n# Or with GCC:\n$ RANLIB=/usr/bin/aarch64-linux-gnu-gcc-ranlib AR=/usr/bin/aarch64-linux-gnu-gcc-ar CC=/usr/bin/aarch64-linux-gnu-gcc ./configure --target=aarch64 --disable-shared\n\n# Either way (Clang/GCC), the build command itself is the same.\n$ make We now have the two artifacts we want: crt1.o and libc.a . We can confirm that they have been correctly built for our target: $ file lib/crt1.o\nlib/crt1.o: ELF 64-bit LSB relocatable, ARM aarch64, version 1 (SYSV), not stripped\n$ readelf -h lib/libc.a | grep \'^\\s*Machine:\'\n  Machine:                           AArch64\n  Machine:                           AArch64\n  Machine:                           AArch64\n  [...] Resolution Now we can finally put all the pieces together. We can use any linker, I am using LLD (the LLVM linker) here, but the GNU LD linker would also work as long as it knows to target ARM64 e.g. using the one coming with the right GCC toolchain would work. $ odin build src  -target=linux_arm64 -build-mode=object\n$ file src.o\nsrc.o: ELF 64-bit LSB relocatable, ARM aarch64, version 1 (SYSV), not stripped\n$ ld.lld main.o ~/not-my-code/musl/lib/libc.a ~/not-my-code/musl/lib/crt1.o\n$ file a.out\na.out: ELF 64-bit LSB executable, ARM aarch64, version 1 (SYSV), statically linked, not stripped Alternatively, we can decide to stick with the Odin compiler through and through, and we pass it the (lengthy) required build options: $ odin build src -target=linux_arm64 -extra-linker-flags:\'-L ~/not-my-code/musl/lib/ -nostdlib -fuse-ld=lld --target=linux-aarch64 ~/not-my-code/musl/lib/crt1.o -static\' We can even verify it works by running it inside an ARM64 Linux system using qemu : $ qemu-aarch64-static a.out\n# It runs! Cherry on the cake, the resulting program is small: $ llvm-strip a.out\n$ du -h a.out \n288K\ta.out So it\'s a breeze to scp or rsync our small executable over to the Raspberry Pi Zero while hacking on it. Perhaps Odin will have built-in support for musl in the future like Zig does. In the meantime, this article shows it\'s absolutely possible to do that ourselves! By the way, this technique can be used to cross-compile any C library that\'s a dependency of our project, assuming the library did not do anything silly that would prevent cross-compilation. Appendix: Maybe you don\'t even need a libc Odin comes with batteries included with a rich standard library. So why do we even need libc? Let\'s inspect which functions we really use from libc, i.e. are undefined symbols in the object file built from our source code: $ nm -u src.o\n                 U calloc\n                 U free\n                 U malloc\n                 U memcpy\n                 U memmove\n                 U memset\n                 U realloc Ok, so basically: heap allocation and some functions to copy/set memory. Heap allocation functions are not actually required if our program does not do heap allocations (Odin provides the option -default-to-nil-allocator for this case), or if we implement these ourselves, for example with a naive mmap implementation, or by setting in our program the default allocator to be an arena. Odin has first class support for custom allocators! The functions to manipulate memory are required even if we do not call them directly because typically, the compiler will replace some code patterns, e.g. struct or array initialization, with these functions behind the scene. These memxxx functions could potentially be implemented by us, likely incurring a performance cost compared to the hand-optimized libc versions. But Odin can provide them for us! We can just use the -no-crt option. Note that not all targets will be equally supported for this use-case. ARM64 is not yet supported, so I will demonstrate targeting AMD64 (i.e. Intel/AMD 64 bits). I also had to install nasm to make it work because Odin ships with some assembly files which are then built for the target with nasm , but Odin does not ship with nasm itself. Let\'s try with a \'hello world\' example: package main\n\nimport &quot;core:fmt&quot;\n\nmain :: proc() {\n\tfmt.println(&quot;Hello&quot;)\n} We can build it as outlined like this: $ odin build hello.odin -file -target=linux_amd64 -default-to-nil-allocator -no-crt\n$ file hello\nhello: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, BuildID[sha1]=ef8dfc9dc297295808f80ec66e92763358a598d1, not stripped And we can see the malloc symbol is not present since we opted out of it, and that Odin provided with these assembly files the correct implementation for memset : $ nm hello | grep malloc\n# Nothing\n$ nm hello | grep memset\n00000000004042c0 T memset I\'ll soon write about what programs I made for the Raspberry Pi Zero, so check back soon! ",
titles:[
{
title:"Inciting incident",
slug:"inciting-incident",
offset:779,
},
{
title:"Confrontation",
slug:"confrontation",
offset:2141,
},
{
title:"Resolution",
slug:"resolution",
offset:4627,
},
{
title:"Appendix: Maybe you don\'t even need a libc",
slug:"appendix-maybe-you-don-t-even-need-a-libc",
offset:6206,
},
],
},
{
html_file_name:"rust_c++_interop_trick.html",
title:"A small trick for simple Rust/C++ interop",
text:"Discussions: /r/rust , HN . I am rewriting a gnarly C++ codebase in Rust at work. Due to the heavy use of callbacks (sigh), Rust sometimes calls C++ and C++ sometimes calls Rust. This done by having both sides expose a C API for the functions they want the other side to be able to call. This is for functions; but what about C++ methods? Here is a trick to rewrite one C++ method at a time, without headaches. And by the way, this works whatever the language you are rewriting the project in, it does not have to be Rust! The trick Make the C++ class a standard layout class . This is defined by the C++ standard. In layman terms, this makes the C++ class be similar to a plain C struct. With a few allowances, for example the C++ class can still use inheritance and a few other things. Most notably, virtual methods are forbidden. I don\'t care about this limitation because I never use virtual methods myself and this is my least favorite feature in any programming language. Create a Rust struct with the exact same layout as the C++ class. Create a Rust function with a C calling convention, whose first argument is this Rust class. You can now access every C++ member of the class! Note: Depending on the C++ codebase you find yourself in, the first point could be either trivial or not feasible at all. It depends on the amount of virtual methods used, etc. In my case, there were a handful of virtual methods, which could all be advantageously made non virtual, so I first did this. This is all very abstract? Let\'s proceed with an example! Example Here is our fancy C++ class, User . It stores a name, a uuid, and a comment count. A user can write comments, which is just a string, that we print. // Path: user.cpp\n\n#include &lt;cstdint&gt;\n#include &lt;cstdio&gt;\n#include &lt;cstring&gt;\n#include &lt;string&gt;\n\nclass User {\n  std::string name;\n  uint64_t comments_count;\n  uint8_t uuid[16];\n\npublic:\n  User(std::string name_) : name{name_}, comments_count{0} {\n    arc4random_buf(uuid, sizeof(uuid));\n  }\n\n  void write_comment(const char *comment, size_t comment_len) {\n    printf(&quot;%s (&quot;, name.c_str());\n    for (size_t i = 0; i &lt; sizeof(uuid); i += 1) {\n      printf(&quot;%x&quot;, uuid[i]);\n    }\n    printf(&quot;) says: %.*s\\n&quot;, (int)comment_len, comment);\n    comments_count += 1;\n  }\n\n  uint64_t get_comment_count() { return comments_count; }\n};\n\nint main() {\n  User alice{&quot;alice&quot;};\n  const char msg[] = &quot;hello, world!&quot;;\n  alice.write_comment(msg, sizeof(msg) - 1);\n\n  printf(&quot;Comment count: %lu\\n&quot;, alice.get_comment_count());\n\n  // This prints:\n  // alice (fe61252cf5b88432a7e8c8674d58d615) says: hello, world!\n  // Comment count: 1\n} So let\'s first ensure it is a standard layout class. We add this compile-time assertion in the constructor (could be placed anywhere, but the constructor is as good a place as any): // Path: user.cpp\n\n    static_assert(std::is_standard_layout_v&lt;User&gt;); And... it builds! Now onto the second step: let\'s define the equivalent class on the Rust side. We create a new Rust library project: $ cargo new --lib user-rs-lib And place our Rust struct in src/lib.rs . We just need to be careful about alignment (padding between fields) and the order the fields, so we mark the struct repr(C) to make the Rust compiler use the same layout as C does: // Path: ./user-rs/src/lib.rs\n\n#[repr(C)]\npub struct UserC {\n    pub name: [u8; 32],\n    pub comments_count: u64,\n    pub uuid: [u8; 16],\n} Note that the fields can be named differently from the C++ fields if you so choose. Also note that std::string is represented here by an opaque array of 32 bytes. That\'s because on my machine, with the standard library I have, sizeof(std::string) is 32. That is not guaranteed by the standard, so this makes it very much not portable. We\'ll go over some options to work-around this at the end. I wanted to include a standard library type to show that it does not prevent the class from being a \'standard layout class\', but that is also creates challenges. For now, let\'s forget about this hurdle. We can also write a stub for the Rust function equivalent to the C++ method: // Path: ./user-rs-lib/src/lib.rs\n\n#[no_mangle]\npub extern &quot;C&quot; fn RUST_write_comment(user: &amp;mut UserC, comment: *const u8, comment_len: usize) {\n    todo!()\n} Now, let\'s use the tool cbindgen to generate the C header corresponding to this Rust code: $ cargo install cbindgen\n$ cbindgen -v src/lib.rs --lang=c++ -o ../user-rs-lib.h And we get this C header: // Path: user-rs-lib.h\n\n#include &lt;cstdarg&gt;\n#include &lt;cstdint&gt;\n#include &lt;cstdlib&gt;\n#include &lt;ostream&gt;\n#include &lt;new&gt;\n\nstruct UserC {\n  uint8_t name[32];\n  uint64_t comments_count;\n  uint8_t uuid[16];\n};\n\nextern &quot;C&quot; {\n\nvoid RUST_write_comment(UserC *user, const uint8_t *comment, uintptr_t comment_len);\n\n} // extern &quot;C&quot; Now, let\'s go back to C++, include this C header, and add lots of compile-time assertions to ensure that the layouts are indeed the same. Again, I place these asserts in the constructor: #include &quot;user-rs-lib.h&quot;\n\nclass User {\n // [..]\n\n  User(std::string name_) : name{name_}, comments_count{0} {\n    arc4random_buf(uuid, sizeof(uuid));\n\n    static_assert(std::is_standard_layout_v&lt;User&gt;);\n    static_assert(sizeof(std::string) == 32);\n    static_assert(sizeof(User) == sizeof(UserC));\n    static_assert(offsetof(User, name) == offsetof(UserC, name));\n    static_assert(offsetof(User, comments_count) ==\n                  offsetof(UserC, comments_count));\n    static_assert(offsetof(User, uuid) == offsetof(UserC, uuid));\n  }\n\n  // [..]\n} With that, we are certain that the layout in memory of the C++ class and the Rust struct are the same. We could probably generate all of these asserts, with a macro or with a code generator, but for this article, it\'s fine to do manually. So let\'s rewrite the C++ method in Rust. We will for now leave out the name field since it is a bit problematic. Later we will see how we can still use it from Rust: // Path: ./user-rs-lib/src/lib.rs\n\n#[no_mangle]\npub extern &quot;C&quot; fn RUST_write_comment(user: &amp;mut UserC, comment: *const u8, comment_len: usize) {\n    let comment = unsafe { std::slice::from_raw_parts(comment, comment_len) };\n    let comment_str = unsafe { std::str::from_utf8_unchecked(comment) };\n    println!(&quot;({:x?}) says: {}&quot;, user.uuid.as_slice(), comment_str);\n\n    user.comments_count += 1;\n} We want to build a static library so we instruct cargo to do so by sticking these lines in Cargo.toml : [lib]\ncrate-type = [&quot;staticlib&quot;] We now build: $ cargo build\n# This is our artifact:\n$ ls target/debug/libuser_rs_lib.a We can use our Rust function from C++ in main , with some cumbersome casts: // Path: user.cpp\n\nint main() {\n  User alice{&quot;alice&quot;};\n  const char msg[] = &quot;hello, world!&quot;;\n  alice.write_comment(msg, sizeof(msg) - 1);\n\n  printf(&quot;Comment count: %lu\\n&quot;, alice.get_comment_count());\n\n  RUST_write_comment(reinterpret_cast&lt;UserC *&gt;(&amp;alice),\n                     reinterpret_cast&lt;const uint8_t *&gt;(msg), sizeof(msg) - 1);\n  printf(&quot;Comment count: %lu\\n&quot;, alice.get_comment_count());\n} And link (manually) our brand new Rust library to our C++ program: $ clang++ user.cpp ./user-rs-lib/target/debug/libuser_rs_lib.a\n$ ./a.out\nalice (336ff4cec0a2ccbfc0c4e4cb9ba7c152) says: hello, world!\nComment count: 1\n([33, 6f, f4, ce, c0, a2, cc, bf, c0, c4, e4, cb, 9b, a7, c1, 52]) says: hello, world!\nComment count: 2 The output is slightly different for the uuid, because we use in the Rust implementation the default Debug trait to print the slice, but the content is the same. A couple of thoughts: The calls alice.write_comment(..) and RUST_write_comment(alice, ..) are strictly equivalent and in fact, a C++ compiler will transform the former into the latter in a pure C++ codebase, if you look at the assembly generated. So our Rust function is just mimicking what the C++ compiler would do anyway. However, we are free to have the User argument be in any position in the function. An other way to say it: We rely on the API, not the ABI, compatibility. The Rust implementation can freely read and modify private members of the C++ class, for example the comment_count field is only accessible in C++ through the getter, but Rust can just access it as if it was public. That\'s because public/private are just rules enforced by the C++ compiler. However your CPU does not know nor care. The bytes are the bytes. If you can access the bytes at runtime, it does not matter that they were marked \'private\' in the source code. We have to use tedious casts which is normal. We are indeed reinterpreting memory from one type ( User ) to another ( UserC ). This is allowed by the standard because the C++ class is a \'standard layout class\'. If it was not the case, this would be undefined behavior and likely work on some platforms but break on others. Accessing std::string from Rust std::string should be an opaque type from the perspective of Rust, because it is not the same across platforms or even compiler versions, so we cannot exactly describe its layout. But we only want to access the underlying bytes of the string. We thus need a helper on the C++ side, that will extract these bytes for us. First, the Rust side. We define a helper type ByteSliceView which is a pointer and a length (the equivalent of a std::string_view in C++ latest versions and &amp;[u8] in Rust), and our Rust function now takes an additional parameter, the name : #[repr(C)]\n// Akin to `&amp;[u8]`, for C.\npub struct ByteSliceView {\n    pub ptr: *const u8,\n    pub len: usize,\n}\n\n\n#[no_mangle]\npub extern &quot;C&quot; fn RUST_write_comment(\n    user: &amp;mut UserC,\n    comment: *const u8,\n    comment_len: usize,\n    name: ByteSliceView, // &lt;-- Additional parameter\n) {\n    let comment = unsafe { std::slice::from_raw_parts(comment, comment_len) };\n    let comment_str = unsafe { std::str::from_utf8_unchecked(comment) };\n\n    let name_slice = unsafe { std::slice::from_raw_parts(name.ptr, name.len) };\n    let name_str = unsafe { std::str::from_utf8_unchecked(name_slice) };\n\n    println!(\n        &quot;{} ({:x?}) says: {}&quot;,\n        name_str,\n        user.uuid.as_slice(),\n        comment_str\n    );\n\n    user.comments_count += 1;\n} We re-run cbindgen, and now C++ has access to the ByteSliceView type. We thus write a helper to convert a std::string to this type, and pass the additional parameter to the Rust function (we also define a trivial get_name() getter for User since name is still private): // Path: user.cpp\n\nByteSliceView get_std_string_pointer_and_length(const std::string &amp;str) {\n  return {\n      .ptr = reinterpret_cast&lt;const uint8_t *&gt;(str.data()),\n      .len = str.size(),\n  };\n}\n\n// In main:\nint main() {\n    // [..]\n  RUST_write_comment(reinterpret_cast&lt;UserC *&gt;(&amp;alice),\n                     reinterpret_cast&lt;const uint8_t *&gt;(msg), sizeof(msg) - 1,\n                     get_std_string_pointer_and_length(alice.get_name()));\n} We re-build, re-run, and lo and behold, the Rust implementation now prints the name: alice (69b7c41491ccfbd28c269ea4091652d) says: hello, world!\nComment count: 1\nalice ([69, b7, c4, 14, 9, 1c, cf, bd, 28, c2, 69, ea, 40, 91, 65, 2d]) says: hello, world!\nComment count: 2 Alternatively, if we cannot or do not want to change the Rust signature, we can make the C++ helper get_std_string_pointer_and_length have a C convention and take a void pointer, so that Rust will call the helper itself, at the cost of numerous casts in and out of void* . Improving the std::string situation Instead of modeling std::string as an array of bytes whose size is platform-dependent, we could move this field to the end of the C++ class and remove it entirely from Rust (since it is unused there). This would break sizeof(User) == sizeof(UserC) , it would now be sizeof(User) - sizeof(std::string) == sizeof(UserC) . Thus, the layout would be exactly the same (until the last field which is fine) between C++ and Rust. However, it will be an ABI breakage, if external users depend on the exact layout of the C++ class, and C++ constructors will have to be adapted since they rely on the order of fields. This approach is basically the same as the flexible array member feature in C. If allocations are cheap, we could store the name as a pointer: std::string * name; on the C++ side, and on the Rust side, as a void pointer: name: *const std::ffi::c_void , since pointers have a guaranteed size on all platforms. That has the advantage that Rust can access the data in std::string , by calling a C++ helper with a C calling convention. But some will dislike that a naked pointer is being used in C++. Conclusion We now have successfully re-written a C++ class method. This technique is great because the C++ class could have hundreds of methods, in a real codebase, and we can still rewrite them one at a time, without breaking or touching the others. The big caveat is that: the more C++ specific features and standard types the class is using, the more difficult this technique is to apply, necessitating helpers to make conversions from one type to another, and/or numerous tedious casts. If the C++ class is basically a C struct only using C types, it will be very easy. Still, I have employed this technique at work a lot and I really enjoy its relative simplicity and incremental nature. It can also be in theory automated, say with tree-sitter or libclang to operate on the C++ AST: Add a compile-time assert in the C++ class constructor to ensure it is a \'standard layout class\' e.g. static_assert(std::is_standard_layout_v&lt;User&gt;); . If this fails, skip this class, it requires manual intervention. Generate the equivalent Rust struct e.g. the struct UserC. For each field of the C++ class/Rust struct, add a compile-time assert to make sure the layout is the same e.g. static_assert(sizeof(User) == sizeof(UserC)); static_assert(offsetof(User, name) == offsetof(UserC, name)); . If this fails, bail. For each C++ method, generate an (empty) equivalent Rust function. E.g. RUST_write_comment . A developer implements the Rust function. Or AI. Or something. For each call site in C++, replace the C++ method call by a call to the Rust function. E.g. alice.write_comment(..); becomes RUST_write_comment(alice, ..); . Delete the C++ methods that have been rewritten. And boom, project rewritten. Addendum: the full code The full code // Path: user.cpp\n\n#include &quot;user-rs-lib.h&quot;\n#include &lt;cstdint&gt;\n#include &lt;cstdio&gt;\n#include &lt;cstring&gt;\n#include &lt;string&gt;\n\nextern &quot;C&quot; ByteSliceView\nget_std_string_pointer_and_length(const std::string &amp;str) {\n  return {\n      .ptr = reinterpret_cast&lt;const uint8_t *&gt;(str.data()),\n      .len = str.size(),\n  };\n}\n\nclass User {\n  std::string name;\n  uint64_t comments_count;\n  uint8_t uuid[16];\n\npublic:\n  User(std::string name_) : name{name_}, comments_count{0} {\n    arc4random_buf(uuid, sizeof(uuid));\n\n    static_assert(std::is_standard_layout_v&lt;User&gt;);\n    static_assert(sizeof(std::string) == 32);\n    static_assert(sizeof(User) == sizeof(UserC));\n    static_assert(offsetof(User, name) == offsetof(UserC, name));\n    static_assert(offsetof(User, comments_count) ==\n                  offsetof(UserC, comments_count));\n    static_assert(offsetof(User, uuid) == offsetof(UserC, uuid));\n  }\n\n  void write_comment(const char *comment, size_t comment_len) {\n    printf(&quot;%s (&quot;, name.c_str());\n    for (size_t i = 0; i &lt; sizeof(uuid); i += 1) {\n      printf(&quot;%x&quot;, uuid[i]);\n    }\n    printf(&quot;) says: %.*s\\n&quot;, (int)comment_len, comment);\n    comments_count += 1;\n  }\n\n  uint64_t get_comment_count() { return comments_count; }\n\n  const std::string &amp;get_name() { return name; }\n};\n\nint main() {\n  User alice{&quot;alice&quot;};\n  const char msg[] = &quot;hello, world!&quot;;\n  alice.write_comment(msg, sizeof(msg) - 1);\n\n  printf(&quot;Comment count: %lu\\n&quot;, alice.get_comment_count());\n\n  RUST_write_comment(reinterpret_cast&lt;UserC *&gt;(&amp;alice),\n                     reinterpret_cast&lt;const uint8_t *&gt;(msg), sizeof(msg) - 1,\n                     get_std_string_pointer_and_length(alice.get_name()));\n  printf(&quot;Comment count: %lu\\n&quot;, alice.get_comment_count());\n} // Path: user-rs-lib.h\n\n#include &lt;cstdarg&gt;\n#include &lt;cstdint&gt;\n#include &lt;cstdlib&gt;\n#include &lt;ostream&gt;\n#include &lt;new&gt;\n\nstruct UserC {\n  uint8_t name[32];\n  uint64_t comments_count;\n  uint8_t uuid[16];\n};\n\nstruct ByteSliceView {\n  const uint8_t *ptr;\n  uintptr_t len;\n};\n\nextern &quot;C&quot; {\n\nvoid RUST_write_comment(UserC *user,\n                        const uint8_t *comment,\n                        uintptr_t comment_len,\n                        ByteSliceView name);\n\n} // extern &quot;C&quot; // Path: user-rs-lib/src/lib.rs\n\n#[repr(C)]\npub struct UserC {\n    pub name: [u8; 32],\n    pub comments_count: u64,\n    pub uuid: [u8; 16],\n}\n\n#[repr(C)]\n// Akin to `&amp;[u8]`, for C.\npub struct ByteSliceView {\n    pub ptr: *const u8,\n    pub len: usize,\n}\n\n#[no_mangle]\npub extern &quot;C&quot; fn RUST_write_comment(\n    user: &amp;mut UserC,\n    comment: *const u8,\n    comment_len: usize,\n    name: ByteSliceView,\n) {\n    let comment = unsafe { std::slice::from_raw_parts(comment, comment_len) };\n    let comment_str = unsafe { std::str::from_utf8_unchecked(comment) };\n\n    let name_slice = unsafe { std::slice::from_raw_parts(name.ptr, name.len) };\n    let name_str = unsafe { std::str::from_utf8_unchecked(name_slice) };\n\n    println!(\n        &quot;{} ({:x?}) says: {}&quot;,\n        name_str,\n        user.uuid.as_slice(),\n        comment_str\n    );\n\n    user.comments_count += 1;\n} ",
titles:[
{
title:"The trick",
slug:"the-trick",
offset:523,
},
{
title:"Example",
slug:"example",
offset:1548,
},
{
title:"Accessing std::string from Rust",
slug:"accessing-std-string-from-rust",
offset:9006,
},
{
title:"Improving the std::string situation",
slug:"improving-the-std-string-situation",
offset:11670,
},
{
title:"Conclusion",
slug:"conclusion",
offset:12810,
},
{
title:"Addendum: the full code",
slug:"addendum-the-full-code",
offset:14516,
},
],
},
{
html_file_name:"tip_of_day_1.html",
title:"Tip of the day #1: Count lines of Rust code, ignoring tests",
text:"I have a Rust codebase at work. The other day, I was wondering how many lines of code were in there. Whether you use wc -l ***.rs or a more fancy tool like tokei , there is an issue: this will count the source code as well as tests. That\'s because in Rust and in some other languages, people write their tests in the same files as the implementation. Typically it looks like that: // src/foo.rs\n\nfn foo() { \n ...\n}\n\n#[cfg(test)]\nmod tests {\n    fn test_foo(){\n      ...\n    }\n\n    ...\n} But I only want to know how big is the implementation. I don\'t care about the tests. And wc or tokei will not show me that. So I resorted to my trusty awk . Let\'s first count all lines, like wc does: $ awk \'{count += 1} END{print(count)}\' src/***.rs\n# Equivalent to:\n$ wc -l src/***/.rs On my open-source Rust project , this prints 11485 . Alright, now let\'s exclude the tests. When we encounter the line mod tests , we stop counting. Note that this name is just a convention, but that\'s one that followed pretty much universally in Rust code, and there is usually no more code after this section. Tweak the name if needed: $ awk \'/mod tests/{skip[FILENAME]=1}  !skip[FILENAME]{count += 1} END{print(count)}\'  src/***.rs And this prints in the same project: 10057 . Let\'s unpack it: We maintain a hashtable called skip which is a mapping of the file name to whether or not we should skip the rest of this file. In AWK we do not need to initialize variables, we can use them right away and they are zero initialized. AWK also automatically stores the name of the current file in the global builtin variable FILENAME . /mod tests/ : this pattern matches the line containing mod tests . The action for this line is to flag this file as \'skipped\', by setting the value in the map for this file to 1 (i.e. true ). !skip[FILENAME]{count += 1} : If this line for the current file is not flagged as \'skipped\', we increment for each line, the global counter. Most people think that AWK can only use patterns as clauses before the action, but in fact it also supports boolean conditions, and both can be use together, e.g.: /foo/ &amp;&amp; !skip[FILENAME] {print(&quot;hello&quot;)} END{print(count)} : we print the count at the very end. And that\'s it. AWK is always very nifty. Addendum: exit Originally I implemented it wrongly, like this: $ awk \'/mod tests/{exit 0} {count += 1} END{print(count)}\'  src/***.rs If we encounter tests, stop processing the file altogether, with the builtin statement exit ( docs ). Running this on the same Rust codebase prints: 1038 which is obviously wrong. Why is it wrong then? Well, as I understand it, AWK processes all inputs files one by one, as if it was one big sequential file (it will still fill the builtin constant FILENAME though, that\'s why the solution above works). Since there is no isolation between the processing each file (AWK does not spawn a subprocess for each file), it means we simply stop altogether at the first encountered test in any file. ",
titles:[
{
title:"Addendum: exit",
slug:"addendum-exit",
offset:2258,
},
],
},
{
html_file_name:"tip_of_the_day_2.html",
title:"Tip of the day #2: A safer arena allocator",
text:"Discussions: /r/programming , /r/cprogramming The most transformative action you can do to dramatically improve your code in a programming language where you are in control of the memory is: to use arenas. Much has been written about arenas ( 1 , 2 ). In short, it means grouping multiple allocations with the same lifetime in one batch that gets allocated and deallocated only once. Another way to look at it, is that the allocations are append only. They never get freed during their \'life\'. The program is split into \'phases\'. Typically, each phase has its own arena, and when it reaches its end, the whole arena gets nuked from space along with all entities allocated from it. It\'s a great way to simplify the code, make it faster, and escape from the \'web of pointers\' hell. The standard arena A typical arena looks like that: #include &lt;stdint.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;sys/mman.h&gt;\n#include &lt;unistd.h&gt;\n\ntypedef struct {\n  uint8_t *start;\n  uint8_t *end;\n} Arena;\n\nstatic Arena arena_make_from_virtual_mem(uint64_t size) {\n  uint8_t *alloc = mmap(nullptr, size, PROT_READ | PROT_WRITE,\n                   MAP_ANON | MAP_PRIVATE, -1, 0);\n  return (Arena){.start = alloc, .end = alloc + size};\n}\n\nstatic void *\narena_alloc(Arena *a, uint64_t size, uint64_t align, uint64_t count) {\n  const uint64_t padding = (-(uint64_t)a-&gt;start &amp; (align - 1));\n  const int64_t available = (int64_t)a-&gt;end - (int64_t)a-&gt;start - (int64_t)padding;\n\n  void *res = a-&gt;start + padding;\n\n  a-&gt;start += padding + count * size;\n\n  return memset(res, 0, count * size);\n}\n\nint main() {\n  Arena a = arena_make_from_virtual_mem(4096);\n} Very simple, just ask the OS to give us a region of virtual memory and off we go (on Windows, the system call is named differently but is equivalent). The bug Now, since we use a system call directly, sanitizers and runtime checks from the libc allocator do not apply, since we bypass them completely. In a way, it is also a feature: it means that our program will behave exactly the same on all OSes, have the exact same memory layout, and use the exact same amount of memory. It does not depend on the libc or allocator. So it turns out that I had a bug in my code: I allocated an array from the arena, and then accidentally wrote past the bounds of my array (so far, this sounds like a typical story from the C trenches). Normally, this would likely (depending on a few factors, like where in the arena was this allocation located, how big was it, and by how many bytes did the write go past the bounds, etc) write past the memory page that the OS gave us, thus triggering a SIGSEGV . However, in that instance, I got unlucky, because my code actually did something like that: int main() {\n  Arena a = arena_make_from_virtual_mem(4096);\n  Arena b = arena_make_from_virtual_mem(4096);\n\n  // Simulate writing past the arena:\n  a.start + 5000 = 42;\n} And...the program did not crash. The symptoms were very weird: data was subtly wrong in another place of the program, thus making it very difficult to troubleshoot. That\'s basically the nightmare scenario for any engineer. A crash would be so much easier. But why? Well, we basically asked the OS to give us one page of virtual memory when creating the first arena. Right after, we asked for a second page. And more often than not, the OS gives us a page right after the first page. So from the OS perspective, we allocated 2 * 4096 = 8192 bytes, and wrote in the middle, so all is good. We wanted to write into the first arena but instead wrote into the second one accidentally. This behavior is however not consistent, running the programs many times will sometimes crash and sometimes not. It all depends if the memory pages for the different arenas are contiguous or not. The solution So how do we fix it? What I did was defense in depth: Add asserts everywhere I could to check pre- and post-conditions. I believe that\'s how I discovered the bug in the first place, when one assert failed, even though it seemed impossible. Replace all direct array and pointer accesses with macros that check bounds (like most modern programming languages) Tweak how the arena is created to make it safer. That\'s our tip of the day, so let\'s see it. The idea is not new, most allocators do so in \'hardening\' mode: when the arena is created, we place a \'guard page\' right before and after the real allocation. We mark these guard pages as neither readable nor writable, so any access will trigger a SIGSEGV , even though that\'s memory owned by our program. That way, going slightly past the bounds of the real allocation in either direction, will result in a crash that\'s easy to diagnose. Note that this is a trade-off: It will not catch all out-of-bounds accesses. We could get unlucky and accidentally hit the memory of another arena still. This is a protection that typically helps with off-by-one errors. It\'s very lightweight: the OS only has to maintain an entry in a table, recording that the program owns the two additional pages (per arena). No actually physical memory will be dedicated for them. But, if there are millions of arenas, it could make a difference. It\'s theoretically tunable: nothing prevents us from having larger guard \'regions\'. If we are paranoid, we could make the guard region 64 GiB before and after the real allocation of 4096 bytes, if we wish. That\'s the power of virtual memory. The granularity is still the page (typically 4096 bytes, something larger). We cannot easily prevent out-of-bounds accesses within a page. The original implementation at the beginning of the article did not have to bother with the size of a page. But this implementation has to, which slightly complicates the logic (but not by much). So here it is: static Arena arena_make_from_virtual_mem(uint64_t size) {\n  uint64_t page_size = (uint64_t)sysconf(_SC_PAGE_SIZE);\n  uint64_t alloc_real_size = round_up_multiple_of(size, page_size);\n\n  // Page guard before + after.\n  uint64_t mmap_size = alloc_real_size + 2 * page_size;\n\n  uint8_t *alloc = mmap(nullptr, mmap_size, PROT_READ | PROT_WRITE,\n                   MAP_ANON | MAP_PRIVATE, -1, 0);\n\n  uint64_t page_guard_before = (uint64_t)alloc;\n\n  alloc += page_size;\n  uint64_t page_guard_after = (uint64_t)alloc + alloc_real_size;\n\n  mprotect((void *)page_guard_before, page_size, PROT_NONE);\n  mprotect((void *)page_guard_after, page_size, PROT_NONE);\n\n  return (Arena){.start = alloc, .end = alloc + size};\n} We get the page size with POSIX\'s sysconf (3) . Again, that\'s required because we will use the system call mprotect to change the permissions on parts of the memory, and mprotect expects a page-aligned memory range. Since an allocation is at least one page, even if the user asked for an arena of size 1 , we first round the user allocation size up, to the next page size. E.g. for a page size of 4096 : 1 -&gt; 4096 , 4095 -&gt; 4096 , 4096 -&gt; 4096 , 4097 -&gt; 8192 . Then, in one mmap call, we allocate all the memory we need including the two guard pages. For a brief moment, all the memory is readable and writable. The very next thing we do is mark the first page and last page as neither readable nor writable. We then return the arena, and the user is none the wiser. Wouldn\'t it be simpler to issue 3 mmap calls with the right permissions from the get go? Well, yes, but there is no guarantee that the OS would give us a contiguous region of memory across these 3 calls. On Linux, we can give hints, but still there is no guarantee. Remember, our program is one of many running concurrently, and could get interrupted for some time between these mmap calls, the whole OS could go to sleep, etc. What we want is an atomic operation, thus, one mmap call. Note, we can alternatively create the whole allocation as PROT_NONE and then mark the real (user-visible) allocation as PROT_READ | PROT_WRITE , that also works. So that\'s it, a poor man Adress Sanitizer in a few lines of code. Variations The paranoid approach If we are really paranoid, we could change how the arena works, to make every allocation get a new, separate page from the OS. That means that creating the arena would do nothing, and allocating from the arena would do the real allocation. This approach is, to me, indistinguishable from a general purpose allocator a la malloc from libc, just one that\'s very naive, and probably much slower. But, if there is a pesky out-of-bound bug pestering you, that could be worth trying. The bucket per type approach On Apple platforms , the libc allocator has a hardening mode that can be enabled at compile time. It stems from the realization that many security vulnerabilities rely on type confusion: The program thinks it is handling an entity of type X , but due to a logic bug, or an attacker\'s meddling, or the allocator reusing freshly freed memory from another place in the program, it is actually of another type Y . This results in an entity being in an \'impossible\' state which is great for an attacker. Also, reusing a previously allocated-then-freed object with a different type, without zero-initializing it, can leak secrets or information about the state of the program, to an attacker. There\'s a whole class of attacks where the first step is to make the program allocate and free objects many times, of an attacker controlled size, so that the heap is in the right \'shape\', with a high statistical chance. Meaning, a few targeted objects are next to each other in the heap, for the attack to occur. So, the mitigation is to place all allocations of the same type in one bucket (supposedly, it\'s a separate memory region with guard pages before and after). When an object of type X is allocated, then freed, and then the program allocates an object of type Y , of roughly the same size, a typical allocator will reuse the memory of X . This Apple allocator would give memory from a separate bucket, from a completely different memory region. What I don\'t know, is whether or not there are runtime checks as well, for example when casting one object from one type to another e.g. from X to void* , back to X , with reinterpret_cast in C++. It seems that this allocator would have the information needed at runtime to do so, which could be an interesting feature. Now, having one bucket per type turns out to be too slow in reality, and consumes too much memory, according to Apple developers, so this allocator groups a handful a different types in one bucket. This is a typical trade-off between performance and security. Still, this is an interesting approach, and could be implemented in our context by having one arena store all entities of one type, i.e. one arena is one bucket. See also Astute readers have also mentioned: using canaries in the available space in the arena to detect illegal accesses, putting the real data at the start or end of the page to catch out-of-bounds accesses respectively before and after the allocation, periodic checks for long-running applications, randomizing where the guard pages are placed relative to the allocation, running the tests a number of times to catch inconsistent behavior, and finally, teaching Address Sanitizer to be aware of our custom arena allocator so that it does these checks for us. That\'s super cool! See the linked discussions at the start. I wrote in the past about adding memory profiling to an arena allocator: Roll your own memory profiling: it\'s actually not hard . ",
titles:[
{
title:"The standard arena",
slug:"the-standard-arena",
offset:780,
},
{
title:"The bug",
slug:"the-bug",
offset:1811,
},
{
title:"The solution",
slug:"the-solution",
offset:3787,
},
{
title:"Variations",
slug:"variations",
offset:7967,
},
{
title:"The paranoid approach",
slug:"the-paranoid-approach",
offset:7978,
},
{
title:"The bucket per type approach",
slug:"the-bucket-per-type-approach",
offset:8478,
},
{
title:"See also",
slug:"see-also",
offset:10692,
},
],
},
{
html_file_name:"lessons_learned_from_a_successful_rust_rewrite.html",
title:"Lessons learned from a successful Rust rewrite",
text:"Discussions: /r/rust , /r/programming , HN , lobsters I have written about my on-going rewrite-it-to-Rust effort at work: 1 , 2 , 3 . And now it\'s finished, meaning it\'s 100% Rust and 0% C++ - the public C API has not changed, just the implementation, one function at time until the end. Let\'s have a look back at what worked, what didn\'t, and what can be done about it. For context, I have written projects in pure Rust before, so I won\'t mention all of the usual Rust complaints, like &quot;learning it is hard&quot;, they did not affect me during this project. What worked well The rewrite was done incrementally, in a stop-and-go fashion. At some point, as I expected, we had to add brand new features while the rewrite was on-going and that was very smooth with this approach. Contrast this with the (wrong) approach of starting a new codebase from scratch in parallel, and then the feature has to be implemented twice. The new code is much, much simpler and easier to reason about. It is roughly the same number of lines of code as the old C++ codebase, or slightly more. Some people think that equivalent Rust code will be much shorter (I have heard ratios of 1/2 or 2/3), but in my experience, it\'s not really the case. C++ can be incredibly verbose in some instances, but Rust as well. And the C++ code will often ignore some errors that the Rust compiler forces the developer to handle, which is a good thing, but also makes the codebase slightly bigger. Undergoing a rewrite, even a bug-for-bug one like ours, opens many new doors in terms of performance. For example, some fields in C++ were assumed to be of a dynamic size, but we realized that they were always 16 bytes according to business rules, so we stored them in an array of a fixed size, thus simplifying lots of code and reducing heap allocations. That\'s not strictly due to Rust, it\'s just that having this holistic view of the codebase yields many benefits. Related to this: we delete lots and lots of dead code. I estimate that we removed perhaps a third or half of the whole C++ codebase because it was simply never used. Some of it were half-assed features some long-gone customer asked for, and some were simply never run or even worse, never even built (they were C++ files not even present in the CMake build system). I feel that modern programming languages such as Rust or Go are much more aggressive at flagging dead code and pestering the developer about it, which again, is a good thing. We don\'t have to worry about out-of-bounds accesses and overflow/underflows with arithmetic. These were the main issues in the C++ code. Even if C++ containers have this .at() method to do bounds check, in my experience, most people do not use them. It\'s nice that this happens by default. And overflows/underflows checks are typically never addressed in C and C++ codebases. Cross-compilation is pretty smooth, although not always, see next section. The builtin test framework in Rust is very serviceable. All the ones I used in C++ were terrible and took so much time to even compile. Rust is much more concerned with correctness than C++, so it sparked a lot of useful discussions. For example: oh, the Rust compiler is forcing me to check if this byte array is valid UTF8 when I try to convert it to a string. The old C++ code did no such check. Let\'s add this check. It felt so good to remove all the CMake files. On all the C or C++ projects I worked on, I never felt that CMake was worth it and I always lost a lot of hours to coerce it into doing what I needed. What did not work so well This section is surprisingly long and is the most interesting in my opinion. Did Rust hold its promises? I am still chasing Undefined Behavior Doing an incremental rewrite from C/C++ to Rust, we had to use a lot of raw pointers and unsafe{} blocks. And even when segregating these to the entry point of the library, they proved to be a big pain in the neck. All the stringent rules of Rust still apply inside these blocks but the compiler just stops checking them for you, so you are on your own. As such, it\'s so easy to introduce Undefined Behavior. I honestly think from this experience that it is easier to inadvertently introduce Undefined Behavior in Rust than in C++, and it turn, it\'s easier in C++ than in C. The main rule in Rust is: multiple read-only pointers XOR one mutable pointer multiple read-only reference XOR one mutable reference . That\'s what the borrow checker is always pestering you about. But when using raw pointers, it\'s so easy to silently break, especially when porting C or C++ code as-is, which is mutation and pointer heavy: Note: Astute readers have pointed out that the issue in the snippet below is having multiple mutable references, not pointers, and that using the syntax let a = &amp;raw mut x; in recent Rust versions, or addr_of_mut in older versions, avoids creating multiple mutable references. fn main() {\n    let mut x = 1;\n    unsafe {\n        let a: *mut usize = &amp;mut x;\n        let b: *mut usize = &amp;mut x;\n\n        *a = 2;\n        *b = 3;\n    }\n} You might think that this code is dumb and obviously wrong, but in a big real codebase, this is not so easy to spot, especially when these operations are hidden inside helper functions or layers and layers of abstraction, as Rust loves to do. cargo run is perfectly content with the code above. The Rust compiler can and will silently assume that there is only one mutable pointer to x , and make optimizations, and generate machine code, based on that assumption, which this code breaks. The only savior here is Miri : $ cargo +nightly-2024-09-01 miri r\nerror: Undefined Behavior: attempting a write access using &lt;2883&gt; at alloc1335[0x0], but that tag does not exist in the borrow stack for this location\n --&gt; src/main.rs:7:9\n  |\n7 |         *a = 2;\n  |         ^^^^^^\n  |         |\n  |         attempting a write access using &lt;2883&gt; at alloc1335[0x0], but that tag does not exist in the borrow stack for this location\n  |         this error occurs as part of an access at alloc1335[0x0..0x8]\n  |\n  [...]\n --&gt; src/main.rs:4:29\n  |\n4 |         let a: *mut usize = &amp;mut x;\n  |                             ^^^^^^\nhelp: &lt;2883&gt; was later invalidated at offsets [0x0..0x8] by a Unique retag\n --&gt; src/main.rs:5:29\n  |\n5 |         let b: *mut usize = &amp;mut x;\n  |                             ^^^^^^\n  [...] So, what could have been a compile time error, is now a runtime error. Great. I hope you have 100% test coverage! Thank god there\'s Miri. If you are writing unsafe{} code without Miri checking it, or if you do so without absolutely having to, I think this is foolish. It will blow up in your face. Miri is awesome. But... Miri does not always work and I still have to use Valgrind I am not talking about some parts of Miri that are experimental. Or the fact that running code under Miri is excruciatingly slow. Or the fact that Miri only works in nightly . No, I am talking about code that Miri cannot run, period: |\n471 |     let pkey_ctx = LcPtr::new(unsafe { EVP_PKEY_CTX_new_id(EVP_PKEY_EC, null_mut()) })?;\n    |                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ can\'t call foreign function `\u{2401}aws_lc_0_16_0_EVP_PKEY_CTX_new_id` on OS `linux`\n    |\n    = help: if this is a basic API commonly used on this target, please report an issue with Miri\n    = help: however, note that Miri does not aim to support every FFI function out there; for instance, we will not support APIs for things such as GUIs, scripting languages, or databases If you are using a library that has parts written in C or assembly, which is usual for cryptography libraries, or video compression, etc, you are out of luck. So we resorted to add a feature flag to split the codebase between parts that use this problematic library and parts that don\'t. And Miri only runs tests with the feature disabled. That means that there is a lot of unsafe code that is simply not being checked right now. Bummer. Perhaps there could be a fallback implementation for these libraries that\'s entirely implemented in software (and in pure Rust). But that\'s not really feasible for most libraries to maintain two implementations just for Rust developers. I resorted to run the problematic tests in valgrind , like I used to do with pure C/C++ code. It does not detect many things that Miri would, for example having more than one mutable pointer to the same value, which is perfectly fine in C/C++/Assembly, but not in Rust. I am still chasing memory leaks Our library offers a C API, something like this: void* handle = MYLIB_init();\n\n// Do some stuff with the handle...\n\nMYLIB_release(handle); Under the hood, MYLIB_init allocates some memory and MYLIB_release() frees it. This is a very usual pattern in C libraries, e.g. curl_easy_init()/curl_easy_cleanup() . So immediately, you are thinking: well, it\'s easy to forget to call MYLIB_release in some code paths, and thus leak memory. And you\'d be right. So let\'s implement them to illustrate. We are good principled developers so we write a Rust test: #[no_mangle]\npub extern &quot;C&quot; fn MYLIB_init() -&gt; *mut std::ffi::c_void {\n    let alloc = Box::leak(Box::new(1usize));\n\n    alloc as *mut usize as *mut std::ffi::c_void\n}\n\n#[no_mangle]\npub extern &quot;C&quot; fn MYLIB_do_stuff(_handle: *mut std::ffi::c_void) {\n    // Do some stuff.\n}\n\n#[no_mangle]\npub extern &quot;C&quot; fn MYLIB_release(handle: *mut std::ffi::c_void) {\n    let _ = unsafe { Box::from_raw(handle as *mut usize) };\n}\n\nfn main() {}\n\n#[cfg(test)]\nmod test {\n    #[test]\n    fn test_init_release() {\n        let x = super::MYLIB_init();\n\n        super::MYLIB_do_stuff(x);\n\n        super::MYLIB_release(x);\n    }\n} A Rust developer first instinct would be to use RAII by creating a wrapper object which implements Drop and automatically calls the cleanup function.\nHowever, we wanted to write our tests using the public C API of the library like a normal C application would, and it would not have access to this Rust feature.\nAlso, it can become unwieldy when there are tens of types that have an allocation/deallocation function. It\'s a lot of boilerplate! And often, there is complicated logic with lots of code paths, and we need to ensure that the cleanup is always called. In C, this is typically done with goto to an end: label that always cleans up the resources. But Rust does not support this form of goto . So we solved it with the defer crate in Rust and implementing a defer statement in C++. However, the Rust borrow checker really does not like the defer pattern. Typically, a cleanup function will take as its argument as &amp;mut reference and that precludes the rest of the code to also store and use a second &amp;mut reference to the same value. So we could not always use defer on the Rust side. Cross-compilation does not always work Same issue as with Miri, using libraries with a Rust API but with parts implemented in C or Assembly will make cargo build --target=... not work out of the box. It won\'t affect everyone out there, and perhaps it can be worked around by providing a sysroot like in C or C++. But that\'s a bummer still. For example, I think Zig manages this situation smoothly for most targets, since it ships with a C compiler and standard library, whereas cargo does not. Cbindgen does not always work cbindgen is a conventionally used tool to generate a C header from a Rust codebase. It mostly works, until it does not. I hit quite a number of limitations or bugs. I thought of contributing PRs, but I found for most of these issues, a stale open PR, so I didn\'t. Every time, I thought of dumping cbindgen and writing all of the C prototypes by hand. I think it would have been simpler in the end. Again, as a comparison, I believe Zig has a builtin C header generation tool. Unstable ABI I talked about this point in my previous articles so I won\'t be too long. Basically, all the useful standard library types such as Option have no stable ABI, so they have to be replicated manually with the repr(C) annotation, so that they can be used from C or C++. This again is a bummer and creates friction. Note that I am equally annoyed at C++ ABI issues for the same reason. Many, many hours of hair pulling would be avoided if Rust and C++ adopted, like C, a stable ABI . No support for custom memory allocators With lots of C libraries, the user can provide its own allocator at runtime, which is often very useful. In Rust, the developer can only pick the global allocator at compile time. So we did not attempt to offer this feature in the library API. Additionally, all of the aforementioned issues about cleaning up resources would have been instantly fixed by using an arena allocator , which is not at all idiomatic in Rust and does not integrate with the standard library (even though there are crates for it). Again, Zig and Odin all support arenas natively, and it\'s trivial to implement and use them in C. I really longed for an arena while chasing subtle memory leaks. Complexity From the start, I decided I would not touch async Rust with a ten-foot pole, and I did not miss it at all, for this project. Whilst reading the docs for UnsafeCell for the fourth time, and pondering whether I should use that or RefCell , while just having been burnt by the pitfalls of MaybeUninit , and asking myself if I need Pin , I really asked myself what life choices had led me to this. Pure Rust is already very complex, but add to it the whole layer that is mainly there to deal with FFI, and it really becomes a beast. Especially for new Rust learners. Some developers in our team straight declined to work on this codebase, mentioning the real or perceived Rust complexity.\nNow, I think that Rust is still mostly easier to learn than C++, but admittedly not by much, especially in this FFI heavy context. Conclusion I am mostly satisfied with this Rust rewrite, but I was disappointed in some areas, and it overall took much more effort than I anticipated. Using Rust with a lot of C interop feels like using a completely different language than using pure Rust. There is much friction, many pitfalls, and many issues in C++, that Rust claims to have solved, that are in fact not really solved at all. I am deeply grateful to the developers of Rust, Miri, cbindgen, etc. They have done tremendous work. Still, the language and tooling, when doing lots of C FFI, feel immature, almost pre v1.0. If the ergonomics of unsafe (which are being worked and slightly improved in the recent versions), the standard library, the docs, the tooling, and the unstable ABI, all improve in the future, it could become a more pleasant experience. I think that all of these points have been felt by Microsoft and Google, and that\'s why they are investing real money in this area to improve things. If you do not yet know Rust, I recommend for your first project to use pure Rust, and stay far away from the whole FFI topic. I initially considered using Zig or Odin for this rewrite, but I really did not want to use a pre v1.0 language for an enterprise production codebase (and I anticipated that it would be hard to convince other engineers and managers). Now, I am wondering if the experience would have really been worse than with Rust. Perhaps the Rust model is really at odds with the C model (or with the C++ model for that matter) and there is simply too much friction when using both together. If I have to undertake a similar effort in the future, I think I would strongly consider going with Zig instead. We\'ll see. In any case, the next time someone say \'just rewrite it in Rust\', point them to this article, and ask them if that changed their mind ;) ",
titles:[
{
title:"What worked well",
slug:"what-worked-well",
offset:564,
},
{
title:"What did not work so well",
slug:"what-did-not-work-so-well",
offset:3544,
},
{
title:"I am still chasing Undefined Behavior",
slug:"i-am-still-chasing-undefined-behavior",
offset:3675,
},
{
title:"Miri does not always work and I still have to use Valgrind",
slug:"miri-does-not-always-work-and-i-still-have-to-use-valgrind",
offset:6730,
},
{
title:"I am still chasing memory leaks",
slug:"i-am-still-chasing-memory-leaks",
offset:8528,
},
{
title:"Cross-compilation does not always work",
slug:"cross-compilation-does-not-always-work",
offset:10852,
},
{
title:"Cbindgen does not always work",
slug:"cbindgen-does-not-always-work",
offset:11346,
},
{
title:"Unstable ABI",
slug:"unstable-abi",
offset:11852,
},
{
title:"No support for custom memory allocators",
slug:"no-support-for-custom-memory-allocators",
offset:12344,
},
{
title:"Complexity",
slug:"complexity",
offset:13053,
},
{
title:"Conclusion",
slug:"conclusion",
offset:13880,
},
],
},
{
html_file_name:"tip_of_day_3.html",
title:"Tip of the day #3: Convert a CSV to a markdown or HTML table",
text:"The other day at work, I found myself having to produce a human-readable table of all the direct dependencies in the project, for auditing purposes. There is a tool for Rust projects that outputs a TSV (meaning: a CSV where the separator is the tab character) of this data. That\'s great, but not really fit for consumption by a non-technical person. I just need to convert that to a human readable table in markdown or HTML, and voila! Here\'s the output of this tool in my open-source Rust project : $ cargo license --all-features --avoid-build-deps --avoid-dev-deps --direct-deps-only --tsv\nname\tversion\tauthors\trepository\tlicense\tlicense_file\tdescription\nclap\t2.33.0\tKevin K. &lt;kbknapp@gmail.com&gt;\thttps://github.com/clap-rs/clap\tMIT\t\tA simple to use, efficient, and full-featured Command Line Argument Parser\nheck\t0.3.1\tWithout Boats &lt;woboats@gmail.com&gt;\thttps://github.com/withoutboats/heck\tApache-2.0 OR MIT\t\theck is a case conversion library.\nkotlin\t0.1.0\tPhilippe Gaultier &lt;philigaultier@gmail.com&gt;\t\t\t\t\nlog\t0.4.8\tThe Rust Project Developers\thttps://github.com/rust-lang/log\tApache-2.0 OR MIT\t\tA lightweight logging facade for Rust\npretty_env_logger\t0.3.1\tSean McArthur &lt;sean@seanmonstar&gt;\thttps://github.com/seanmonstar/pretty-env-logger\tApache-2.0 OR MIT\t\ta visually pretty env_logger\ntermcolor\t1.1.0\tAndrew Gallant &lt;jamslam@gmail.com&gt;\thttps://github.com/BurntSushi/termcolor\tMIT OR Unlicense\t\tA simple cross platform library for writing colored text to a terminal. Not really readable. We need to transform this data into a markdown table , something like that: | First Header  | Second Header |\n| ------------- | ------------- |\n| Content Cell  | Content Cell  |\n| Content Cell  | Content Cell  | Technically, markdown tables are an extension to standard markdown (if there is such a thing), but they are very common and supported by all the major platforms e.g. Github, Azure, etc. So how do we do that? Once again, I turn to the trusty AWK. It\'s always been there for me. And it\'s present on every UNIX system out of the box. AWK neatly handles all the \'decoding\' of the CSV format for us, we just need to output the right thing: Given a line (which AWK calls \'record\'): output each field interleaved with the | character Output a delimiting line between the table headers and rows. The markdown table spec states that this delimiter should be at least 3 - characters in each cell. Alignment is not a goal, it does not matter for a markdown parser. If you want to produce a pretty markdown table, it\'s easy to achieve, it simply makes the implementation a bit bigger Here\'s the full implementation (don\'t forget to mark the file executable). The shebang line instructs AWK to use the tab character \\t as the delimiter between fields: #!/usr/bin/env -S awk -F \'\\t\' -f\n\n{\n    printf(&quot;|&quot;);\n    for (i = 1; i &lt;= NF; i++) {\n        # Note: if a field contains the character `|`,\n        # it will mess up the table.\n        # In this case, we should replace this character\n        # by something else e.g. `,`:\n        gsub(/\\|/, &quot;,&quot;, $i);\n        printf(&quot; %s |&quot;, $i);\n    } \n    printf(&quot;\\n&quot;);\n} \n\nNR==1 { # Output the delimiting line\n    printf(&quot;|&quot;);\n    for(i = 1; i &lt;= NF; i++) {\n        printf(&quot; --- | &quot;);\n    }\n    printf(&quot;\\n&quot;);\n} The first clause will execute for each line of the input.\nThe for loop then iterates over each field and outputs the right thing. The second clause will execute only for the first line ( NR is the line number). The same line can trigger multiple clauses, here, the first line of the input will trigger both clauses, whilst the remaining lines will only trigger the first clause. So let\'s run it! $ cargo license --all-features --avoid-build-deps --avoid-dev-deps --direct-deps-only --tsv | ./md-table.awk \n| name | version | authors | repository | license | license_file | description |\n| --- |  --- |  --- |  --- |  --- |  --- |  --- | \n| clap | 2.33.0 | Kevin K. &lt;kbknapp@gmail.com&gt; | https://github.com/clap-rs/clap | MIT |  | A simple to use, efficient, and full-featured Command Line Argument Parser |\n| heck | 0.3.1 | Without Boats &lt;woboats@gmail.com&gt; | https://github.com/withoutboats/heck | Apache-2.0 OR MIT |  | heck is a case conversion library. |\n| kotlin | 0.1.0 | Philippe Gaultier &lt;philigaultier@gmail.com&gt; |  |  |  |  |\n| log | 0.4.8 | The Rust Project Developers | https://github.com/rust-lang/log | Apache-2.0 OR MIT |  | A lightweight logging facade for Rust |\n| pretty_env_logger | 0.3.1 | Sean McArthur &lt;sean@seanmonstar&gt; | https://github.com/seanmonstar/pretty-env-logger | Apache-2.0 OR MIT |  | a visually pretty env_logger |\n| termcolor | 1.1.0 | Andrew Gallant &lt;jamslam@gmail.com&gt; | https://github.com/BurntSushi/termcolor | MIT OR Unlicense |  | A simple cross platform library for writing colored text to a terminal. | Ok, it\'s hard to really know if that\'s correct or not. Let\'s pipe it into cmark-gfm to render this markdown table as HTML: $ cargo license --all-features --avoid-build-deps --avoid-dev-deps --direct-deps-only --tsv | ./md-table.awk | cmark-gfm -e table And voila: name version authors repository license license_file description clap 2.33.0 Kevin K. kbknapp@gmail.com https://github.com/clap-rs/clap MIT A simple to use, efficient, and full-featured Command Line Argument Parser heck 0.3.1 Without Boats woboats@gmail.com https://github.com/withoutboats/heck Apache-2.0 OR MIT heck is a case conversion library. kotlin 0.1.0 Philippe Gaultier philigaultier@gmail.com log 0.4.8 The Rust Project Developers https://github.com/rust-lang/log Apache-2.0 OR MIT A lightweight logging facade for Rust pretty_env_logger 0.3.1 Sean McArthur sean@seanmonstar https://github.com/seanmonstar/pretty-env-logger Apache-2.0 OR MIT a visually pretty env_logger termcolor 1.1.0 Andrew Gallant jamslam@gmail.com https://github.com/BurntSushi/termcolor MIT OR Unlicense A simple cross platform library for writing colored text to a terminal. All in all, very little code. I have a feeling that I will use this approach a lot in the future for reporting or even inspecting data easily, for example from a database dump. ",
titles:[
],
},
{
html_file_name:"perhaps_rust_needs_defer.html",
title:"Perhaps Rust needs \"defer\"",
text:"Or, how FFI in Rust is a pain in the neck. Discussions: /r/rust , /r/programming , HN , lobsters In a previous article I mentioned that we use the defer idiom in Rust through a crate, but that it actually rarely gets past the borrow checker. Some comments were claiming this issue does not exist surprised and I did not have an example at hand. Well, today at work I hit this issue again so I thought I would document it. And the whole experience showcases well how working in Rust with lots of FFI interop feels like. Setting the stage So, I have a Rust API like this: #[repr(C)]\npub struct Foo {\n    value: usize,\n}\n\n#[no_mangle]\npub extern &quot;C&quot; fn MYLIB_get_foos(out_foos: *mut *mut Foo, out_foos_count: &amp;mut usize) -&gt; i32 {\n    let res = vec![Foo { value: 42 }, Foo { value: 99 }];\n    *out_foos_count = res.len();\n    unsafe { *out_foos = res.leak().as_mut_ptr() };\n    0\n} It allocates and returns a dynamically allocated array as a pointer and a length. Of course in reality, Foo has many fields and the values are not known in advance but what happens is that we send messages to a Smartcard to ask it to send us a piece of data residing on it, and it replies with some encoded messages that our library decodes and returns to the user. I tell Cargo this is a static library: # Cargo.toml\n\n[lib]\ncrate-type = [&quot;staticlib&quot;] It\'s a straightforward API, so I generate the corresponding C header with cbindgen: $ cbindgen -v src/lib.rs --lang=c -o mylib.h And I get: #include &lt;stdarg.h&gt;\n#include &lt;stdbool.h&gt;\n#include &lt;stdint.h&gt;\n#include &lt;stdlib.h&gt;\n\ntypedef struct Foo {\n  uintptr_t value;\n} Foo;\n\nint32_t MYLIB_get_foos(struct Foo **out_foos, uintptr_t *out_foos_count); I can now use it from C so: #include &quot;mylib.h&quot;\n#include &lt;assert.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main() {\n  Foo *foos = NULL;\n  size_t foos_count = 0;\n  assert(0 == MYLIB_get_foos(&amp;foos, &amp;foos_count));\n\n  for (size_t i = 0; i &lt; foos_count; i++) {\n    printf(&quot;%lu\\n&quot;, foos[i].value);\n  }\n\n  if (NULL != foos) {\n    free(foos);\n  }\n} I build it with all the warnings enabled, run it with sanitizers on, and/or in Valgrind, all good. This code has a subtle mistake (can you spot it?), so keep on reading. If we feel fancy (and non-portable), we can even automate the freeing of the memory in C with __attribute(cleanup) , like defer (ominous sounds). But let\'s not, today. Let\'s focus on the Rust side. Now, we are principled developers who test their code (right?). So let\'s write a Rust test for it. We expect it to be exactly the same as the C code: #[cfg(test)]\nmod tests {\n    #[test]\n    fn test_get_foos() {\n        let mut foos = std::ptr::null_mut();\n        let mut foos_count = 0;\n        assert_eq!(0, super::MYLIB_get_foos(&amp;mut foos, &amp;mut foos_count));\n    }\n} And it passes: $ cargo test\n...\nrunning 1 test\ntest tests::test_get_foos ... ok\n... Of course, we have not yet freed anything, so we expect Miri to complain, and it does: $ cargo +nightly miri test\n...\nerror: memory leaked: alloc59029 (Rust heap, size: 16, align: 8), allocated here:\n... Note that the standard test runner does not report memory leaks, unfortunately. If Miri does not work for a given use case, and we still want to check that there are no leaks, we have to reach for nightly sanitizers or Valgrind. First attempt at freeing the memory properly Great, so let\'s free it at the end of the test, like C does, with free from libc, which we add as a dependency: #[test]\n    fn test_get_foos() {\n        ...\n\n        unsafe { libc::free(foos as *mut std::ffi::c_void) };\n    } The test passes, great. Let\'s try with Miri: $ cargo +nightly miri test\n...\n error: Undefined Behavior: deallocating alloc59029, which is Rust heap memory, using C heap deallocation operation\n... Hmm...ok...Well that\'s a bit weird, because what Rust does, when the Vec is allocated, is to call out to malloc from libc, as we can see with strace : $ strace -k -v -e brk ./a.out\n...\nbrk(0x213c0000)                         = 0x213c0000\n &gt; /usr/lib64/libc.so.6(brk+0xb) [0x10fa9b]\n &gt; /usr/lib64/libc.so.6(__sbrk+0x6b) [0x118cab]\n &gt; /usr/lib64/libc.so.6(__default_morecore@GLIBC_2.2.5+0x15) [0xa5325]\n &gt; /usr/lib64/libc.so.6(sysmalloc+0x57b) [0xa637b]\n &gt; /usr/lib64/libc.so.6(_int_malloc+0xd39) [0xa7399]\n &gt; /usr/lib64/libc.so.6(tcache_init.part.0+0x36) [0xa7676]\n &gt; /usr/lib64/libc.so.6(__libc_malloc+0x125) [0xa7ef5]\n &gt; /home/pg/scratch/rust-blog2/a.out(alloc::alloc::alloc+0x6a) [0x4a145a]\n &gt; /home/pg/scratch/rust-blog2/a.out(alloc::alloc::Global::alloc_impl+0x140) [0x4a15a0]\n &gt; /home/pg/scratch/rust-blog2/a.out(alloc::alloc::exchange_malloc+0x3a) [0x4a139a]\n &gt; /home/pg/scratch/rust-blog2/a.out(MYLIB_get_foos+0x26) [0x407cc6]\n &gt; /home/pg/scratch/rust-blog2/a.out(main+0x2b) [0x407bfb] Depending on your system, the call stack and specific system call may vary. It depends on the libc implementation, but point being, malloc from libc gets called by Rust. Note the irony that we do not need to have a third-party dependency on the libc crate to allocate with malloc (being called under the hood), but we do need it, in order to deallocate the memory with free . Perhaps it\'s by design. Anyway. Where was I. The docs for Vec indeed state: In general, Vec\u{2019}s allocation details are very subtle \u{2014} if you intend to allocate memory using a Vec and use it for something else (either to pass to unsafe code, or to build your own memory-backed collection), be sure to deallocate this memory by using from_raw_parts to recover the Vec and then dropping it. But a few sentences later it also says: That is, the reported capacity is completely accurate, and can be relied on. It can even be used to manually free the memory allocated by a Vec if desired. So now I am confused, am I allowed to free() the Vec \'s pointer directly or not? By the way, we also spot in the same docs that there was no way to correctly free the Vec by calling free() on the pointer without knowing the capacity because: The pointer will never be null, so this type is null-pointer-optimized. However, the pointer might not actually point to allocated memory. Hmm, ok... So I guess the only way to not trigger Undefined Behavior on the C side when freeing, would be to keep the capacity of the Vec around and do: if (capacity &gt; 0) {\n    free(foos);\n  } Let\'s ignore for now that this will surprise every C developer out there that have been doing if (NULL != ptr) free(ptr) for 50 years now. I also tried to investigate how drop is implemented for Vec to understand what\'s going on and I stopped at this function in core/src/alloc/mod.rs : unsafe fn deallocate(&amp;self, ptr: NonNull&lt;u8&gt;, layout: Layout); Not sure where the implementation is located... Ok, let\'s move on. Let\'s stay on the safe side and assume that we ought to use Vec::from_raw_parts and let the Vec free the memory when it gets dropped at the end of the scope. The only problem is: This function requires the pointer, the length, and the capacity . Wait, but we lost the capacity when we returned the pointer + length to the caller in MYLIB_get_foos() , and the caller does not care one bit about the capacity ! It\'s irrelevant to them! At work, the mobile developers using our library rightfully asked: wait, what is this cap field? Why do I care? What do I do with it? If you are used to manually managing your own memory, this is a very old concept, but if you are used to a Garbage Collector, it\'s very much new. Second attempt at freeing the memory properly So, let\'s first try to dodge the problem the hacky simple way by pretending that the memory is allocated by a Box , which only needs the pointer, just like free() : #[test]\n    fn test_get_foos() {\n        ...\n\n        unsafe {\n            let _ = Box::from_raw(foos);\n        }\n    } That\'s I think the first instinct for a C developer. Whatever way the memory was heap allocated, be it with malloc , calloc , realloc , be it for one struct or for a whole array, we want to free it with one call, passing it the base pointer. Let\'s ignore for a moment the docs that state that sometimes the pointer is heap-allocated and sometimes not. So this Rust code builds. The test passes. And Miri is unhappy. I guess you know the drill by now: $ cargo +nightly miri test\n...\n incorrect layout on deallocation: alloc59029 has size 16 and alignment 8, but gave size 8 and alignment 8\n... Let\'s take a second to marvel at the fact that Rust, probably the programming language the most strict at compile time, the if-it-builds-it-runs-dude-I-swear language, seems to work at compile time and at run time, but only fails when run under an experimental analyzer that only works in nightly and does not support lots of FFI patterns, which is the place where you need Miri the most! That\'s the power of Undefined Behavior and unsafe{} . Again: audit all of your unsafe blocks, and be very suspicious of any third-party code that uses unsafe . I think Rust developers on average do not realize the harm that it is very easy to inflict to your program by using unsafe unwisely even if everything seems fine. Anyways, I guess we have to refactor our whole C API to do it the Rust Way(tm)! Third attempt at freeing the memory properly So, in our codebase at work, we have defined this type: /// Owning Array i.e. `Vec&lt;T&gt;` in Rust or `std::vector&lt;T&gt;` in C++.\n#[repr(C)]\npub struct OwningArrayC&lt;T&gt; {\n    pub data: *mut T,\n    pub len: usize,\n    pub cap: usize,\n} It clearly signifies to the caller that they are in charge of freeing the memory, and also it carries the capacity of the Vec with it, so it\'s not lost. In our project, this struct is used a lot. We also define a struct for non owning arrays (slices), etc. So let\'s adapt the function, and also add a function in the API to free it for convenience: #[no_mangle]\npub extern &quot;C&quot; fn MYLIB_get_foos(out_foos: &amp;mut OwningArrayC&lt;Foo&gt;) -&gt; i32 {\n    let res = vec![Foo { value: 42 }, Foo { value: 99 }];\n    let len = res.len();\n    let cap = res.capacity();\n\n    *out_foos = OwningArrayC {\n        data: res.leak().as_mut_ptr(),\n        len,\n        cap,\n    };\n    0\n}\n\n#[no_mangle]\npub extern &quot;C&quot; fn MYLIB_free_foos(foos: &amp;mut OwningArrayC&lt;Foo&gt;) {\n    if foos.cap &gt; 0 {\n        unsafe {\n            let _ = Vec::from_raw_parts(foos.data, foos.len, foos.cap);\n        }\n    }\n} Let\'s also re-generate the C header, adapt the C code, rebuild it, communicate with the various projects that use our C API to make them adapt, etc... Back to the Rust test: #[cfg(test)]\nmod tests {\n    #[test]\n    fn test_get_foos() {\n        let mut foos = crate::OwningArrayC {\n            data: std::ptr::null_mut(),\n            len: 0,\n            cap: 0,\n        };\n        assert_eq!(0, super::MYLIB_get_foos(&amp;mut foos));\n        println!(&quot;foos: {}&quot;, foos.len);\n        super::MYLIB_free_foos(&amp;mut foos);\n    }\n} And now, Miri is happy. Urgh. So, back to what we set out to do originally, defer . Defer The test is trivial right now but in real code, there are many code paths that sometimes allocate, sometimes not, with validation interleaved, and early returns, so we\'d really like if we could statically demonstrate that the memory is always correctly freed. To ourselves, to auditors, etc. One example at work of such hairy code is: building a linked list (in Rust), fetching more from the network based on the content of the last node in the list, and appending the additional data to the linked list, until some flag is detected in the encoded data. Oh, and there is also validation of the incoming data, so you might have to return early with a partially constructed list which should be properly cleaned up. And there are many such examples like this, where the memory is often allocated/deallocated with a C API and it\'s not always possible to use RAII. So defer comes in handy. Let\'s use the scopeguard crate which provides a defer! macro, in the test, to automatically free the memory: #[test]\n    fn test_get_foos() {\n        let mut foos = crate::OwningArrayC {\n            data: std::ptr::null_mut(),\n            len: 0,\n            cap: 0,\n        };\n        assert_eq!(0, super::MYLIB_get_foos(&amp;mut foos));\n        defer! {\n            super::MYLIB_free_foos(&amp;mut foos);\n        }\n\n        println!(&quot;foos: {}&quot;, foos.len);\n    } And we get a compile error: $ cargo test\nerror[E0502]: cannot borrow `foos.len` as immutable because it is also borrowed as mutable\n  --&gt; src/lib.rs:54:30\n   |\n50 | /         defer! {\n51 | |             super::MYLIB_free_foos(&amp;mut foos);\n   | |                                         ---- first borrow occurs due to use of `foos` in closure\n52 | |         }\n   | |_________- mutable borrow occurs here\n53 |\n54 |           println!(&quot;foos: {}&quot;, foos.len);\n   |                                ^^^^^^^^ immutable borrow occurs here\n55 |       }\n   |       - mutable borrow might be used here, when `_guard` is dropped and runs the `Drop` code for type `ScopeGuard`\n   | Dum dum duuuum....Yes, we cannot use the defer idiom here (or at least I did not find a way). In some cases it\'s possible, in lots of cases it\'s not. The borrow checker considers that the defer block holds an exclusive mutable reference and the rest of the code cannot use that reference in any way. Despite the fact, that the version without defer, and with defer, are semantically equivalent and the borrow checker is fine with the former and not with the latter. Possible solutions So that is why I argue that Rust should get a defer statement in the language and the borrow checker should be made aware of this construct to allow this approach to take place. But what can we do otherwise? Are there any alternatives? We can be very careful and make sure we deallocate everything by hand in every code paths. Obviously that doesn\'t scale to team size, code complexity, etc. And it\'s unfortunate since using a defer-like approach in C with __attribute(cleanup) and in C++ by implementing our own defer is trivial. And even Go which is garbage-collected has a first-class defer . So not being able to do so in Rust is unfortunate. We can use a goto-like approach, as a reader suggested in a previous article, even though Rust does not have goto per se: fn foo_init() -&gt; *mut () { &amp;mut () }\nfn foo_bar(_: *mut ()) -&gt; bool { false }\nfn foo_baz(_: *mut ()) -&gt; bool { true }\nfn foo_free(_: *mut ()) {}\n\nfn main() {\n  let f = foo_init();\n  \n  \'free: {\n    if foo_bar(f) {\n        break \'free;\n    }\n    \n    if foo_baz(f) {\n        break \'free;\n    }\n    \n    // ...\n  };\n  \n  foo_free(f);\n} It\'s very nifty, but I am not sure I would enjoy reading and writing this kind of code, especially with multiple levels of nesting. Again, it does not scale very well. But it\'s something. We can work-around the borrow-checker to still use defer by refactoring our code to make it happy. Again, tedious and not always possible. One thing that possibly works is using handles (numerical ids) instead of pointers, so that they are Copy and the borrow checker does not see an issue with sharing/copying them. Like file descriptors work in Unix. The potential downside here is that it creates global state since some component has to bookkeep these handles and their mapping to the real pointer. But it\'s a common pattern in gamedev. Perhaps the borrow checker can be improved upon without adding defer to the language, \'just\' by making it smarter? We can use arenas everywhere and sail away in the sunset, leaving all these nasty problems behind us Rust can stabilize various nightly APIs and tools, like custom allocators and sanitizers, to make development simpler Conclusion Rust + FFI is nasty and has a lot of friction. I went at work through all these steps I went through in this article, and this happens a lot. The crux of the issue is that there is a lot of knowledge to keep in our heads, lots of easy ways to shoot ourselves in the foot, and we have to reconcile what various tools tell us: even if the compiler is happy, the tests might not be. Even the tests are happy, Miri might not be. Even if we think we have done the right thing, we discover later, buried deep in the docs, that in fact, we didn\'t. It\'s definitely for experts only. This should not be so hard! Won\'t somebody think of the children Rust FFI users? EDIT: It\'s been pointed out to me that there are two on-going internal discussions by the Rust developers about this topic to possibly reserve the defer keyword for future use and maybe one day add this facility to the language: 1 , 2 . Addendum: One more gotcha Rust guarantees that the underlying pointer in Vec is not null. And OwningArrayC mirrors Vec , so it should be the same, right? Well consider this C code: int main() {\n    OwningArrayC_Foo foos = {0};\n    if (some_condition) {\n         MYLIB_get_foos(&amp;foos);\n    }\n\n    // `foos.data` is null here in some code paths.\n    MYLIB_free_foos(&amp;foos);\n} In this case, MYLIB_free_foos actually can receive an argument with a null pointer (the data field), which would then trigger an assert inside Vec::from_raw_parts . So we should check that in MY_LIB_free_foos : #[no_mangle]\npub extern &quot;C&quot; fn MYLIB_free_foos(foos: &amp;mut OwningArrayC&lt;Foo&gt;) {\n    if !foos.data.is_null() {\n        unsafe {\n            let _ = Vec::from_raw_parts(foos.data, foos.len, foos.cap);\n        }\n    }\n} It might be a bit surprising to a pure Rust developer given the Vec guarantees, but since the C side could pass anything, we must be defensive. ",
titles:[
{
title:"Setting the stage",
slug:"setting-the-stage",
offset:519,
},
{
title:"First attempt at freeing the memory properly",
slug:"first-attempt-at-freeing-the-memory-properly",
offset:3384,
},
{
title:"Second attempt at freeing the memory properly",
slug:"second-attempt-at-freeing-the-memory-properly",
offset:7559,
},
{
title:"Third attempt at freeing the memory properly",
slug:"third-attempt-at-freeing-the-memory-properly",
offset:9275,
},
{
title:"Defer",
slug:"defer",
offset:11105,
},
{
title:"Possible solutions",
slug:"possible-solutions",
offset:13621,
},
{
title:"Conclusion",
slug:"conclusion",
offset:15819,
},
{
title:"Addendum: One more gotcha",
slug:"addendum-one-more-gotcha",
offset:16723,
},
],
},
{
html_file_name:"way_too_many_ways_to_wait_for_a_child_process_with_a_timeout.html",
title:"Way too many ways to wait on a child process with a timeout",
text:"Windows is not covered at all in this article. Discussions: /r/programming , HN , Lobsters I often need to launch a program in the terminal in a retry loop. Maybe because it\'s flaky, or because it tries to contact a remote service that is not available. A few scenarios: ssh to a (re)starting machine. psql to a (re)starting database. Ensuring that a network service started fine with netcat . File system commands over NFS. It\'s a common problem, so much so that there are two utilities that I usually reach for: timeout from GNU coreutils, which launches a command with a timeout (useful if the command itself does not have a --timeout option). eb which runs a command with a certain number of times with an exponential backoff. That\'s useful to avoid hammering a server with connection attempts for example. This will all sound familiar to people who develop distributed systems: they have long known that this is best practice to retry an operation: With a timeout (either constant or adaptive). A bounded number of times e.g. 10. With a waiting time between each retry, either a constant one or an increasing one e.g. with exponential backoff. With jitter, although this point also seemed the least important since most of us use non real-time operating systems which introduce some jitter anytime we sleep or wait on something with a timeout. The AWS article makes a point that in highly contended systems, the jitter parameter is very important, but for the scope of this article I\'ll leave it out. This is best practice in distributed systems, and we often need to do the same on the command line. But the two aforementioned tools only do that partially: timeout does not retry. eb does not have a timeout. So let\'s implement our own that does both! As we\'ll see, it\'s much less straightforward, and thus more interesting, than I thought. It\'s a whirlwind tour through Unix deeps. If you\'re interested in systems programming, Operating Systems, multiplexed I/O, data races, weird historical APIs, and all the ways you can shoot yourself in the foot with just a few system calls, you\'re in the right place! What are we building? I call the tool we are building ueb for: micro exponential backoff. It does up to 10 retries, with a waiting period in between that starts at an arbitrary 128 ms and doubles every retry. The timeout for the subprocess is the same as the sleep time, so that it\'s adaptive and we give the subprocess a longer and longer time to finish successfully. These numbers would probably be exposed as command line options in a real polished program, but there\'s no time, we have to demo it: # This returns immediately since it succeeds on the first try.\n$ ueb true\n\n# This retries 10 times since the command always fails, waiting more and more time between each try, and finally returns the last exit code of the command (1).\n$ ueb false\n\n# This retries a few times (~ 4 times), until the waiting time exceeds the duration of the sub-program. It exits with `0` since from the POV of our program, the sub-program finally finished in its alloted time.\n$ ueb sleep 1\n\n\n# Run a program that prints the date and time, and exits with a random status code, to see how it works.\n$ ueb sh -c \'date --iso-8601=ns; export R=$(($RANDOM % 5)); echo $R; exit $R\'\n2024-11-10T15:48:49,499172093+01:00\n4\n2024-11-10T15:48:49,628818472+01:00\n3\n2024-11-10T15:48:49,886557676+01:00\n4\n2024-11-10T15:48:50,400199626+01:00\n3\n2024-11-10T15:48:51,425937132+01:00\n2\n2024-11-10T15:48:53,475565645+01:00\n2\n2024-11-10T15:48:57,573278508+01:00\n1\n2024-11-10T15:49:05,767338611+01:00\n0\n\n# Some more practical examples.\n$ ueb ssh &lt;some_ip&gt;\n$ ueb createdb my_great_database -h 0.0.0.0 -U postgres If you want to monitor the retries and the sleeps, you can use strace or dtrace : $ strace ueb sleep 1 Note that the sub-command should be idempotent, otherwise we might create a given resource twice, or the command might have succeeded right after our timeout triggered but also right before we killed it, so our program thinks it timed out and thus needs to be retried. There is this small data race window, which is completely fine if the command is idempotent but will erroneously retry the command to the bitter end otherwise. There is also the case where the sub-command does stuff over the network for example creating a resource, it succeeds, but the ACK is never received due to network issues. The sub-command will think it failed and retry. Again, fairly standard stuff in distributed systems but I thought it was worth mentioning. So how do we implement it? Immediately, we notice something: even though there are a bazillion ways to wait on a child process to finish ( wait , wait3 , wait4 , waitid , waitpid ), none of them take a timeout as an argument. This has sparked numerous questions online ( 1 , 2 ), with in my opinion unsatisfactory answers. So let\'s explore this rabbit hole. We\'d like the pseudo-code to be something like: wait_ms := 128\n\nfor retry in 0..&lt;10:\n    child_pid := run_command_in_subprocess(cmd)\n\n    ret := wait_for_process_to_finish_with_timeout_ms(child_pid, wait_ms)\n    if (did_process_finish_successfully(ret)):\n        exit(0)\n        \n    // In case of a timeout, we need to kill the child process and retry.\n    kill(child_pid, SIGKILL)\n\n    // Reap zombie process to avoid a resource leak.\n    waitpid(child_pid)\n\n    sleep_ms(wait_ms);\n\n    wait_ms *= 2;\n\n// All retries exhausted, exit with an error code.\nexit(1) There is a degenerate case where the give command to run is wrong (e.g. typo in the parameters) or the executable does not exist, and our program will happily retry it to the bitter end. But there is solace: this is bounded by the number of retries (10). That\'s why we do not retry forever. First approach: old-school sigsuspend That\'s how timeout from coreutils implements it. This is quite simple on paper: We opt-in to receive a SIGCHLD signal when the child processes finishes with: signal(SIGCHLD, on_chld_signal) where on_chld_signal is a function pointer we provide. Even if the signal handler does not do anything in this case. We schedule a SIGALARM signal with alarm or more preferably setitimer which can take a duration in microseconds whereas alarm can only handle seconds. There\'s also timer_create/timer_settime which handles nanoseconds. It depends what the OS and hardware support. We wait for either signal with sigsuspend which suspends the program until a given set of signals arrive. We should not forget to wait on the child process to avoid leaving zombie processes behind. The reality is grimmer, looking through the timeout implementation: We could have inherited any signal mask from our parent so we need to explicitly unblock the signals we are interested in. Signals can be sent to a process group we need to handle that case. We have to avoid entering a \'signal loop\'. Our process can be implicitly multi-threaded due to some timer_settime implementations, therefore a SIGALRM signal sent to a process group, can result in the signal being sent multiple times to a process (I am directly quoting the code comments from the timeout program here). When using timer_create , we need to take care of cleaning it up with timer_delete , lest we have a resource leak when retrying. The signal handler may be called concurrently and we have to be aware of that. Depending on the timer implementation we chose, we are susceptible to clock adjustments for example going back. E.g. setitimer only offers the CLOCK_REALTIME clock option for counting time, which is just the wall clock. We\'d like something like CLOCK_MONOTONIC or CLOCK_MONOTONIC_RAW (the latter being Linux specific). So... I don\'t love this approach: I find signals hard. It\'s basically a global goto to a completely different location. A signal handler is forced to use global mutable state, which is better avoided if possible, and it does not play nice with threads. Lots of functions are not \'signal-safe\', and that has led to security vulnerabilities in the past e.g. in ssh . In short, non-atomic operations are not signal safe because they might be suspended in the middle, thus leaving an inconsistent state behind. So, we have to read documentation very carefully to ensure that we only call signal safe functions in our signal handler, and cherry on the cake, that varies from platform to platform, or even between libc versions on the same platform. Signals do not compose well with other Unix entities such as file descriptors and sockets. For example, we cannot poll on signals. There are platform specific solutions though, keep on reading. Different signals have different default behaviors, and this gets inherited in child processes, so you cannot assume anything in your program and have to be very defensive. Who knows what the parent process, e.g. the shell, set as the signal mask? If you read through the whole implementation of the timeout program, a lot of the code is dedicated to setting signal masks in the parent, forking, immediately changing the signal mask in the child and the parent, etc. Now, I believe modern Unices offer more control than fork() about what signal mask the child should be created with, so maybe it got better. Still, it\'s a lot of stuff to know. They are many libc functions and system calls relating to signals and that\'s a lot to learn. A non-exhaustive list e.g. on Linux: kill(1), alarm(2), kill(2), pause(2), sigaction(2), signalfd(2),  sigpending(2), sigprocmask(2), sigsuspend(2), bsd_signal(3), killpg(3), raise(3), siginterrupt(3), sigqueue(3), sigsetops(3), sigvec(3), sysv_signal(3), signal(7) . Oh wait, I forgot sigemptyset(3) and sigaddset(3) . And I\'m sure I overlooked a few more! So, let\'s stick with signals for a bit but simplify our current approach. Second approach: sigtimedwait Wouldn\'t it be great if we could wait on a signal, say, SIGCHLD , with a timeout? Oh look, a system call that does exactly that and is standardized by POSIX 2001. Cool! I am not quite sure why the timeout program does not use it, but we sure as hell can. My only guess would be that they want to support old Unices pre 2001, or non POSIX systems. A knowledgeable reader has pointed out that sigtimedwait was optional in POSIX 2001 and as such not implemented in some operating systems. It was made mandatory in POSIX 2008 but the adoption was slow. Anyways, here\'s a very straightforward implementation: #define _GNU_SOURCE\n#include &lt;errno.h&gt;\n#include &lt;signal.h&gt;\n#include &lt;stdint.h&gt;\n#include &lt;sys/wait.h&gt;\n#include &lt;unistd.h&gt;\n\nvoid on_sigchld(int sig) { (void)sig; }\n\nint main(int argc, char *argv[]) {\n  (void)argc;\n  signal(SIGCHLD, on_sigchld);\n\n  uint32_t wait_ms = 128;\n\n  for (int retry = 0; retry &lt; 10; retry += 1) {\n    int child_pid = fork();\n    if (-1 == child_pid) {\n      return errno;\n    }\n\n    if (0 == child_pid) { // Child\n      argv += 1;\n      if (-1 == execvp(argv[0], argv)) {\n        return errno;\n      }\n      __builtin_unreachable();\n    }\n\n    sigset_t sigset = {0};\n    sigemptyset(&amp;sigset);\n    sigaddset(&amp;sigset, SIGCHLD);\n\n    siginfo_t siginfo = {0};\n\n    struct timespec timeout = {\n        .tv_sec = wait_ms / 1000,\n        .tv_nsec = (wait_ms % 1000) * 1000 * 1000,\n    };\n\n    int sig = sigtimedwait(&amp;sigset, &amp;siginfo, &amp;timeout);\n    if (-1 == sig &amp;&amp; EAGAIN != errno) { // Error\n      return errno;\n    }\n    if (-1 != sig) { // Child finished.\n      if (WIFEXITED(siginfo.si_status) &amp;&amp; 0 == WEXITSTATUS(siginfo.si_status)) {\n        return 0;\n      }\n    }\n\n    if (-1 == kill(child_pid, SIGKILL)) {\n      return errno;\n    }\n\n    if (-1 == wait(NULL)) {\n      return errno;\n    }\n\n    usleep(wait_ms * 1000);\n    wait_ms *= 2;\n  }\n  return 1;\n} I like this implementation. It\'s pretty easy to convince ourselves looking at the code that it is obviously correct, and that\'s a very important factor for me. We still have to deal with signals though. Could we reduce their imprint on our code? Third approach: Self-pipe trick This is a really nifty, quite well known trick at this point, where we bridge the world of signals with the world of file descriptors with the pipe(2) system call. Usually, pipes are a form of inter-process communication, and here we do not want to communicate with the child process (since it could be any program, and most programs do not get chatty with their parent process). What we do is: in the signal handler for SIGCHLD , we simply write (anything) to our own pipe. We know this is signal-safe so it\'s good. And you know what\'s cool with pipes? They are simply a file descriptor which we can poll . With a timeout. Nice! Here goes: #define _GNU_SOURCE\n#include &lt;errno.h&gt;\n#include &lt;poll.h&gt;\n#include &lt;signal.h&gt;\n#include &lt;stdint.h&gt;\n#include &lt;sys/wait.h&gt;\n#include &lt;unistd.h&gt;\n\nstatic int pipe_fd[2] = {0};\nvoid on_sigchld(int sig) {\n  (void)sig;\n  char dummy = 0;\n  write(pipe_fd[1], &amp;dummy, 1);\n}\n\nint main(int argc, char *argv[]) {\n  (void)argc;\n\n  if (-1 == pipe(pipe_fd)) {\n    return errno;\n  }\n\n  signal(SIGCHLD, on_sigchld);\n\n  uint32_t wait_ms = 128;\n\n  for (int retry = 0; retry &lt; 10; retry += 1) {\n    int child_pid = fork();\n    if (-1 == child_pid) {\n      return errno;\n    }\n\n    if (0 == child_pid) { // Child\n      argv += 1;\n      if (-1 == execvp(argv[0], argv)) {\n        return errno;\n      }\n      __builtin_unreachable();\n    }\n\n    struct pollfd poll_fd = {\n        .fd = pipe_fd[0],\n        .events = POLLIN,\n    };\n\n    // Wait for the child to finish with a timeout.\n    poll(&amp;poll_fd, 1, (int)wait_ms);\n\n    kill(child_pid, SIGKILL);\n    int status = 0;\n    wait(&amp;status);\n    if (WIFEXITED(status) &amp;&amp; 0 == WEXITSTATUS(status)) {\n      return 0;\n    }\n\n    char dummy = 0;\n    read(pipe_fd[0], &amp;dummy, 1);\n\n    usleep(wait_ms * 1000);\n    wait_ms *= 2;\n  }\n  return 1;\n} So we still have one signal handler but the rest of our program does not deal with signals in any way (well, except to kill the child when the timeout triggers, but that\'s invisible). There are a few catches with this implementation: Contrary to sigtimedwait , poll does not give us the exit status of the child, we have to get it with wait . Which is fine. In the case that the timeout fired, we kill the child process. However, the child process, being forcefully ended, will result in a SIGCHLD signal being sent to our program. Which will then trigger our signal handler, which will then write a value to the pipe. So we need to unconditionally read from the pipe after killing the child and before retrying. If we only read from the pipe if the child ended by itself, that will result in the pipe and the child process being desynced. In some complex programs, we\'d have to use ppoll instead of poll . ppoll prevents a set of signals from interrupting the polling. That\'s to avoid some data races (again, more data races!). Quoting from the man page for pselect which is analogous to ppoll : The  reason  that pselect() is needed is that if one wants to wait for either a signal\nor for a file descriptor to become ready, then an atomic test  is  needed  to  prevent\nrace  conditions.  (Suppose the signal handler sets a global flag and returns.  Then a\ntest of this global flag followed by a call of select() could hang indefinitely if the\nsignal arrived just after the test but just before the call.  By  contrast,  pselect()\nallows one to first block signals, handle the signals that have come in, then call pselect()\nwith the desired sigmask, avoiding the race.) So, this trick is clever, but wouldn\'t it be nice if we could avoid signals entirely ? A simpler self-pipe trick An astute reader pointed out that this trick can be simplified to not deal with signals at all and instead leverage two facts: A child inherits the open file descriptors of the parent (including the ones from a pipe) When a process exits, the OS automatically closes its file descriptors Behind the scenes, at the OS level, there is a reference count for a file descriptor shared by multiple processes. It gets decremented when doing close(fd) or by a process terminating. When this count reaches 0, it is closed for real. And you know what system call can watch for a file descriptor closing? Good old poll ! So the improved approach is as follows: Each retry, we create a new pipe. We fork. The parent closes the write end pipe and the child closes the read end pipe. Effectively, the parent owns the read end and the child owns the write end. The parent polls on the read end. When the child finishes, it automatically closes the write end which in turn triggers an event in poll . We cleanup before retrying (if needed) So in a way, it\'s not really a self -pipe, it\'s more precisely a pipe between the parent and the child, and nothing gets written or read, it\'s just used by the child to signal it\'s done when it closes its end. Which is a useful approach for many cases outside of our little program. Here is the code: #define _GNU_SOURCE\n#include &lt;errno.h&gt;\n#include &lt;poll.h&gt;\n#include &lt;stdint.h&gt;\n#include &lt;sys/wait.h&gt;\n#include &lt;unistd.h&gt;\n\nint main(int argc, char *argv[]) {\n  (void)argc;\n\n  uint32_t wait_ms = 128;\n\n  for (int retry = 0; retry &lt; 10; retry += 1) {\n    int pipe_fd[2] = {0};\n    if (-1 == pipe(pipe_fd)) {\n      return errno;\n    }\n\n    int child_pid = fork();\n    if (-1 == child_pid) {\n      return errno;\n    }\n\n    if (0 == child_pid) { // Child\n      // Close the read end of the pipe.\n      close(pipe_fd[0]);\n\n      argv += 1;\n      if (-1 == execvp(argv[0], argv)) {\n        return errno;\n      }\n      __builtin_unreachable();\n    }\n\n    // Close the write end of the pipe.\n    close(pipe_fd[1]);\n\n    struct pollfd poll_fd = {\n        .fd = pipe_fd[0],\n        .events = POLLHUP | POLLIN,\n    };\n\n    // Wait for the child to finish with a timeout.\n    poll(&amp;poll_fd, 1, (int)wait_ms);\n\n    kill(child_pid, SIGKILL);\n    int status = 0;\n    wait(&amp;status);\n    if (WIFEXITED(status) &amp;&amp; 0 == WEXITSTATUS(status)) {\n      return 0;\n    }\n\n    close(pipe_fd[0]);\n\n    usleep(wait_ms * 1000);\n    wait_ms *= 2;\n  }\n  return 1;\n} Voila, no signals and no global state! Fourth approach: Linux\'s signalfd This is a short one: on Linux, there is a system call that does exactly the same as the self-pipe trick: from a signal, it gives us a file descriptor that we can poll . So, we can entirely remove our pipe and signal handler and instead poll the file descriptor that signalfd gives us. Cool, but also....Was it really necessary to introduce a system call for that? I guess the advantage is clarity. I would prefer extending poll to support things other than file descriptors, instead of converting everything a file descriptor to be able to use poll . Ok, next! Fifth approach: process descriptors Recommended reading about this topic: 1 and 2 . In the recent years (starting with Linux 5.3 and FreeBSD 9), people realized that process identifiers ( pid s) have a number of problems: PIDs are recycled and the space is small, so collisions will happen. Typically, a process spawns a child process, some work happens, and then the parent decides to send a signal to the PID of the child. But it turns out that the child already terminated (unbeknownst to the parent) and another process took its place with the same PID. So now the parent is sending signals, or communicating with, a process that it thinks is its original child but is in fact something completely different. Chaos and security issues ensue. Now, in our very simple case, that would not really happen, but perhaps the root user is running our program, or, imagine that you are implementing the init process with PID 1, e.g. systemd: you can kill any process on the machine! Or think of the case of re-parenting a process. Or sending a certain PID to another process and they send a signal to it at some point in the future. It becomes hairy and it\'s a very real problem. Data races are hard to escape (see the previous point). It\'s easy to accidentally send a signal to all processes with kill(0, SIGKILL) or kill(-1, SIGKILL) if the developer has not checked that all previous operations succeeded. This is a classic mistake: int child_pid = fork();  // This fork fails and returns -1.\n... // (do not check that fork succeeded);\nkill(child_pid, SIGKILL); // Effectively: kill(-1, SIGKILL) And the kernel developers have worked hard to introduce a better concept: process descriptors, which are (almost) bog-standard file descriptors, like files or sockets. After all, that\'s what sparked our whole investigation: we wanted to use poll and it did not work on a PID. PIDs and signals do not compose well, but file descriptors do. Also, just like file descriptors, process descriptors are per-process. If I open a file with open() and get the file descriptor 3 , it is scoped to my process. Another process can close(3) and it will refer to their own file descriptor, and not affect my file descriptor. That\'s great, we get isolation, so bugs in our code do not affect other processes. So, Linux and FreeBSD have introduced the same concepts but with slightly different APIs (unfortunately), and I have no idea about other OSes: A child process can be created with clone3(..., CLONE_PIDFD) (Linux) or pdfork() (FreeBSD) which returns a process descriptor which is almost like a normal file descriptor. On Linux, a process descriptor can also be obtained from a PID with pidfd_open(pid) e.g. after a normal fork was done (but there is a risk of a data race in some cases!). Once we have the process descriptor, we do not need the PID anymore. We wait on the process descriptor with poll(..., timeout) (or select , or epoll , etc). We kill the child process using the process descriptor with pidfd_send_signal (Linux) or close (FreeBSD) or pdkill (FreeBSD). We wait on the zombie child process again using the process descriptor to get its exit status. And voila, no signals! Isolation! Composability! (Almost) No PIDs in our program! Life can be nice sometimes. It\'s just unfortunate that there isn\'t a cross-platform API for that. Here\'s the Linux implementation: #define _GNU_SOURCE\n#include &lt;errno.h&gt;\n#include &lt;poll.h&gt;\n#include &lt;stdint.h&gt;\n#include &lt;sys/syscall.h&gt;\n#include &lt;sys/wait.h&gt;\n#include &lt;unistd.h&gt;\n\nint main(int argc, char *argv[]) {\n  (void)argc;\n\n  uint32_t wait_ms = 128;\n\n  for (int retry = 0; retry &lt; 10; retry += 1) {\n    int child_pid = fork();\n    if (-1 == child_pid) {\n      return errno;\n    }\n\n    if (0 == child_pid) { // Child\n      argv += 1;\n      if (-1 == execvp(argv[0], argv)) {\n        return errno;\n      }\n      __builtin_unreachable();\n    }\n\n    // Parent.\n\n    int child_fd = (int)syscall(SYS_pidfd_open, child_pid, 0);\n    if (-1 == child_fd) {\n      return errno;\n    }\n\n    struct pollfd poll_fd = {\n        .fd = child_fd,\n        .events = POLLHUP | POLLIN,\n    };\n    // Wait for the child to finish with a timeout.\n    if (-1 == poll(&amp;poll_fd, 1, (int)wait_ms)) {\n      return errno;\n    }\n\n    if (-1 == syscall(SYS_pidfd_send_signal, child_fd, SIGKILL, NULL, 0)) {\n      return errno;\n    }\n\n    siginfo_t siginfo = {0};\n    // Get exit status of child &amp; reap zombie.\n    if (-1 == waitid(P_PIDFD, (id_t)child_fd, &amp;siginfo, WEXITED)) {\n      return errno;\n    }\n\n    if (WIFEXITED(siginfo.si_status) &amp;&amp; 0 == WEXITSTATUS(siginfo.si_status)) {\n      return 0;\n    }\n\n    wait_ms *= 2;\n    usleep(wait_ms * 1000);\n\n    close(child_fd);\n  }\n} A small note: To poll a process descriptor, Linux wants us to use POLLIN whereas FreeBSD wants us to use POLLHUP . So we use POLLHUP | POLLIN since there are no side-effects to use both. Another small note: a process descriptor, just like a file descriptor, takes up resources on the kernel side and we can reach some system limits (or even the memory limit), so it\'s good practice to close it as soon as possible to free up resources. For us, that\'s right before retrying. On FreeBSD, closing the process descriptor also kills the process, so it\'s very short, just one system call. On Linux, we need to do both. Sixth approach: MacOS\'s and BSD\'s kqueue It feels like cheating, but MacOS and the BSDs have had kqueue for decades which works out of the box with PIDs. It\'s a bit similar to poll or epoll on Linux: #include &lt;errno.h&gt;\n#include &lt;signal.h&gt;\n#include &lt;stdint.h&gt;\n#include &lt;sys/event.h&gt;\n#include &lt;sys/wait.h&gt;\n#include &lt;unistd.h&gt;\n\nint main(int argc, char *argv[]) {\n  (void)argc;\n\n  uint32_t wait_ms = 128;\n  int queue = kqueuex(KQUEUE_CLOEXEC);\n\n  for (int retry = 0; retry &lt; 10; retry += 1) {\n    int child_pid = fork();\n    if (-1 == child_pid) {\n      return errno;\n    }\n\n    if (0 == child_pid) { // Child\n      argv += 1;\n      if (-1 == execvp(argv[0], argv)) {\n        return errno;\n      }\n      __builtin_unreachable();\n    }\n\n    struct kevent change_list = {\n        .ident = child_pid,\n        .filter = EVFILT_PROC,\n        .fflags = NOTE_EXIT,\n        .flags = EV_ADD | EV_CLEAR,\n    };\n\n    struct kevent event_list = {0};\n\n    struct timespec timeout = {\n        .tv_sec = wait_ms / 1000,\n        .tv_nsec = (wait_ms % 1000) * 1000 * 1000,\n    };\n\n    int ret = kevent(queue, &amp;change_list, 1, &amp;event_list, 1, &amp;timeout);\n    if (-1 == ret) { // Error\n      return errno;\n    }\n    if (1 == ret) { // Child finished.\n      int status = 0;\n      if (-1 == wait(&amp;status)) {\n        return errno;\n      }\n      if (WIFEXITED(status) &amp;&amp; 0 == WEXITSTATUS(status)) {\n        return 0;\n      }\n    }\n\n    kill(child_pid, SIGKILL);\n    wait(NULL);\n\n    change_list = (struct kevent){\n        .ident = child_pid,\n        .filter = EVFILT_PROC,\n        .fflags = NOTE_EXIT,\n        .flags = EV_DELETE,\n    };\n    kevent(queue, &amp;change_list, 1, NULL, 0, NULL);\n\n    usleep(wait_ms * 1000);\n    wait_ms *= 2;\n  }\n  return 1;\n} The only surprising thing, perhaps, is that a kqueue is stateful, so once the child process exited by itself or was killed, we have to remove the watcher on its PID, since the next time we spawn a child process, the PID will very likely be different. kqueue offers the flag EV_ONESHOT , which automatically deletes the event from the queue once it has been consumed by us. However, it would not help in all cases: if the timeout triggers, no event was consumed, and we have to kill the child process, which creates an event in the queue! So we have to always consume/delete the event from the queue right before we retry, with a second kevent call. That\'s the same situation as with the self-pipe approach where we unconditionally read from the pipe to \'clear\' it before retrying. I love that kqueue works with every kind of Unix entity: file descriptor, pipes, PIDs, Vnodes, sockets, etc. Even signals! However, I am not sure that I love its statefulness. I find the poll API simpler, since it\'s stateless. But perhaps this behavior is necessary for some corner cases or for performance to avoid the linear scanning that poll entails? It\'s interesting to observe that Linux\'s epoll went the same route as kqueue with a similar API, however, epoll can only watch plain file descriptors. A parenthesis: libkqueue kqueue is only for MacOS and BSDs....Or is it? There is this library, libkqueue , that acts as a compatibility layer to be able to use kqueue on all major operating systems, mainly Windows, Linux, and even Solaris/illumos! So...How do they do it then? How can we, on an OS like Linux, watch a PID with the kqueue API, when the OS does not support that functionality (neither with poll or epoll )? Well, the solution is actually very simple: On Linux 5.3+, they use pidfd_open + poll/epoll . Hey, we just did that a few sections above! On older versions of Linux, they handle the signals, like GNU\'s timeout . It has a number of known shortcomings which is testament to the hardships of using signals. To just quote one piece: Because the Linux kernel coalesces SIGCHLD (and other signals), the only way to reliably determine if a monitored process has exited, is to loop through all PIDs registered by any kqueue when we receive a SIGCHLD. This involves many calls to waitid(2) and may have a negative performance impact. Another parenthesis: Solaris/illumos\'s ports So, if it was not enough that each major OS has its own way to watch many different kinds of entities (Windows has its own thing called I/O completion ports , MacOS &amp; BSDs have kqueue , Linux has epoll ), Solaris/illumos shows up and says: Watch me do my own thing. Well actually I do not know the chronology, they might in fact have been first, and some illumos kernel developers (namely Brian Cantrill in the fabulous Cantrillogy ) have admitted that it would have been better for everyone if they also had adopted kqueue . Anyways, their own system is called port (or is it ports?) and it looks so similar to kqueue it\'s almost painful. And weirdly, they support all the different kinds of entities that kqueue supports except PIDs! And I am not sure that they support process descriptors either e.g. pidfd_open . However, they have an extensive compatibility layer for Linux so perhaps they do there. EDIT: illumos has Pctlfd which seems to give a file descriptor for a given process, and this file descriptor could then be used port_create or poll . Seventh approach: Linux\'s io_uring io_uring is the last candidate to enter the already packed ring (eh) of different-yet-similar ways to do \'I/O multiplexing\', meaning to wait with a timeout on various kinds of entities to do interesting \'stuff\'. We queue a system call e.g. wait , as well as a timeout, and we wait for either to complete. If wait completed first and the exit status is a success, we exit. Otherwise, we retry. Familiar stuff at this point. io_uring essentially makes every system call asynchronous with a uniform API. That\'s exactly what we want! io_uring only exposes waitid and only in very recent versions, which is completely fine. Incidentally, this approach is exactly what liburing does in a unit test . Alternatively, we can only queue the waitid and use io_uring_wait_cqe_timeout to mimick poll(..., timeout) : #define _DEFAULT_SOURCE\n#include &lt;liburing.h&gt;\n#include &lt;sys/wait.h&gt;\n#include &lt;unistd.h&gt;\n\nint main(int argc, char *argv[]) {\n  (void)argc;\n\n  struct io_uring ring = {0};\n  if (io_uring_queue_init(2, &amp;ring,\n                          IORING_SETUP_SINGLE_ISSUER |\n                              IORING_SETUP_DEFER_TASKRUN) &lt; 0) {\n    return 1;\n  }\n\n  uint32_t wait_ms = 128;\n\n  for (int retry = 0; retry &lt; 10; retry += 1) {\n    int child_pid = fork();\n    if (-1 == child_pid) {\n      return errno;\n    }\n\n    if (0 == child_pid) { // Child\n      argv += 1;\n      if (-1 == execvp(argv[0], argv)) {\n        return errno;\n      }\n      __builtin_unreachable();\n    }\n\n    struct io_uring_sqe *sqe = NULL;\n\n    // Queue `waitid`.\n    sqe = io_uring_get_sqe(&amp;ring);\n    siginfo_t si = {0};\n    io_uring_prep_waitid(sqe, P_PID, (id_t)child_pid, &amp;si, WEXITED, 0);\n    sqe-&gt;user_data = 1;\n\n    io_uring_submit(&amp;ring);\n\n    struct __kernel_timespec ts = {\n        .tv_sec = wait_ms / 1000,\n        .tv_nsec = (wait_ms % 1000) * 1000 * 1000,\n    };\n    struct io_uring_cqe *cqe = NULL;\n\n    int ret = io_uring_wait_cqe_timeout(&amp;ring, &amp;cqe, &amp;ts);\n\n    // If child exited successfully: the end.\n    if (ret == 0 &amp;&amp; cqe-&gt;res &gt;= 0 &amp;&amp; cqe-&gt;user_data == 1 &amp;&amp;\n        WIFEXITED(si.si_status) &amp;&amp; 0 == WEXITSTATUS(si.si_status)) {\n      return 0;\n    }\n    if (ret == 0) {\n      io_uring_cqe_seen(&amp;ring, cqe);\n    } else {\n      kill(child_pid, SIGKILL);\n      // Drain the CQE.\n      ret = io_uring_wait_cqe(&amp;ring, &amp;cqe);\n      io_uring_cqe_seen(&amp;ring, cqe);\n    }\n\n    wait(NULL);\n\n    wait_ms *= 2;\n    usleep(wait_ms * 1000);\n  }\n  return 1;\n} The only difficulty here is in case of timeout: we kill the child directly, and we need to consume and discard the waitid entry in the completion queue. Just like kqueue . One caveat for io_uring: it\'s only supported on modern kernels (5.1+). Another caveat: some cloud providers e.g. Google Cloud disable io_uring due to security concerns when running untrusted code. So it\'s not ubiquitous. Eigth approach: Threads Readers have pointed out that threads are also a solution, albeit a suboptimal one. Here\'s the approach: Spawn a thread, it will be in charge of spawning the child process, storing the child PID in a global thread-safe variable (e.g. protected by a mutex). It then wait s on the child in a blocking way. If the child exits, wait will return the status, which is also written in a global thread-safe variable, and the thread ends. In the main thread, wait on the other thread with a timeout, e.g. with pthread_timedjoin_np . If the child did not exit successfully, this is the same as usual: kill, wait, sleep, and retry. If the threads library supports returning a value from a thread, like pthread or C11 threads do, that could be used to return the exit status of the child to simplify the code a bit. Also, we could make the thread spawning logic a bit more efficient by not spawning a new thread for each retry, if we wanted to. Instead, we communicate with the other thread with a queue or such to instruct it to spawn the child again. It\'s more complex though. Now, this approach works but is kind of cumbersome (as noted by the readers), because threads interact in surprising ways with signals (yay, another thing to watch out for!) so we may have to set up signal masks to block/ignore some, and we must take care of not introducing data-races due to the global variables. Unless the problem is embarassingly parallel and the threads share nothing (e.g.: dividing an array into pieces and each thread gets its own piece to work on), I am reminded of the adage: &quot;You had two problems. You reach out for X. You now have 3 problems&quot;. And threads are often the X. Still, it\'s a useful tool in the toolbox. Ninth approach: Active polling. That\'s looping in user code with micro-sleeping to actively poll on the child status in a non-blocking way, for example using wait(..., WNOHANG) . Unless you have a very bizzare use case and you know what you are doing, please do not do this. This is unnecessary, bad for power consumption, and all we achieve is noticing late that the child ended. This approach is just here for completeness. Conclusion I find signals and spawning child process to be the hardest parts of Unix. Evidently this is not a rare opinion, looking at the development in these areas: process descriptors, the various expansions to the venerable fork with vfork , clone , clone3 , clone6 , a bazillion different ways to do I/O multiplexing, etc. So what\'s the best approach then in a complex program? Let\'s recap: If you need maximum portability and are a Unix wizard, you can use sigsuspend . If you are not afraid of signals, want a simpler API that still widely supported, and the use case is very specific (like ours), you can use sigtimedwait . If you favor correctness and work with recent Linux and FreeBSD versions, you can use process descriptors with shims to get the same API on both OSes. That\'s probably my favorite option if it\'s applicable. If you only care about MacOS and BSDs (or accept to use libkqueue on Linux), you can use kqueue because it works out of the box with PIDs, you avoid signals completely, and it\'s used in all the big libraries out of there e.g. libuv . If you only care about bleeding edge Linux, are already using io_uring in your code, you can use io_uring . If you only care about Linux and are afraid of using io_uring , you can use signalfd + poll . I often look at complex code and think: what are the chances that this is correct? What are the chances that I missed something? Is there a way to make it so simple that it is obviously correct? And how can I limit the blast of a bug I wrote? Will I understand this code in 3 months? When dealing with signals, I was constantly finding weird corner cases and timing issues leading to data races. You would not believe how many times I got my system completely frozen while writing this article, because I accidentally fork-bombed myself or simply forgot to reap zombie processes. And to be fair to the OS developers that have to implement them: I do not think they did a bad job! I am sure it\'s super hard to implement! It\'s just that the whole concept and the available APIs are very easy to misuse. It\'s a good illustration of how a good API, the right abstraction, can enable great programs, and a poor API, the wrong abstraction, can be the root cause of various bugs in many programs for decades. And OS developers have noticed and are working on new, better abstractions! Process descriptors seem to me so straightforward, so obviously correct, that I would definitely favor them over signals. They simply remove entire classes of bugs. If these are not available to me, I would perhaps use kqueue instead (with libkqueue emulation when necessary), because it means my program can be extended easily to watch for over types of entities and I like that the API is very straightforward: one call to create the queue and one call to use it. Finally, I regret that there is so much fragmentation across all operating systems. Perhaps io_uring will become more than a Linuxism and spread to Windows, MacOS, the BSDs, and illumos in the future? Addendum: The code The code is available here . It does not have any dependencies except libc (well, and libkqueue for kqueue.c on Linux). All of these programs are in the worst case 27 KiB in size, with debug symbols enabled and linking statically to musl. They do not allocate any memory themselves.\nFor comparison, eb has 24 dependencies and is 1.2 MiB! That\'s roughly 50x times more. ",
titles:[
{
title:"What are we building?",
slug:"what-are-we-building",
offset:2114,
},
{
title:"First approach: old-school sigsuspend",
slug:"first-approach-old-school-sigsuspend",
offset:5751,
},
{
title:"Second approach: sigtimedwait",
slug:"second-approach-sigtimedwait",
offset:9770,
},
{
title:"Third approach: Self-pipe trick",
slug:"third-approach-self-pipe-trick",
offset:11997,
},
{
title:"A simpler self-pipe trick",
slug:"a-simpler-self-pipe-trick",
offset:15652,
},
{
title:"Fourth approach: Linux\'s signalfd",
slug:"fourth-approach-linux-s-signalfd",
offset:18221,
},
{
title:"Fifth approach: process descriptors",
slug:"fifth-approach-process-descriptors",
offset:18816,
},
{
title:"Sixth approach: MacOS\'s and BSD\'s kqueue",
slug:"sixth-approach-macos-s-and-bsd-s-kqueue",
offset:24174,
},
{
title:"A parenthesis: libkqueue",
slug:"a-parenthesis-libkqueue",
offset:27253,
},
{
title:"Another parenthesis: Solaris/illumos\'s ports",
slug:"another-parenthesis-solaris-illumos-s-ports",
offset:28300,
},
{
title:"Seventh approach: Linux\'s io_uring",
slug:"seventh-approach-linux-s-io-uring",
offset:29404,
},
{
title:"Eigth approach: Threads",
slug:"eigth-approach-threads",
offset:32373,
},
{
title:"Ninth approach: Active polling.",
slug:"ninth-approach-active-polling",
offset:34118,
},
{
title:"Conclusion",
slug:"conclusion",
offset:34544,
},
{
title:"Addendum: The code",
slug:"addendum-the-code",
offset:37563,
},
],
},
{
html_file_name:"the_missing_cross_platform_os_api_for_timers.html",
title:"The missing cross-platform OS API for timers",
text:"Discussions: /r/programming , HN Most serious programs will need to trigger some action at a delayed point in time, often repeatedly: set timeouts, clean up temporary files or entries in the database, send keep-alives, garbage-collect unused entities, etc. All while doing some work in the meantime. A blocking sleep won\'t cut it! For example, JavaScript has setTimeout . But how does it work under the hood? How does each OS handle that? Lately, I have found myself in need of doing just that, repeatedly sending a keep-alive over the network to many remote peers, in C. My program has an event loop, a la NodeJS or Redis. It is doing lots of file I/O, network I/O, and handling timers, all in a single thread, in a non-blocking way. And I wanted to do all that in a cross-platform way. And to my surprise, I could not find a (sane) libc function or syscall to create a timer, and that is the same on all Unices! Each Unix variant had its own weird way to do it, as I discovered. I am used to Windows being the odd kid in its corner doing its thing, but usually, Unices (thanks to POSIX) agree on a simple API to do something. There\'s the elephant in the room, of course, with epoll/kqueue/event ports... Which is such a pain that numerous libraries have sprung up to paper over the differences and offer The One API To Rule Them All : libuv, libev, libevent, etc. So, are timers the same painful ordeal? Well, let\'s take a tour of all the OS APIs to handle them. Windows: SetTimer This will be brief because I do not develop on Windows. The official documentation mentions the SetTimer function from Win32 and you pass it a timeout and a callback. Alternatively, since Windows applications come with a built-in event queue, an event of type WM_TIMER gets emitted and can be consumed from the queue. Simple, and it composes with other OS events, I like it. Readers suggestions: CreateWaitableTimer, CreateThreadpoolTimer, CreateTimerQueueTimer Readers with experience with SetTimer have pointed out that SetTimer has flaws: An invisible window must be created to get a message queue, which is opt-in. The parameter size is limited which can be an issue. The event WM_TIMER is low-priority so any other events, say mouse events, will take precedence. Worry not, there are alternatives (and again, I mention them in passing as someone who does not develop on Windows): CreateThreadpoolTimer which works with the threadpool every Win32 process gets by default CreateWaitableTimer does not need a window, and can be shared between processes to do inter-process synchronization. Which is pretty nifty. And a timer created by this function can be referred by name. POSIX: timer_create, timer_settime POSIX has one API for timers, and it sucks. A timer is created with timer_create , which does initially nothing, and the timer is started with a timeout using timer_settime . When the timeout is reached, a signal is sent to the program. And that\'s the issue. Signals are very problematic, as seen in my previous article : They do not compose with other OS primitives. This forces numerous syscalls to have a normal version and a signal-aware version that can block some signals for its duration: poll/ppoll , select/pselect , etc. They are affected by the signal mask of the parent (e.g.: the shell, the service runner, etc) They behave confusingly with child processes. Normally, a signal mask is inherited by the child. But some signal-triggering APIs (e.g.: timer_settime )  explicitly prevent child processes from inheriting their signals. I guess we\'ll have to read the fine prints in the man page! It\'s hard to write complex programs with signals in mind due to their global nature. Code of our own, or in a library we use, could block some signals for some period of time, unbeknownst to us. Or simply modify the signal mask of the process, so we can never assume that the signal mask has a given value. A signal handler has to use global variables, there is no way to pass it a pointer to some data. Most functions are not async-signal-safe and should not be used from within a signal handler but no compiler warns about that and most example code is wrong. This is exacerbated by the fact that a given function may be async-signal safe on some OS but not on another. Or for some version of this OS but not for another version. This has caused real security vulnerabilities in the past. I\'ll just quote here the Linux man page for timer_create : /* Note: calling printf() from a signal handler is not safe\n  (and should not be done in production programs), since\n  printf() is not async-signal-safe; see signal-safety(7).\n  Nevertheless, we use printf() here [...]. */ Enough said. And this is really tricky to get right. For example, malloc is not async-signal-safe. By the way, you have to go out of your way to find this out, because the man page (at least on my system) does not mention anything about signals or async safety. Well, you think, let\'s just remember to not use malloc in signal handlers! Done! Feeling confident, we happen to call qsort in our signal handler. Should be fine, right? We just sort some data in-place... Well, we just introduced a security vulnerability! That\'s because in glibc, the innocent looking qsort calls malloc under the hood! (And that was, in the past, the cause of qsort segfaulting, which I find hilarious): to our great surprise, we discovered\nthat the glibc\'s qsort() is not, in fact, a quick sort by default, but a\nmerge sort (in stdlib/msort.c).\n[...]\nBut merge sort suffers from one\nmajor drawback: it does not sort in-place -- it malloc()ates a copy of\nthe array of elements to be sorted. So...let\'s accept that writing signal handlers correctly is not feasible for us mere mortals. Many people have concluded the same in the past and have created better OS APIs that do not involve signals at all. Let\'s look into that. Linux: timerfd_create, timerfd_settime So, we all heard the saying: In Unix, everything is a file. So, what if a timer was also a file (descriptor)? And we could ask the OS to notify our program whenever there is data to read (i.e.: when our timer triggers), like with a file or a socket?\nThat\'s the whole idea behind timerfd_create and timerfd_settime . We create a timer, we get a file descriptor back. In the previous article, we saw that Linux added similar APIs for signals with signalfd and processes with pidfd_open , so there is a consistent effort to indeed make everything a file. That means that using the venerable poll(2) , we can wait on an array of very diverse things: sockets, files, signals, timers, processes, pipes, etc. This is great! That\'s simple (one API for all OS entities) and composable (handling an additional OS entity does not force our program to undergo big changes, and we can wait on diverse OS entities using the same API). Let\'s see it in action by creating 10 timers and waiting for them to trigger: #include &lt;assert.h&gt;\n#include &lt;inttypes.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;sys/epoll.h&gt;\n#include &lt;sys/timerfd.h&gt;\n#include &lt;unistd.h&gt;\n\nint main() {\n  int queue = epoll_create(1 /* Ignored */);\n  assert(-1 != queue);\n\n  int res = 0;\n\n  for (int i = 0; i &lt; 10; i++) {\n    res = timerfd_create(CLOCK_MONOTONIC, TFD_NONBLOCK);\n    assert(-1 != res);\n\n    int fd = res;\n    struct itimerspec ts = {.it_value.tv_nsec = i * 50 * 1000 * 1000};\n    res = timerfd_settime(fd, 0, &amp;ts, NULL);\n    assert(-1 != res);\n\n    struct epoll_event ev = {\n        .events = EPOLLIN,\n        .data.fd = fd,\n    };\n    res = epoll_ctl(queue, EPOLL_CTL_ADD, fd, &amp;ev);\n    assert(-1 != res);\n  }\n\n  int timeout_ms = 1000;\n\n  struct epoll_event events[10] = {0};\n  int events_len = 10;\n\n  for (;;) {\n    res = epoll_wait(queue, events, events_len, timeout_ms);\n    assert(-1 != res);\n\n    if (0 == res) { // The end.\n      return 0;\n    }\n\n    for (int i = 0; i &lt; res; i++) {\n      struct epoll_event event = events[i];\n      if (event.events &amp; EPOLLIN) {\n        struct timespec now = {0};\n        clock_gettime(CLOCK_REALTIME, &amp;now);\n        printf(&quot;[%ld.%03ld] timer %d triggered\\n&quot;, now.tv_sec,\n               now.tv_nsec / 1000 / 1000, event.data.fd);\n        close(event.data.fd);\n      }\n    }\n  }\n} And it prints: [1738530944.233] timer 5 triggered\n[1738530944.283] timer 6 triggered\n[1738530944.333] timer 7 triggered\n[1738530944.383] timer 8 triggered\n[1738530944.433] timer 9 triggered\n[1738530944.483] timer 10 triggered\n[1738530944.533] timer 11 triggered\n[1738530944.583] timer 12 triggered\n[1738530944.633] timer 13 triggered The only gotcha, which is mentioned by the man page, is that we need to remember to read(2) from the timer whenever it triggers. That only matters for repeating timers (also sometimes called interval timers). There\'s even an additional benefit with this API: thanks to ProcFS , timers appear on the file system under /proc/&lt;pid&gt;/fd/ , so troubleshooting is a bit easier. However, it\'s unfortunate that this is a Linux-only API...or is it really? FreeBSD has it too : The timerfd facility was\toriginally ported to FreeBSD\'s Linux  compatibility  layer  [...] in FreeBSD 12.0.\nIt  was\trevised\t and  adapted  to   be\t native\t  [...] in FreeBSD 14.0. Illumos has it too . NetBSD has it too : The timerfd interface first appeared in NetBSD 10.  It is compatible with\nthe timerfd interface that appeared in Linux 2.6.25. OpenBSD does not seem to have it. macOS does not seem to have it. So, pretty good, but not ubiquitous. The search continues. BSD: kqueue/kevent kqueue might be my favorite OS API: it can watch any OS entity for changes with just one call. Even timers! As it is often the case for BSD-borne APIs, they are well designed and well documented. The man page says: EVFILT_TIMER   Establishes an arbitrary timer identified by ident. That\'s great, we do not even have to use various APIs to  create the timer, set the time, read from the timer, etc. We do not even have to destroy the timer ourselves, thanks to the EV_ONESHOT flag. Let\'s see it in action: #include &lt;assert.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;sys/event.h&gt;\n#include &lt;sys/time.h&gt;\n\nint main() {\n  int queue = kqueue();\n  assert(-1 != queue);\n\n  int res = 0;\n\n  struct kevent changelist[10] = {0};\n  for (int i = 0; i &lt; 10; i++) {\n    changelist[i] = (struct kevent){\n        .ident = i + 1,\n        .flags = EV_ADD | EV_ONESHOT,\n        .data = i * 50,\n        .filter = EVFILT_TIMER,\n        .fflags = NOTE_MSECONDS,\n    };\n  }\n\n  res = kevent(queue, changelist, 10, NULL, 0, 0);\n  assert(-1 != res);\n\n  struct kevent eventlist[10] = {0};\n  struct timespec timeout = {.tv_sec = 1};\n  for (;;) {\n    res = kevent(queue, NULL, 0, eventlist, 10, &amp;timeout);\n    assert(-1 != res);\n\n    if (0 == res) { // The end.\n      return 0;\n    }\n\n    for (int i = 0; i &lt; res; i++) {\n      struct kevent event = eventlist[i];\n      if (event.filter &amp; EVFILT_TIMER) {\n        struct timespec now = {0};\n        clock_gettime(CLOCK_REALTIME, &amp;now);\n        printf(&quot;[%ld.%03ld] timer %ld triggered\\n&quot;, now.tv_sec,\n               now.tv_nsec / 1000 / 1000, event.ident);\n      }\n    }\n  }\n} And it prints: [1738380963.984] timer 1 triggered\n[1738380964.034] timer 2 triggered\n[1738380964.084] timer 3 triggered\n[1738380964.134] timer 4 triggered\n[1738380964.184] timer 5 triggered\n[1738380964.234] timer 6 triggered\n[1738380964.284] timer 7 triggered\n[1738380964.334] timer 8 triggered\n[1738380964.384] timer 9 triggered\n[1738380964.434] timer 10 triggered What about the portability? FreeBSD has it NetBSD has it OpenBSD has it macOS has it A past version of this section mentioned that this was not implemented on macOS. This used to be the case way back in the day, but an astute reader pointed out that Apple added this functionality at some point around macOS 10.9 (circa 2013).\nGreat news, and thanks, nice reader! Illumos: port_create So, Illumos (in)famously has its own API for multiplexing events from disjoint sources, that is different from kqueue , and some Illumos developers have publicly stated they now wished they had adopted kqueue back in the day. Anyways, similarly to kqueue, their API ( port_create ) also supports timers! From the man page : PORT_SOURCE_TIMER events represent one or more timer expirations for a\ngiven timer.  A timer is associated with a port by specifying SIGEV_PORT\nas its notification mechanism. Interestingly, the timer is created using the POSIX API that normally triggers a signal upon timer expiration, but thanks to port_create , the signal is instead turned into an event ports notification, as if it was a file descriptor. I think it\'s pretty clever, because that means that historical code creating timers need not be modified. In other words, it makes the POSIX API sane by circumventing signals and integrating it into a more modern facility to make it composable with other OS entities. macOS: dispatch_source_create Apple developers, in their infinite wisdom, decided to support kqueue timers, and also invented their own thing. It\'s called dispatch_source_create and it supports timers with DISPATCH_SOURCE_TYPE_TIMER . I do not currently have access to an Apple computer so I have not tried it. All I know is that Grand Central Dispatch/libdispatch is an effort to have applications have an event queue and thread pool managed for them by the OS. It\'s more of a task system, actually. All of this seems to me somewhat redundant with kqueue (which, on Apple platforms, came first!), but I am not an Apple engineer. libdispatch has technically been ported to many platforms but I suppose this is just like libkqueue on Linux: it exposes the familiar API, but under the hood, it translates all calls to the OS-specific API, so for all intents and purposes, this syscall is macOS specific (well, and iOS, tvOS, IpadOS, etc, but let\'s group them all into a \'macOS\' bucket). Linux: io_uring io_uring is a fascinating Linux-only approach to essentially make every blocking system call... non-blocking. A syscall is enqueued into a ring buffer shared between userspace and the kernel, as a \'request\', and at some point in time, a \'response\' is enqueued by the kernel into a separate ring buffer that our program can read. It\'s simple, it\'s composable, it\'s great. At the beginning I said that a blocking \'sleep\' was not enough, because our program cannot do any work while sleeping. io_uring renders this moot: we can (conceptually) enqueue a sleep, do some work, for example enqueue other syscalls, and whenever our sleep finishes, we can dequeue it from the second ring buffer, and voila: we just implemented a timer. It\'s so simple it\'s brilliant! Sadly, it\'s Linux only, only for recent-ish kernels, and some cloud providers disable this facility. Let\'s see it in action: #include &lt;assert.h&gt;\n#include &lt;liburing.h&gt;\n#include &lt;liburing/io_uring.h&gt;\n#include &lt;stdio.h&gt;\n\nint main() {\n  struct io_uring ring = {0};\n  if (io_uring_queue_init(10, &amp;ring, IORING_SETUP_SINGLE_ISSUER) &lt; 0) {\n    return 1;\n  }\n\n  // Queue `sleep`.\n  struct io_uring_sqe *sqe = NULL;\n  for (int i = 1; i &lt;= 10; i++) {\n    sqe = io_uring_get_sqe(&amp;ring);\n    struct __kernel_timespec ts = {.tv_nsec = i * 50 * 1000 * 1000};\n    io_uring_prep_timeout(sqe, &amp;ts, 1, IORING_TIMEOUT_ETIME_SUCCESS);\n    sqe-&gt;user_data = i;\n    assert(1 == io_uring_submit(&amp;ring));\n  }\n\n  for (int i = 0; i &lt; 10; i++) {\n    struct io_uring_cqe *cqe = NULL;\n\n    int ret = io_uring_wait_cqe(&amp;ring, &amp;cqe);\n    assert(0 == ret);\n    assert(-ETIME == cqe-&gt;res);\n\n    struct timespec now = {0};\n    clock_gettime(CLOCK_REALTIME, &amp;now);\n    printf(&quot;[%ld.%03ld] timer %lld triggered\\n&quot;, now.tv_sec,\n           now.tv_nsec / 1000 / 1000, cqe-&gt;user_data);\n    io_uring_cqe_seen(&amp;ring, cqe);\n  }\n} And it outputs: [1738532785.771] timer 1 triggered\n[1738532785.821] timer 2 triggered\n[1738532785.871] timer 3 triggered\n[1738532785.921] timer 4 triggered\n[1738532785.971] timer 5 triggered\n[1738532786.021] timer 6 triggered\n[1738532786.071] timer 7 triggered\n[1738532786.121] timer 8 triggered\n[1738532786.171] timer 9 triggered\n[1738532786.221] timer 10 triggered All OSes: timers fully implemented in userspace Frustrated by my research, not having found one sane API that exists on all Unices, I wondered: How does libuv , the C library powering all of the asynchronous I/O for NodeJS, do it? I knew they support timers . And they support all OSes, even the most obscure ones like AIX. Surely, they have found the best OS API! Let\'s make a super simple C program using libuv timers (loosely adapted from their test suite): #include &lt;assert.h&gt;\n#include &lt;uv.h&gt;\n\nstatic void once_cb(uv_timer_t *handle) {\n  printf(&quot;timer %#x triggered\\n&quot;, handle);\n}\n\nint main() {\n  uv_timer_t once_timers[10] = {0};\n  int r = 0;\n\n  /* Start 10 timers. */\n  for (int i = 0; i &lt; 10; i++) {\n    r = uv_timer_init(uv_default_loop(), &amp;once_timers[i]);\n    assert(0 == r);\n    r = uv_timer_start(&amp;once_timers[i], once_cb, i * i * 50, 0);\n    assert(0 == r);\n  }\n\n  uv_run(uv_default_loop(), UV_RUN_DEFAULT);\n} We create 10 timers with increasing durations, and run the event loop. When a timer triggers, our callback is called by libuv . Of course, in a real program, we would also do real work while the timers run, e.g. network I/O. Let\'s compile our program and look at what syscalls are being done (here I am on Linux but we\'ll soon seen it does not matter at all): $ cc uv-timers.c -luv\n$ strace ./a.out\n[...]\nepoll_pwait(3, [], 1024, 49, NULL, 8)   = 0\nwrite(1, &quot;timer 0x27432398 triggered\\n&quot;, 27timer 0x27432398 triggered\n) = 27\nepoll_pwait(3, [], 1024, 149, NULL, 8)  = 0\nwrite(1, &quot;timer 0x27432430 triggered\\n&quot;, 27timer 0x27432430 triggered\n) = 27\nepoll_pwait(3, [], 1024, 249, NULL, 8)  = 0\nwrite(1, &quot;timer 0x274324c8 triggered\\n&quot;, 27timer 0x274324c8 triggered\n) = 27\nepoll_pwait(3, [], 1024, 349, NULL, 8)  = 0\nwrite(1, &quot;timer 0x27432560 triggered\\n&quot;, 27timer 0x27432560 triggered\n) = 27\n[...] Huh, no call to timerfd_create or something like this, just... epoll_pwait which is basically just epoll_wait , which is basically just a faster poll . And no events, just a timeout... So... are libuv timers fully implemented in userspace? I was at this moment reminded of a sentence I had read from an Illumos man page: timerfd is a Linux-borne facility for creating POSIX timers and receiving\ntheir subsequent events via a file descriptor.  The facility itself is\narguably unnecessary: portable code can [...] use the timeout value\npresent in poll(2) [...]. So, what libuv does is quite simple in fact: When a timer is created, it is added to an efficient data structure. It\'s a min-heap , i.e. a binary tree that is easy to implement and is designed to get the smallest element in a set quickly. It is typically used to implement priority queues, which is what this bookkeeping of user-space timers really is. A typical event loop tick first gets the current time from the OS. Then, it computes the timeout to pass to poll/epoll/kqueue/etc.  If there are no active timers, it\'s easy, there is no timeout (that means that the program will block indefinitely until some I/O happens). If there are active timers, get the \'smallest\' one, meaning: the first that would trigger. The OS timeout is thus now - timer.value .\nWhenever a timer expires, it is removed from the min-heap. Simple, (relatively) efficient. The only caveat is that epoll only offers a millisecond precision for the timeout parameter so that\'s also the precision of libuv timers. This approach is reminiscent of this part from the man page of select (which is basically poll with more limitations): Emulating usleep(3) Before the advent of usleep(3), some code employed a call to\nselect() with all three sets empty, nfds zero, and a non-NULL\ntimeout as a fairly portable way to sleep with subsecond\nprecision. That way, we can \'sleep\' while we also do meaningful work, for example network I/O. If some I/O completes before a timer triggers, we\'ll get notified by the OS and we can react to it. Then, during the next event loop tick, we\'ll compute a shorter timeout than the first one (since some time elapsed). If no I/O happens at all, the OS will wake us up when our timeout is elapsed. In short, we have multiplexed multiple timers using one system call (and a min-heap to remember what timers are on-going and when will the next one trigger). Addendum : A reader has pointed out that Webkit does exactly the same. Conclusion Writing cross-platform C code typically means writing two code paths: one for Windows and one for Unix. But for multiplexed I/O, and for timers, each Unix has its own idea of what\'s the Right Way(tm). To sum up: OS API Windows macOS Linux FreeBSD NetBSD OpenBSD Illumos SetTimer \u{2713} POSIX timers 1 \u{2713} \u{2713} \u{2713} \u{2713} \u{2713} \u{2713} timerfd \u{2713} \u{2713} \u{2713} \u{2713} kevent timer \u{2713} \u{2713} \u{2713} \u{2713} port_create timer \u{2713} dispatch_source_create \u{2713} io_uring sleep 2 \u{2713} Userspace timers \u{2713} \u{2713} \u{2713} \u{2713} \u{2713} \u{2713} \u{2713} For performant multiplexed I/O, that means that we have to have a code path for each OS (using epoll on Linux, kqueue on macOS and BSDs, event ports on Illumos, I/O completion ports on Windows). For timers, it seems that the easiest approach is to implement them fully in userspace, as long as we have an efficient data structure to manage them. Do not recommend using in non-trivial programs. \u{21a9} Not always enabled. \u{21a9} ",
titles:[
{
title:"Windows: SetTimer",
slug:"windows-settimer",
offset:1465,
},
{
title:"Readers suggestions: CreateWaitableTimer, CreateThreadpoolTimer, CreateTimerQueueTimer",
slug:"readers-suggestions-createwaitabletimer-createthreadpooltimer-createtimerqueuetimer",
offset:1858,
},
{
title:"POSIX: timer_create, timer_settime",
slug:"posix-timer-create-timer-settime",
offset:2660,
},
{
title:"Linux: timerfd_create, timerfd_settime",
slug:"linux-timerfd-create-timerfd-settime",
offset:5875,
},
{
title:"BSD: kqueue/kevent",
slug:"bsd-kqueue-kevent",
offset:9537,
},
{
title:"Illumos: port_create",
slug:"illumos-port-create",
offset:11919,
},
{
title:"macOS: dispatch_source_create",
slug:"macos-dispatch-source-create",
offset:12941,
},
{
title:"Linux: io_uring",
slug:"linux-io-uring",
offset:13926,
},
{
title:"All OSes: timers fully implemented in userspace",
slug:"all-oses-timers-fully-implemented-in-userspace",
offset:16237,
},
{
title:"Conclusion",
slug:"conclusion",
offset:20614,
},
],
},
{
html_file_name:"tip_of_the_day_4.html",
title:"Tip of the day #4: Type annotations on Rust match patterns",
text:"Discussions: /r/rust Today at work I was adding error logs to our Rust codebase and I hit an interesting case. I had a match pattern, and the compiler asked me to add type annotations to a branch of the pattern, because it could not infer by itself the right type. fn decode_foo(input: &amp;[u8]) -&gt; Result&lt;(&amp;[u8], [u8; 33]), Error&gt; {\n    if input.len() &lt; 33 {\n        return Err(Error::InvalidData);\n    }\n\n    let (left, right) = input.split_at(33);\n    let value = match left.try_into() {\n        Ok(v) =&gt; v,\n        Err(err) =&gt; {\n            let err_s = err.to_string(); // &lt;= Here is the compilation error.\n            eprintln!(&quot;failed to decode data, wrong length: {}&quot;, err_s);\n            return Err(Error::InvalidData);\n        }\n    };\n\n    Ok((right, value))\n} error[E0282]: type annotations needed\n  --&gt; src/main.rs:14:25\n   |\n14 |             let err_s = err.to_string();\n   |                         ^^^ cannot infer type This function parses a slice of bytes, and on error, logs the error. The real code is of course more complex but I could reduce the error to this minimal code. So I tried to add type annotations the usual Rust way: let value = match left.try_into() {\n        Ok(v) =&gt; v,\n        Err(err: TryFromSliceError) =&gt; {\n            // [...]\n        }\n    }; Which leads to this nice error: error: expected one of `)`, `,`, `@`, or `|`, found `:`\n  --&gt; src/main.rs:15:16\n   |\n15 |         Err(err: TryFromSliceError) =&gt; {\n   |                ^ expected one of `)`, `,`, `@`, or `|` If you\'re feeling smart, thinking, \'Well that\'s because you did not use inspect_err or map_err !\'. Well they suffer from the exact same problem: a type annotation is needed. However, since they use a lambda, the intuitive type annotation, like the one I tried, works. But not for match . Alright, so after some searching around , I came up with this mouthful of a syntax: let value = match left.try_into() {\n        Ok(v) =&gt; v,\n        Err::&lt;_, TryFromSliceError&gt;(err) =&gt; {\n            // [...]\n        }\n    }; Which works! And the same syntax can be applied to the Ok branch (per the link above) if needed. Note that this is a partial type annotation: we only care about the Err part of the Result type. That was a TIL for me. It\'s a bit of a weird syntax here. It\'s usually the syntax for type annotations on methods (more on that in a second). You can be even more verbose by mentioning the whole type, if you want to: let value = match left.try_into() {\n        Ok(v) =&gt; v,\n        Result::&lt;_, TryFromSliceError&gt;::Err(err) =&gt; {\n          // [...]\n        }\n     }; Anyways, there\'s a much better way to solve this issue. We can simply  annotate the resulting variable outside of the whole match pattern, so that rustc knows which try_into method we are using: let value: [u8; 33] = match left.try_into() {\n        Ok(v) =&gt; v,\n        Err(err) =&gt; {\n          // [...]\n        }\n    }; Or alternatively, as pointed out by a perceptive reader, annotate the err variable inside the body for the Err branch: let value = match left.try_into() {\n        Ok(v) =&gt; v,\n        Err(err) =&gt; {\n            let err: TryFromSliceError = err; // &lt;= Here.\n            let err_s = err.to_string();\n            eprintln!(&quot;failed to decode data, wrong length: {}&quot;, err_s);\n            return Err(Error::InvalidData);\n        }\n    }; Another reader had a different idea: use a match binding, which mentions the error type explicitly (that only works if the error type is a struct): let value = match left.try_into() {\n        Ok(v) =&gt; v,\n        Err(err @ TryFromSliceError { .. }) =&gt; {\n          // [...]\n        }\n     }; Pretty succinct! This reader mentions this PR to expand this to all types, but that the general feedback is that the \'intuitive\' syntax Err(err: Bar) =&gt; { should be possible instead. Yet another approach that works is to annotate the try_into() function with the type, but I find it even noisier than annotating the Err branch: let value = match TryInto::&lt;[u8; 33]&gt;::try_into(left) {\n        Ok(v) =&gt; v,\n        Err(err) =&gt; {\n          // [...]\n        }\n    }; Astute readers will think at this point that all of this is unnecessary: let\'s just have the magic traits(tm) do their wizardry. We do not convert the error to a string, we simply let eprintln! call err.fmt() under the hood, since TryFromSliceError implements the Display trait (which is why we could convert it to a String with .to_string() ): let value = match left.try_into() {\n        Ok(v) =&gt; v,\n        Err(err) =&gt; {\n            eprintln!(&quot;failed to decode data, wrong length: {}&quot;, err);\n            return Err(Error::InvalidData);\n        }\n    }; That works but in my case I really needed to convert the error to a String , to be able to pass it to C, which does not know anything about fancy traits. I find this issue interesting because it encapsulates well the joy and pain of writing Rust: match patterns are really handy, but they sometimes lead to weird syntax not found elsewhere in the Rust language (maybe due to the OCaml heritage?). Type inference is nice but sometimes the compiler/language server fails at inferring things you\'d think they should really be able to infer. Traits and into/try_into are found everywhere in Rust code, but it\'s hard to know what type is being converted to what, especially when these are chained several times without any type annotation whatsoever. By the way, here\'s a tip I heard some time ago: if you want to know the real type of a variable that\'s obscured by type inference, just add a type annotation that\'s obviously wrong, and the compiler will show the correct type. That\'s how I pinpointed the TryFromSliceError type. Let\'s add a bogus bool type annotation: let value = match left.try_into() {\n        Ok(v) =&gt; v,\n        Err::&lt;_, bool&gt;(err) =&gt; {\n          // [...]\n        }\n    }; And the compiler helpfully gives us the type: error[E0271]: type mismatch resolving `&lt;[u8; 33] as TryFrom&lt;&amp;[u8]&gt;&gt;::Error == bool`\n  --&gt; src/main.rs:11:28\n   |\n11 |     let value = match left.try_into() {\n   |                            ^^^^^^^^ expected `bool`, found `TryFromSliceError` So...it does actually know the type of err ... You naughty compiler, playing games with me! It reminds me of this picture: Coffee or tea? ",
titles:[
],
},
{
html_file_name:"addressing_cgo_pains_one_at_a_time.html",
title:"Addressing CGO pains, one at a time",
text:"Rust? Go? Cgo! I maintain a Go codebase at work which does most of its work through a Rust library that exposes a C API. So they interact via Cgo, Go\'s FFI mechanism. And it works! Also, Cgo has many weird limitations and surprises. Fortunately, over the two years or so I have been working in this project, I have (re-)discovered solutions for most of these issues. Let\'s go through them, and hopefully the next time you use Cgo, you\'ll have a smooth experience. From Go\'s perspective, Rust is invisible, the C library looks like a pure C library (and indeed it used to be 100% C++ before it got incrementally rewritten to Rust). So I will use C snippets in this article, because that\'s what the public C header of the library looks like, and not everybody knows Rust, but most people know a C-like language. But worry not, I will still talk at lengths about Rust in the last section. Let\'s create a sample app: .\n\u{251c}\u{2500}\u{2500} app\n\u{2502}\u{a0}\u{a0} \u{2514}\u{2500}\u{2500} app.go\n\u{251c}\u{2500}\u{2500} c\n\u{2502}\u{a0}\u{a0} \u{251c}\u{2500}\u{2500} api.c\n\u{2502}\u{a0}\u{a0} \u{251c}\u{2500}\u{2500} api.h\n\u{2502}\u{a0}\u{a0} \u{251c}\u{2500}\u{2500} api.o\n\u{2502}\u{a0}\u{a0} \u{251c}\u{2500}\u{2500} libapi.a\n\u{2502}\u{a0}\u{a0} \u{2514}\u{2500}\u{2500} Makefile\n\u{251c}\u{2500}\u{2500} go.mod\n\u{2514}\u{2500}\u{2500} main.go The C code is in the c directory, we build a static library libapi.a from it. The public header file is api.h . The Go code then links this library. The full code can be found at the end of this article. CGO does not have unions This is known to Go developers: Go does not have tagged unions, also called sum types, algebraic data types, etc. But C, and Rust, do have them, and Go needs to generate Go types for each C type, so that we can use them! So what does it do? Let\'s have a look. So, here is a (very useful) C tagged union: // c/api.h\n\n#pragma once\n#include &lt;stdint.h&gt;\n\ntypedef struct {\n  char *data;\n  uint64_t len;\n} String;\n\ntypedef enum {\n  ANIMAL_KIND_DOG,\n  ANIMAL_KIND_CAT,\n} AnimalKind;\n\ntypedef struct {\n  AnimalKind kind;\n  union {\n    String cat_name;   // Only for `ANIMAL_KIND_CAT`.\n    uint16_t dog_tail; // Only for `ANIMAL_KIND_DOG`.\n  };\n} Animal;\n\nAnimal animal_make_dog();\n\nAnimal animal_make_cat();\n\nvoid animal_print(Animal *animal); The C implementation is straightforward: // c/api.c\n\n#include &quot;api.h&quot;\n#include &lt;assert.h&gt;\n#include &lt;inttypes.h&gt;\n#include &lt;stdio.h&gt;\n\nAnimal animal_make_dog() {\n  return (Animal){\n      .kind = ANIMAL_KIND_DOG,\n      .dog_tail = 42,\n  };\n}\n\nAnimal animal_make_cat() {\n  return (Animal){\n      .kind = ANIMAL_KIND_CAT,\n      .cat_name =\n          {\n              .data = &quot;kitty&quot;,\n              .len = 5,\n          },\n  };\n}\n\nvoid animal_print(Animal *animal) {\n  switch (animal-&gt;kind) {\n  case ANIMAL_KIND_DOG:\n    printf(&quot;Dog: %&quot; PRIu16 &quot;\\n&quot;, animal-&gt;dog_tail);\n    break;\n  case ANIMAL_KIND_CAT:\n    printf(&quot;Cat: %.*s\\n&quot;, (int)animal-&gt;cat_name.len, animal-&gt;cat_name.data);\n    break;\n  default:\n    assert(0 &amp;&amp; &quot;unreachable&quot;);\n  }\n} And here\'s how we use it in Go: // app/app.go\n\npackage app\n\n// #cgo CFLAGS: -g -O2 -I${SRCDIR}/../c/\n// #cgo LDFLAGS: ${SRCDIR}/../c/libapi.a\n// #include &lt;api.h&gt;\nimport &quot;C&quot;\nimport &quot;fmt&quot;\n\nfunc DoStuff() {\n\tdog := C.animal_make_dog()\n\tC.animal_print(&amp;dog)\n\n\tcat := C.animal_make_cat()\n\tC.animal_print(&amp;cat)\n} So far, so good. Let\'s run it (our main.go simply calls app.DoStuff() ): $ go run .\nDog: 42\nCat: kitty Great! Now, let\'s say we want to access the fields of the C tagged union. We can to have some logic based on whether our cat\'s name is greater than a limit, say 255? What does the Go struct look like for Animal ? type _Ctype_struct___0 struct {\n\tkind\t_Ctype_AnimalKind\n\t_\t[4]byte\n\tanon0\t[16]byte\n} So it\'s a struct with a kind field, so far so good. Then comes 4 bytes of padding, as expected (the C struct also has them). But then, we only see 16 opaque bytes. The size is correct: the C union is the size of its largest member which is 16 bytes long ( String ). But then, how do we access String.len ? Here\'s the very tedious way, by hand: // app/app.go\nfunc DoStuff() {\n    // [...]\n\n\tcat_ptr := unsafe.Pointer(&amp;cat)\n\tcat_name_ptr := unsafe.Add(cat_ptr, 8)\n\tcat_name_len_ptr := unsafe.Add(cat_name_ptr, 8)\n\tfmt.Println(*(*C.uint64_t)(cat_name_len_ptr))\n} And we get: $ go run .\nDog: 42\nCat: kitty\n5 Ok, we are a C compiler now. Back to computing fields offsets by hand! I sure hope you do not forget about alignment! And keep the offsets in sync with the C struct when its layout changes! Well we all agree this sucks, but that\'s all what the unsafe package offers. Cherry on the cake, every pointer in this code has the same type: unsafe.Pointer , even though the first one really is a Animal* , the second one is a String* , and the third one is a uint64_t* . Not great. So the solution is: treat C unions as opaque values in Go, and only access them with C functions (essentially, getters and setters): // c/api.h\n\nuint16_t animal_dog_get_tail(Animal *animal);\n\nString animal_cat_get_name(Animal *animal); // c/api.c\n\nuint16_t animal_dog_get_tail(Animal *animal) {\n  assert(ANIMAL_KIND_DOG == animal-&gt;kind);\n  return animal-&gt;dog_tail;\n}\n\nString animal_cat_get_name(Animal *animal) {\n  assert(ANIMAL_KIND_CAT == animal-&gt;kind);\n  return animal-&gt;cat_name;\n} And now we have a sane Go code: // app/app.go\nfunc DoStuff() {\n    // [...]\n\n\tcat_name := C.animal_cat_get_name(&amp;cat)\n\tfmt.Println(cat_name.len)\n} And as a bonus, whenever the layout of Animal changes, for example the order of fields gets changed, or a new field gets added which changes the alignment and thus the padding (here it\'s not the case because the alignment is already 8 which is the maximum, but in other cases it could happen), the C code gets recompiled, it does the right thing automatically, and everything works as expected. My recommendation: never role-play as a compiler, just use getters and setters for unions and let the C compiler do the dirty work. My ask to the Go team: mention the approach with getters and setters in the docs. The only thing the docs have to say about unions right now is: As Go doesn\'t have support for C\'s union type in the general case, C\'s union types are represented as a Go byte array with the same length . And I don\'t expect Go to have (tagged) unions anytime soon, so that\'s the best we can do. Slices vs Strings Quick Go trivia question: what\'s the difference between []byte (a slice of bytes) and string (which is a slice of bytes underneath)? ... The former is mutable while the latter is immutable.\nYes, I might have learned that while writing this article. Anyways, converting C slices (pointer + length) to Go is straightforward using the unsafe package in modern Go (it used to be much hairier in older Go versions): // app/app.go\nfunc DoStuff() {\n    // [...]\n\n\tcat_name := C.animal_cat_get_name(&amp;cat)\n\tslice := unsafe.Slice(cat_name.data, cat_name.len)\n\tstr := unsafe.String((*byte)(unsafe.Pointer(cat_name.data)), cat_name.len)\n\n\tfmt.Println(slice)\n\tfmt.Println(str)\n} And it does what we expect: $ go run .\nDog: 42\nCat: kitty\n[107 105 116 116 121]\nkitty Ok, but just reading them is boring, let\'s try to mutate them. First, we need to allocate a fresh string in C, otherwise the string constant will be located in the read-only part of the executable, mapped to read-only page, and we will segfault when trying to mutate it. So we modify animal_make_cat : // c/api.c\n\nAnimal animal_make_cat() {\n  return (Animal){\n      .kind = ANIMAL_KIND_CAT,\n      .cat_name =\n          {\n              .data = strdup(&quot;kitty&quot;), // &lt;= Heap allocation here.\n              .len = 5,\n          },\n  };\n} Let\'s mutate all the things! // app/app.go\nfunc DoStuff() {\n    // [...]\n\n\tslice[0] -= 32 // Poor man\'s uppercase.\n\tfmt.Println(slice)\n\tfmt.Println(str)\n} And we get the additional output: [75 105 116 116 121]\nKitty But wait, this is undefined behavior! The string did get mutated! The Go compiler generates code based on the assumption that strings are immutable, so our program may break in very unexpected ways. The docs for unsafe.String state: Since Go strings are immutable, the bytes passed to String\nmust not be modified as long as the returned string value exists. Maybe the runtime Cgo checks will detect it? $ GODEBUG=cgocheck=1 go run .\n[...]\n[75 105 116 116 121]\nKitty\n# Works fine!\n\n$ GOEXPERIMENT=cgocheck2 go run . \n[...]\n[75 105 116 116 121]\nKitty\n# Works fine! Nope... so what can we do about it? In my real-life program I have almost no strings to deal with, but some programs will. My recommendation: In Go, do not use unsafe.String , just use unsafe.Slice and accept that it\'s mutable everywhere in the program If you really want to use unsafe.String , make sure that the string data returned by the C code is immutable, enforced by the OS , so either: It\'s a constant string placed in the read-only segment The string data is allocated in its own virtual memory page and the page permissions are changed to read-only before returning the pointer to Go In C, do not expose string data directly to Go, only expose opaque values ( void* ), and mutations are only done by calling a C function. That way, the Go caller simply cannot use unsafe.String (I guess they could with lots of casts, but that\'s not in the realm of a honest mistake anymore). My ask to the Go team: attempt to develop more advanced checks to detect this issue at runtime. Test a C function in Go tests We are principled programmers who write tests. Let\'s write a test to ensure that animal_make_dog() does indeed create a dog, i.e. the kind is ANIMAL_KIND_DOG : // app/app_test.go\n\npackage app\n\nimport &quot;testing&quot;\nimport &quot;C&quot;\n\nfunc TestAnimalMakeDog(t *testing.T) {\n\tdog := C.animal_make_dog()\n\t_ = dog\n} Let\'s run it: $ go test ./app/\nuse of cgo in test app_test.go not supported Ah... yeah this is a known limitation : _test.go files can\u{2019}t use cgo. . Solution: wrap the C function in a Go one. // app/app.go\n\nfunc AnimalDogKind() int {\n\treturn C.ANIMAL_KIND_DOG\n}\n\nfunc AnimalMakeDog() C.Animal {\n\treturn C.animal_make_dog()\n} And we now have a passing Go test: // app/app_test.go\n\npackage app\n\nimport &quot;testing&quot;\n\nfunc TestAnimalMakeDog(t *testing.T) {\n\tdog := AnimalMakeDog()\n\tif int(dog.kind) != AnimalDogKind() {\n\t\tpanic(&quot;wrong kind&quot;)\n\t}\n} $ go test ./app/ -count=1 -v\n=== RUN   TestAnimalMakeDog\n--- PASS: TestAnimalMakeDog (0.00s)\nPASS\nok  \tcgo/app\t0.003s So... that works, and also: that\'s annoying boilerplate that no one wants to have to write. And if you\'re feeling smug, thinking your favorite LLM will do the right thing for you, I can tell you I tried and the LLM generated the wrong thing, with the test trying to use Cgo directly. My recommendation: Test the C code in C directly (or Rust, or whatever language it is) There is some glue code sometimes that is only useful to the Go codebase, and that\'s written in C. In that case wrap each C utility function in a Go one and write a Go test, hopefully it\'s not that much. My ask to the Go team: let\'s allow the use of Cgo in tests. The Go compiler does not detect changes People are used to say: Go builds so fast! And yes, it\'s not a slow compiler, but if you have ever built the Go compiler from scratch, you will have noticed it takes a significant amount of time still. What Go is really good at, is caching: it\'s really smart at detecting what changed, and only rebuilding that. And that\'s great! Until it isn\'t. Sometimes, changes to the Cgo build flags, or to the .a library, were not detected by Go. I could not really reproduce these issues reliably, but they do happen. Solution: force a clean build with go build -a . False positive warnings Sometimes we need to run some C code once at startup, when the package gets initialized: // app/app.go\n\npackage app\n\n// #cgo CFLAGS: -g -O2 -I${SRCDIR}/../c/\n// #cgo LDFLAGS: ${SRCDIR}/../c/libapi.a\n// #include &lt;api.h&gt;\n// void initial_setup();\nimport &quot;C&quot;\n\nfunc init() {\n\tC.initial_setup()\n}\n\n\n[...] And the C function initial_setup is defined in a second file in the same Go package (this is not strictly necessary but it will turn out to be useful to showcase something later): // app/cfuncs.go\n\npackage app\n\n/*\nvoid initial_setup(){\n    // Do some useful stuff.\n}\n*/\nimport &quot;C&quot; Yes, we can write C code directly in Go files. Inside comments. Not, it\'s not weird at all. We build, everything is fine: $ go build . Since we are serious programmers, we want to enable C warnings, right? Let\'s add -Wall to the CFLAGS : // app/app.go\n\n[...]\n// #cgo CFLAGS: -Wall -g -O2 -I${SRCDIR}/../c/    &lt;= We add -Wall\n[...] We re-build, and get this nice error: $ go build .\n# cgo/app\ncgo-gcc-prolog: In function \u{2018}_cgo_13d20cc583d0_Cfunc_initial_setup\u{2019}:\ncgo-gcc-prolog:78:49: warning: unused variable \u{2018}_cgo_a\u{2019} [-Wunused-variable] Wait, we do not have any variable in initial_setup , how come a variable is unused? Some searching around turns up this Github issue where the official recommendation is: do not use -Wall , it creates false positives. Ok. My recommendation: Write C code in C files and enable all the warnings you want. My ask to the Go team: Let\'s please fix the false positives and allow people to enable some basic warnings. -Wall is the bare minimum! White space is significant Let\'s go back to the app/cfuncs.go we just created that builds fine: package app\n\n/*\nvoid initial_setup(){}\n*/\nimport &quot;C&quot; Let\'s add one empty line near the end: package app\n\n/*\nvoid initial_setup(){}\n*/\n\nimport &quot;C&quot; Let\'s build: $ go build .\n# cgo\n/home/pg/Downloads/go/pkg/tool/linux_amd64/link: running gcc failed: exit status 1\n/usr/bin/gcc -m64 -o $WORK/b001/exe/a.out -Wl,--export-dynamic-symbol=_cgo_panic -Wl,--export-dynamic-symbol=_cgo_topofstack -Wl,--export-dynamic-symbol=crosscall2 -Wl,--compress-debug-sections=zlib /tmp/go-link-2748897775/go.o /tmp/go-link-2748897775/000000.o /tmp/go-link-2748897775/000001.o /tmp/go-link-2748897775/000002.o /tmp/go-link-2748897775/000003.o /tmp/go-link-2748897775/000004.o /tmp/go-link-2748897775/000005.o /tmp/go-link-2748897775/000006.o /tmp/go-link-2748897775/000007.o /tmp/go-link-2748897775/000008.o /tmp/go-link-2748897775/000009.o /tmp/go-link-2748897775/000010.o /tmp/go-link-2748897775/000011.o /tmp/go-link-2748897775/000012.o /tmp/go-link-2748897775/000013.o /tmp/go-link-2748897775/000014.o /tmp/go-link-2748897775/000015.o /tmp/go-link-2748897775/000016.o -O2 -g /home/pg/scratch/cgo/app/../c/libapi.a -O2 -g -lpthread -no-pie\n/usr/bin/ld: /tmp/go-link-2748897775/000001.o: in function `_cgo_f1a74d84225f_Cfunc_initial_setup\':\n/tmp/go-build/cgo-gcc-prolog:80:(.text+0x53): undefined reference to `initial_setup\'\ncollect2: error: ld returned 1 exit status Ok... not much to say here. Here\'s another example. We add a comment about not using -Wall : // app/app.go\n\npackage app\n\n// NOTE: Do not use -Wall.\n// #cgo CFLAGS: -g -O2 -I${SRCDIR}/../c/\n// #cgo LDFLAGS: ${SRCDIR}/../c/libapi.a\n// #include &lt;api.h&gt;\n// void initial_setup();\nimport &quot;C&quot;\n\n[...] We rebuild, and boom: $ go build .\n# cgo/app\napp/app.go:3:6: error: expected \'=\', \',\', \';\', \'asm\' or \'__attribute__\' before \':\' token\n    3 | // NOTE: Do not use -Wall.\n      |      ^ That\'s because when seeing a #cgo directive in the comments, the Go compiler parses what it recognizes, passes the rest to the C compiler, which chokes on it. Solution: insert a blank line between the comment and the #cgo directive to avoid that: // app/app.go\n\npackage app\n\n// NOTE: Do not use -Wall.\n\n// #cgo CFLAGS: -g -O2 -I${SRCDIR}/../c/\n// #cgo LDFLAGS: ${SRCDIR}/../c/libapi.a\n// #include &lt;api.h&gt;\n// void initial_setup();\nimport &quot;C&quot;\n\n[...] My recommendation: If you get a hairy and weird error, compare the white space with official code examples. My ask to the Go team: Can we please fix this? Or at least document it? There is zero mention of this pitfall anywhere, as far as I can see. Debug Go and C/Rust It\'s very simple, you have two exclusive choices: Use a C/Rust debugger (e.g gdb, lldb, etc), which does not understand the Go calling convention so you can see the C/Rust call stack, but it ends at the Cgo FFI boundary, or Use a Go debugger (e.g. delve) which does not (really) understand the C/Rust calling convention so you can see the Go stack, but it ends at the Cgo FFI boundary Let\'s see it for ourselves: $ gdb ./cgo\n(gdb) break animal_print\nBreakpoint 1 at 0x469b6c\n(gdb) run\n[...]\nThread 1 &quot;cgo&quot; hit Breakpoint 1, 0x0000000000469b6c in animal_print ()\n(gdb) backtrace\n#0  0x0000000000469b6c in animal_print ()\n#1  0x0000000000464644 in runtime.asmcgocall () at /home/pg/Downloads/go/src/runtime/asm_amd64.s:923\n#2  0x000000c0000061c0 in ?? ()\n#3  0x0000000000462a0a in runtime.systemstack () at /home/pg/Downloads/go/src/runtime/asm_amd64.s:514\n#4  0x00007fffffffe228 in ?? ()\n#5  0x0000000000466f3f in runtime.newproc (fn=0x46288f &lt;runtime.rt0_go+303&gt;) at &lt;autogenerated&gt;:1\n#6  0x0000000000462905 in runtime.mstart () at /home/pg/Downloads/go/src/runtime/asm_amd64.s:395\n#7  0x000000000046288f in runtime.rt0_go () at /home/pg/Downloads/go/src/runtime/asm_amd64.s:358\n#8  0x0000000000000001 in ?? ()\n#9  0x00007fffffffe388 in ?? ()\n#10 0x00007fffffffe340 in ?? ()\n#11 0x0000000000000001 in ?? ()\n#12 0x00007fffffffe388 in ?? ()\n#13 0x00007ffff7db1248 in __libc_start_call_main (main=0x300000002, argc=192, argv=0x43a3c5 &lt;runtime.reentersyscall+165&gt;) at ../sysdeps/nptl/libc_start_call_main.h:58\nBacktrace stopped: previous frame inner to this frame (corrupt stack?) Where is app.DoStuff ? Where is main ? Probably around frames 8-13 in the corrupt stack ... Now with delve : $ go build   -gcflags=all=&quot;-N -l&quot;\n$ dlv exec ./cgo\n(dlv) b animal_print\nCommand failed: could not find function C.animal_print\n\n(dlv) b app.DoStuff\nBreakpoint 1 set at 0x471f2a for cgo/app.DoStuff() ./app/app.go:23\n(dlv) c\n&gt; [Breakpoint 1] cgo/app.DoStuff() ./app/app.go:23 (hits goroutine(1):1 total:1) (PC: 0x471f2a)\n    18:\t\n    19:\tfunc AnimalMakeDog() C.Animal {\n    20:\t\treturn C.animal_make_dog()\n    21:\t}\n    22:\t\n=&gt;  23:\tfunc DoStuff() {\n    24:\t\tdog := C.animal_make_dog()\n    25:\t\tC.animal_print(&amp;dog)\n    26:\t\n    27:\t}\n(dlv) s\n&gt; cgo/app.DoStuff() ./app/app.go:24 (PC: 0x471f2e)\n    19:\tfunc AnimalMakeDog() C.Animal {\n    20:\t\treturn C.animal_make_dog()\n    21:\t}\n    22:\t\n    23:\tfunc DoStuff() {\n=&gt;  24:\t\tdog := C.animal_make_dog()\n    25:\t\tC.animal_print(&amp;dog)\n    26:\t\n    27:\t}\n(dlv) \n&gt; cgo/app._Cfunc_animal_make_dog() _cgo_gotypes.go:79 (PC: 0x471d93)\n(dlv) \n&gt; cgo/app._Cfunc_animal_make_dog() _cgo_gotypes.go:80 (PC: 0x471dab)\n(dlv) \n&gt; cgo/app._Cfunc_animal_make_dog() _cgo_gotypes.go:81 (PC: 0x471dd3)\n(dlv) \n&gt; cgo/app._Cfunc_animal_make_dog() _cgo_gotypes.go:83 (PC: 0x471de4)\n(dlv) \n&gt; cgo/app.DoStuff() ./app/app.go:24 (PC: 0x471f45)\nValues returned:\n\tr1: cgo/app._Ctype_struct___0 {\n\t\tkind: 0,\n\t\t_: [4]uint8 [0,0,0,0],\n\t\tanon0: [16]uint8 [42,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],}\n\n    19:\tfunc AnimalMakeDog() C.Animal {\n    20:\t\treturn C.animal_make_dog()\n    21:\t}\n    22:\t\n    23:\tfunc DoStuff() {\n=&gt;  24:\t\tdog := C.animal_make_dog()\n    25:\t\tC.animal_print(&amp;dog)\n    26:\t\n    27:\t}\n(dlv) \n&gt; cgo/app.DoStuff() ./app/app.go:25 (PC: 0x471f67)\n    20:\t\treturn C.animal_make_dog()\n    21:\t}\n    22:\t\n    23:\tfunc DoStuff() {\n    24:\t\tdog := C.animal_make_dog()\n=&gt;  25:\t\tC.animal_print(&amp;dog)\n    26:\t\n    27:\t}\n(dlv) \n&gt; cgo/app.DoStuff() ./app/app.go:25 (PC: 0x471f67)\n    20:\t\treturn C.animal_make_dog()\n    21:\t}\n    22:\t\n    23:\tfunc DoStuff() {\n    24:\t\tdog := C.animal_make_dog()\n=&gt;  25:\t\tC.animal_print(&amp;dog)\n    26:\t\n    27:\t}\n(dlv) s\n&gt; cgo/app._Cfunc_animal_print() _cgo_gotypes.go:91 (PC: 0x471e13)\n(dlv) s\n&gt; cgo/app._Cfunc_animal_print() _cgo_gotypes.go:92 (PC: 0x471e17)\n(dlv) stack\n0  0x0000000000471e17 in cgo/app._Cfunc_animal_print\n   at _cgo_gotypes.go:92\n1  0x0000000000471f75 in cgo/app.DoStuff\n   at ./app/app.go:25\n2  0x000000000047228f in main.main\n   at ./main.go:6\n3  0x0000000000437a07 in runtime.main\n   at /home/pg/Downloads/go/src/runtime/proc.go:272\n4  0x000000000046c301 in runtime.goexit\n   at /home/pg/Downloads/go/src/runtime/asm_amd64.s:1700 So the Go debugger does not understand C/Rust debugging information, so as soon as we are inside a C/Rust function, it cannot show anything useful, but it can still kind of understand the call stack...Urgh. And the docs acknowledge that: GDB does not understand Go programs well. The stack management, threading, and runtime contain aspects that differ enough from the execution model GDB expects that they can confuse the debugger and cause incorrect results even when the program is compiled with gccgo. As a consequence, although GDB can be useful in some situations (e.g., debugging Cgo code, or debugging the runtime itself), it is not a reliable debugger for Go programs, particularly heavily concurrent ones. Moreover, it is not a priority for the Go project to address these issues, which are difficult. And it\'s not an issue of missing debugging information, I compiled the C code with -g , or -g3 . This point is close to being a deal-breaker for me. Debugging is really important! A language/tech stack is IMHO only as good as we developers can understand and troubleshoot production applications. Can\'t debug, can\'t pinpoint where the bug is? Not sure if that program is worth much. My recommendation: : Have both debuggers at hand and locate where the problem is: is it on the Go side or on the C/Rust side? Then use the right debugger to inspect local variables and such. Yes, it\'s a pity. I guess you can try to build Go code with Gccgo, perhaps gdb understands the full call stack then? My approach was to insert logs everywhere in the code, both Go and Rust, with logs. Not ideal. It\'s a bit too close to \'printf debugging\' to my taste. My ask for the Go team: : Well, ideally both debuggers would work fully with CGO. But since this issue is known for years...I don\'t have much hope. Strace, bpftrace It\'s the same issue manifesting in a different way: it\'s not just debuggers than do not understand the call stack, it\'s also strace : $ strace -k -e write ./cgo\n--- SIGURG {si_signo=SIGURG, si_code=SI_TKILL, si_pid=438876, si_uid=1000} ---\n &gt; /usr/lib64/libc.so.6(pthread_sigmask@GLIBC_2.2.5+0x48) [0x783b8]\n &gt; /home/pg/scratch/cgo/cgo(_cgo_sys_thread_start+0x7e) [0x4727ee]\n &gt; /home/pg/scratch/cgo/cgo(runtime.asmcgocall.abi0+0x9c) [0x46c01c]\nwrite(1, &quot;Dog: 42\\n&quot;, 8Dog: 42\n)                = 8\n &gt; /usr/lib64/libc.so.6(__write+0x4d) [0xe853d]\n &gt; /usr/lib64/libc.so.6(_IO_file_write@@GLIBC_2.2.5+0x34) [0x68fa4]\n &gt; /usr/lib64/libc.so.6(new_do_write+0x5c) [0x6711c]\n &gt; /usr/lib64/libc.so.6(_IO_do_write@@GLIBC_2.2.5+0x20) [0x67fb0]\n &gt; /usr/lib64/libc.so.6(_IO_file_overflow@@GLIBC_2.2.5+0x11a) [0x6852a]\n &gt; /usr/lib64/libc.so.6(_IO_default_xsputn+0x74) [0x6a624]\n &gt; /usr/lib64/libc.so.6(_IO_file_xsputn@@GLIBC_2.2.5+0x117) [0x69127]\n &gt; /usr/lib64/libc.so.6(__printf_buffer_flush_to_file+0xc8) [0x36448]\n &gt; /usr/lib64/libc.so.6(__printf_buffer_to_file_done+0x1b) [0x3650b]\n &gt; /usr/lib64/libc.so.6(__vfprintf_internal+0xaa) [0x41bea]\n &gt; /usr/lib64/libc.so.6(printf+0xb2) [0x35bf2]\n &gt; /home/pg/scratch/cgo/cgo(animal_print+0x38) [0x472da0]\n &gt; /home/pg/scratch/cgo/cgo(runtime.asmcgocall.abi0+0x63) [0x46bfe3]\n &gt; No DWARF information found\n+++ exited with 0 +++ Same as gdb, the call stack stops at the Cgo FFI boundary. But surprisingly, bpftrace seems to work: $ sudo bpftrace -e \'uprobe:./cgo:animal_print {print(ustack(perf, 64))}\' -c ./cgo\nAttaching 1 probe...\nDog: 42\n\n\t472d68 animal_print+0 (./cgo)\n\t46bfe4 runtime.asmcgocall.abi0+100 (./cgo)\n\t46361f runtime.cgocall+127 (./cgo)\n\t471e3f cgo/app._Cfunc_animal_print.abi0+63 (./cgo)\n\t471f75 cgo/app.DoStuff+85 (./cgo)\n\t47228f main.main+15 (./cgo)\n\t437a07 runtime.main+583 (./cgo)\n\t46c301 runtime.goexit.abi0+1 (./cgo) So, let\'s use bpftrace , I guess. Cross-compile So picture me, building my Go program (a web service) using Cgo. Locally, it builds very quickly, due to Rust and Go caching. Now, time to build in Docker to be able to test my changes: $ time docker build [...]\n[...]\nTotal execution time 101.664413146 That\'s in seconds. Every. Single. Time. Urgh. The issue is that little to no caching is being leveraged by either compiler. Dependencies are fetched from the internet, and essentially, a clean release build is performed. That takes a looong time. Past developers tried to fix this by volume mounting host directories inside Docker but apparently, they missed a few. That\'s why I am convinced that Docker is not meant to build stuff. Only to run stuff. I have seen the same problem with C++ codebases being built in Docker, with Rust codebases being built in Docker, etc. Even with JavaScript/Typescript codebases built in docker (these were for some reason often the slowest to build). In the worst cases it would take over an hour to build, and developers resorted to duplicate the deployment setup to be able to run (and thus test) the app locally, completely bypassing Docker. I think the original intent to do a clean build inside Docker was because developers feared that the local environment was somehow tainted and/or that the compiler would mess up with the caching and result in a borked build. And that was the case I believe, with most C/C++ codebase relying on globally installed libraries built how-knows-how. But modern compilers are, from my perspective, really good at identifying changes, correctly caching what did not change, and rebuilding with the correct build flags, what does need to be rebuilt. And developers have improved on the Bill Of Material side. Much work has been done on existing tools to get reproducible builds. New tools have emerged, like Nix, that focus on controlled reproducible builds. So in my opinion, the ideal Docker build process is: a single static executable is built locally, relying on caching of previous builds (or possibly in CI, remote intermediate artifacts). Then, it is copied inside the image which, again ideally, for security purposes, is very bare bone. The dockerfile can look like this: FROM gcr.io/distroless/static:nonroot\nUSER nonroot\nWORKDIR /home/nonroot\n\nCOPY --chown=nonroot:nonroot app.exe .\n\nCMD [&quot;/home/nonroot/app.exe&quot;] It\'s fast, simple, secure. But, to make it work, regardless of the host, we need to cross-compile. Go is praised for its uncomplicated cross-compiling support. But this goes out of the window when Cgo is enabled. Let\'s try: $ GOOS=linux GOARCH=arm go build  .\ncgo/app: build constraints exclude all Go files in /home/pg/scratch/cgo/app It fails. But fortunately, Go still supports cross-compiling with Cgo as long as we provide it with a cross-compiler. After some experimentation, my favorite way is to use Zig for that. That way it works the same way for people using macOS, Linux, be it on ARM, on x86_64, etc. And it makes it trivial to build native Docker images for ARM without changing the whole build system or installing additional tools. The work on Zig is fantastic, please consider supporting them! So, how does it look like? Let\'s assume we want to target x86_64-linux-musl , built statically, since we use a distroless image that does not come with a libc. The benefit is that our service looks like any other Go service without Cgo. We could also target a specific glibc version and deploy on a LTS version of Ubuntu, Debian, etc. Zig supports that. First, we cross-compile our C code: $ CC=&quot;zig cc --target=x86_64-linux-musl&quot; make -C ./c Or, bring your own cross-compiler, you don\'t have to use Zig: $ CC=musl-gcc make -C ./c If we have Rust code, we do instead: $ rustup target add x86_64-unknown-linux-musl\n$ cargo build --release --all-features --target=x86_64-unknown-linux-musl Then we build our Go code using the Zig C compiler. I put the non cross-compiling build commands just before for comparison: $ go build .\n$ file cgo\ncgo: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=9d5da9b6a211c5635a83e4a8a346ff605f7b6e3b, for GNU/Linux 3.2.0, with debug_info, not stripped\n\n$ CGO_ENABLED=1 CC=\'zig cc --target=x86_64-linux-musl -static\' GOOS=linux GOARCH=amd64 go build .\n$ file cgo\ncgo: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, Go BuildID=gTeiH1YL9FSvJJ2euuGd/P0s07MkwoQlcaBA6EXDD/NOYPaeKuxUZ0cnLxfpC9/YeAu_C7s53nGjPEKZDYI, with debug_info, not stripped Ta dam! Time to build a native ARM image? No problem: $ CC=&quot;zig cc --target=aarch64-linux-musl&quot; make -C ./c\n$ CGO_ENABLED=1 CC=\'zig cc --target=aarch64-linux-musl -static\' GOOS=linux GOARCH=arm64 go build .\n$ file ./cgo\n./cgo: ELF 64-bit LSB executable, ARM aarch64, version 1 (SYSV), statically linked, Go BuildID=QRDa72MrAj44K3mt54PK/_aJwgCwTO37mKpnfElWN/0TEmGFNLMEZCx3Zv_PKs/lgDlIHFQ6-LxhCOsdhQI, with debug_info, not stripped If you\'ve done any work with cross-compilation, you know that this is magic. It\'s supposed to take weeks to make it work, dammit! Note: Rust code typically needs libunwind for printing a backtrace, so it needs to be cross-compiled and linked as well. But no worries, Zig has you covered, it ships with libunwind sources and will, just like with musl, build it for your target, and cache the results! Just add -lunwind to the zig cc invocation, and voila. Oh, and what about the speed now? Here is a full Docker build with my real-life program (Rust + Go): $ make docker-build\nExecuted in    1.47 secs That time includes cargo build --release , go build , and docker build . Most of the time is spent copying the giant executable (around 72 MiB!) into the Docker image since neither Rust nor Go are particularly good at producing small executables. So, we went from ~100s to ~1s, roughly a 100x improvement. Pretty pretty good if you ask me. My recommendation: : Never build in Docker if you can avoid it. Build locally and copy the one static executable into the Docker image. My ask for the Go team : None actually, they have done an amazing job on the build system to support this use-case, and on the documentation. Conclusion Cgo is rocky, but there are no real blocking issues, apart from the debugging pain point, mostly lots of small pains. Half the cure is being aware of the ailment, as the saying goes. So armed with this knowledge, I wish you god speed with your Cgo projects! Addendum: the full code The full code c/Makefile: libapi.a: api.o\n\t$(AR) -rcs libapi.a api.o\n\napi.o: api.c\n\t$(CC) $(CFLAGS) api.c -c c/api.c: #include &quot;api.h&quot;\n#include &lt;assert.h&gt;\n#include &lt;inttypes.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n\nAnimal animal_make_dog() {\n  return (Animal){\n      .kind = ANIMAL_KIND_DOG,\n      .dog_tail = 42,\n  };\n}\n\nAnimal animal_make_cat() {\n  return (Animal){\n      .kind = ANIMAL_KIND_CAT,\n      .cat_name =\n          {\n              .data = strdup(&quot;kitty&quot;),\n              .len = 5,\n          },\n  };\n}\n\nvoid animal_print(Animal *animal) {\n  switch (animal-&gt;kind) {\n  case ANIMAL_KIND_DOG:\n    printf(&quot;Dog: %&quot; PRIu16 &quot;\\n&quot;, animal-&gt;dog_tail);\n    break;\n  case ANIMAL_KIND_CAT:\n    printf(&quot;Cat: %.*s\\n&quot;, (int)animal-&gt;cat_name.len, animal-&gt;cat_name.data);\n    break;\n  default:\n    assert(0 &amp;&amp; &quot;unreachable&quot;);\n  }\n}\n\nuint16_t animal_dog_get_tail(Animal *animal) {\n  assert(ANIMAL_KIND_DOG == animal-&gt;kind);\n  return animal-&gt;dog_tail;\n}\n\nString animal_cat_get_name(Animal *animal) {\n  assert(ANIMAL_KIND_CAT == animal-&gt;kind);\n  return animal-&gt;cat_name;\n} c/api.h: #pragma once\n#include &lt;stdint.h&gt;\n\ntypedef struct {\n  char *data;\n  uint64_t len;\n} String;\n\ntypedef enum {\n  ANIMAL_KIND_DOG,\n  ANIMAL_KIND_CAT,\n} AnimalKind;\n\ntypedef struct {\n  AnimalKind kind;\n  union {\n    String cat_name;   // Only for `ANIMAL_KIND_CAT`.\n    uint16_t dog_tail; // Only for `ANIMAL_KIND_DOG`.\n  };\n} Animal;\n\nAnimal animal_make_dog();\n\nAnimal animal_make_cat();\n\nvoid animal_print(Animal *animal);\n\nuint16_t animal_dog_get_tail(Animal *animal);\n\nString animal_cat_get_name(Animal *animal); app/app.go: package app\n\n// NOTE: Do not use -Wall.\n\n// #cgo CFLAGS: -g -O2 -I${SRCDIR}/../c/\n// #cgo LDFLAGS: ${SRCDIR}/../c/libapi.a\n// #include &lt;api.h&gt;\n// void initial_setup();\nimport &quot;C&quot;\n\nfunc init() {\n\tC.initial_setup()\n}\n\nfunc AnimalDogKind() int {\n\treturn C.ANIMAL_KIND_DOG\n}\n\nfunc AnimalMakeDog() C.Animal {\n\treturn C.animal_make_dog()\n}\n\nfunc DoStuff() {\n\tdog := C.animal_make_dog()\n\tC.animal_print(&amp;dog)\n\n} app/app_test.go: package app\n\nimport &quot;testing&quot;\n\nfunc TestAnimalMakeDog(t *testing.T) {\n\tdog := AnimalMakeDog()\n\tif int(dog.kind) != AnimalDogKind() {\n\t\tpanic(&quot;wrong kind&quot;)\n\t}\n} app/cfuncs.go: package app\n\n/*\nvoid initial_setup(){}\n*/\nimport &quot;C&quot; main.go: package main\n\nimport &quot;cgo/app&quot;\n\nfunc main() {\n\tapp.DoStuff()\n} go.mod: module cgo\n\ngo 1.23.1 ",
titles:[
{
title:"CGO does not have unions",
slug:"cgo-does-not-have-unions",
offset:1331,
},
{
title:"Slices vs Strings",
slug:"slices-vs-strings",
offset:6302,
},
{
title:"Test a C function in Go tests",
slug:"test-a-c-function-in-go-tests",
offset:9383,
},
{
title:"The Go compiler does not detect changes",
slug:"the-go-compiler-does-not-detect-changes",
offset:11047,
},
{
title:"False positive warnings",
slug:"false-positive-warnings",
offset:11644,
},
{
title:"White space is significant",
slug:"white-space-is-significant",
offset:13260,
},
{
title:"Debug Go and C/Rust",
slug:"debug-go-and-c-rust",
offset:15931,
},
{
title:"Strace, bpftrace",
slug:"strace-bpftrace",
offset:22039,
},
{
title:"Cross-compile",
slug:"cross-compile",
offset:24019,
},
{
title:"Conclusion",
slug:"conclusion",
offset:30249,
},
{
title:"Addendum: the full code",
slug:"addendum-the-full-code",
offset:30518,
},
],
},
{
html_file_name:"making_my_debug_build_run_100_times_faster.html",
title:"Making my debug build run 100x faster so that it is finally usable",
text:"SIMD and dedicated silicon to the rescue. Discussions: /r/C_Programming , HN I am writing a torrent application, to download and serve torrent files, in C, because it\'s fun. A torrent download is made of thousands of pieces of the same size, typically a few hundred KiB to a few MiB.  At start-up, the program reads the downloaded file from disk piece by piece, computes the SHA1 hash of the piece, and marks this piece as downloaded if the actual hash is indeed the expected hash. We get from the .torrent file the expected hash of each piece. When we have not downloaded anything yet, the file is completely empty (but still of the right size - we use ftruncate(2) to size it properly even if empty from the get go), and nearly every piece has the wrong hash. Some pieces will accidentally have the right hash, since they are all zeroes in the file we are downloading - good news then, with this approach we do not even have to download them at all!. If we continue an interrupted download (for example the computer restarted), some pieces will have the right hash, and some not. When the download is complete, all pieces will have the correct hash. That way, we know what what pieces we need to download, if any. I read that some torrent clients prefer to skip this verification at startup because they persist their state in a separate file (perhaps a Sqlite database), each time a new piece is downloaded (and its hash is verified). However I favor doing a from scratch verification at startup for a few reasons, over the \'state file\' approach: We might have crashed in the middle of a previous download, before updating the state file, and the persisted state is out-of-sync with the download There may have been data corruption at the disk level (not everybody runs ZFS and can detect that!) We can continue a partial download started with a different torrent client - no need for format interoperability. The downloaded file is the source of truth. Some other program might have corrupted/modified the download, unbeknownst to us and our state file For this reason I do not have a state file at all. It\'s simpler and a whole class of out-of-sync issues disappears. So I have this big NetBSD image torrent that I primarily test with (by the way, thank you NetBSD maintainers for that!). It\'s not that big: $ du -h ./NetBSD-9.4-amd64.iso \n485M\t./NetBSD-9.4-amd64.iso But when I build my code in debug mode (no optimizations) with Address Sanitizer, to detect various issues early, startup takes 20 to 30 seconds (hashing at roughly ~ 18 KiB/s )! That\'s unbearable, especially when working in the debugger and inspecting some code that runs after the startup. We\'d like to finish this verification under 1 second ideally. And making it fast is important, because until it finishes, we do not know which pieces we need to download so that blocks everything. Let\'s see how we can speed it up. Why is it a problem at all and how did it come to be? It\'s important to note that in my case, to reduce third-party dependencies, the SHA1 code is vendored in the source tree and comes from OpenBSD. It is plain C code, not using SIMD or such. It\'s good because I can read it and understand it. I entertained depending on OpenSSL or such, but it feels wasteful to pull in such a huge amount of code just for SHA1. And building OpenSSL ourselves, to tweak the build flags, means depending on Perl (and Go, in the case of aws-lc), and a lot of stuff. And now I need to pick between OpenSSL, LibreSSL, BoringSSL, aws-lc, etc. And upgrade it when the weekly security vulnerability gets announced. I don\'t want any of it, if I can help it. Also I want to understand from top to bottom what code I depend on. For a while, due to this slowness, I simply gave up using a debug build, instead I use minimal optimizations ( -O1 ) with Address Sanitizer. It was much faster, but lots of functions and variables got optimized away, and the debugging experience was thus sub par. I needed to make my debug + Address Sanitizer build viable.  The debug build without Address Sanitizer is much faster: the startup \'only\' takes around 2 seconds. But Address Sanitizer is very valuable, I want to be able to use it! And 2 seconds is still too long. Reducing the iteration cycle is often the deciding factor for software quality in my experience. What\'s vexing is that from first principles, we know it could/should be much, much faster: $ hyperfine --shell=none --warmup 3 \'sha1sum ./NetBSD-9.4-amd64.iso\'\nBenchmark 1: sha1sum ./NetBSD-9.4-amd64.iso\n  Time (mean \u{b1} \u{3c3}):     297.7 ms \u{b1}   3.2 ms    [User: 235.8 ms, System: 60.9 ms]\n  Range (min \u{2026} max):   293.7 ms \u{2026} 304.2 ms    10 runs Granted, computing the hash for the whole file should be slightly faster than computing the hash for N pieces, because the final step for SHA1 is about padding the data to make it 64 bytes aligned and extracting the digest value from the state computed so far with some bit operations. But still, it\'s a marginal difference. Why is it so slow then? I can see on CPU profiles that the SHA1 function takes all of the startup time: The SHA1 code is simplistic, it does not use any SIMD or intrinsics directly. And that\'s fine, because when it\'s compiled with optimizations on, the compiler does a pretty good job at optimizing, and it\'s really fast, around ~300 ms. But the issue is that this code is working one byte at a time. And Address Sanitizer, with its nice runtime and bounds checks, makes each memory access very expensive. So we basically have just written a worst-case stress-test for Address Sanitizer. Let\'s first review the simple SIMD-less C version to understand the baseline. Non-SIMD implementation To isolate the issue, I have created a simple benchmark program. It reads the .torrent file, and the download file, in my case the .iso NetBSD image. Every piece gets hashed and this gets compared with the expected value (a SHA1 hash, or digest, is 20 bytes long). To simplify this example, I skip the decoding of the .torrent file, and hard-code the piece length, as well as where exactly in the file are the expected hashes. The only difficulty is that the last piece might be shorter than the others so we need to compute its exact length to avoid going out of bounds: #include &lt;fcntl.h&gt;\n#include &lt;inttypes.h&gt;\n#include &quot;sha1_sw.c&quot;\n#include &lt;stdbool.h&gt;\n#include &lt;string.h&gt;\n#include &lt;sys/mman.h&gt;\n#include &lt;sys/stat.h&gt;\n#include &lt;unistd.h&gt;\n\nstatic bool is_piece_valid(uint8_t *piece, uint64_t piece_len,\n                           uint8_t digest_expected[20]) {\n  SHA_CTX ctx = {0};\n  SHA1_Init(&amp;ctx);\n\n  SHA1_Update(&amp;ctx, piece, piece_len);\n\n  uint8_t digest_actual[20] = {0};\n  SHA1_Final(digest_actual, &amp;ctx);\n\n  return !memcmp(digest_actual, digest_expected, 20);\n}\n\nint main(int argc, char *argv[]) {\n  if (3 != argc) {\n    return 1;\n  }\n\n  int file_download = open(argv[1], O_RDONLY, 0600);\n  if (!file_download) {\n    return 1;\n  }\n\n  struct stat st_download = {0};\n  if (-1 == fstat(file_download, &amp;st_download)) {\n    return 1;\n  }\n  size_t file_download_size = (size_t)st_download.st_size;\n\n  uint8_t *file_download_data = mmap(NULL, file_download_size, PROT_READ,\n                                     MAP_FILE | MAP_PRIVATE, file_download, 0);\n  if ((void*)-1 == file_download_data) {\n    return 1;\n  }\n\n  int file_torrent = open(argv[2], O_RDONLY, 0600);\n  if (!file_torrent) {\n    return 1;\n  }\n\n  struct stat st_torrent = {0};\n  if (-1 == fstat(file_torrent, &amp;st_torrent)) {\n    return 1;\n  }\n  size_t file_torrent_size = (size_t)st_torrent.st_size;\n\n  uint8_t *file_torrent_data = mmap(NULL, file_torrent_size, PROT_READ,\n                                    MAP_FILE | MAP_PRIVATE, file_torrent, 0);\n  if ((void*)-1 == file_torrent_data) {\n    return 1;\n  }\n  // HACK\n  // The piece hashes begin at offset 237 in the file.\n  uint64_t file_torrent_data_offset = 237;\n  file_torrent_data += file_torrent_data_offset;\n  // The last character in the file must be ignored because it\'s the bencode dictionary closing character \'e\'.\n  file_torrent_size -= file_torrent_data_offset - 1;\n\n  uint64_t piece_length = 262144;\n  uint64_t pieces_count = file_download_size / piece_length +\n                          ((0 == file_download_size % piece_length) ? 0 : 1);\n  for (uint64_t i = 0; i &lt; pieces_count; i++) {\n    uint8_t *data = file_download_data + i * piece_length;\n    uint64_t piece_length_real = ((i + 1) == pieces_count)\n                                     ? (file_download_size - i * piece_length)\n                                     : piece_length;\n    uint8_t *digest_expected = file_torrent_data + i * 20;\n\n    if (!is_piece_valid(data, piece_length_real, digest_expected)) {\n      return 1;\n    }\n  }\n} Explanation In Intel words, what is SHA1? SHA-1 produces a 160 bit (20 byte) hash value (digest), taking as an input a sequence of 1 or more 512 bit (64 byte) data blocks. The original source data also requires some padding according to the standard. The data is treated as an array of big-endian 32-bit values. Processing each of the 64-byte input blocks consists of 80 iterations also known as rounds. In this implementation: State initialization is done with SHA1_Init : the state is an array of 5 uint32_t , set to magic values defined by the standard. Processing is done with SHA1_Update : processing works on chunks of 64 bytes. The result of the processing of a chunk is that the state is updated to new values. Incoming data is buffered into the current chunk until it reaches 64 bytes, and that\'s when the real computation kicks in with SHA1_Transform . This API allows for reading and hashing data in a streaming fashion by repeatedly calling SHA1_Update . Padding and finalization is done with SHA1_Final : the last chunk is padded to 64 bytes if it is too short, processed, and the digest (the final 20 bytes we are after) is the current state, after endianness conversion. The SHA1 algorithm and some implementations support architectures where 1 byte is not 8 bits. But knowing that 1 byte is indeed 8 bits on our architecture unlocks a ton of performance as we\'ll see. SHA1 expects data in big-endian but nearly all CPU nowadays are little-endian so we need to swap the bytes when loading the input data to do SHA1 computations, and back when storing the intermediate results (the SHA1 state). It is done here with lots of clever bit tricks, one uint32_t at a time. The main loop operating on the 64 bytes chunk is unrolled, which avoids having conditionals in the middle of the loop, which might tank performance due to mispredicted branches. The algorithm lends itself to that really well: for i from 0 to 79\n          if 0 \u{2264} i \u{2264} 19 then\n            [..]\n          else if 20 \u{2264} i \u{2264} 39\n            [..]\n          else if 40 \u{2264} i \u{2264} 59\n            [..]\n          else if 60 \u{2264} i \u{2264} 79\n            [..] So it\'s trivial to unroll each section. We\'ll see that every implementation does the unrolling. The code Non-SIMD SHA1 // sha1_sw.c\n\n/*\t$OpenBSD: sha1.c,v 1.27 2019/06/07 22:56:36 dtucker Exp $\t*/\n\n/*\n * SHA-1 in C\n * By Steve Reid &lt;steve@edmweb.com&gt;\n * 100% Public Domain\n *\n * Test Vectors (from FIPS PUB 180-1)\n * &quot;abc&quot;\n *   A9993E36 4706816A BA3E2571 7850C26C 9CD0D89D\n * &quot;abcdbcdecdefdefgefghfghighijhijkijkljklmklmnlmnomnopnopq&quot;\n *   84983E44 1C3BD26E BAAE4AA1 F95129E5 E54670F1\n * A million repetitions of &quot;a&quot;\n *   34AA973C D4C4DAA4 F61EEB2B DBAD2731 6534016F\n */\n\n#include &lt;inttypes.h&gt;\n#include &lt;string.h&gt;\n\n#define SHA1_BLOCK_LENGTH 64\n#define SHA1_DIGEST_LENGTH 20\n#define SHA1_DIGEST_STRING_LENGTH (SHA1_DIGEST_LENGTH * 2 + 1)\n\ntypedef struct {\n  uint32_t state[5];\n  uint64_t count;\n  uint8_t buffer[SHA1_BLOCK_LENGTH];\n} SHA1_CTX;\n#define rol(value, bits) (((value) &lt;&lt; (bits)) | ((value) &gt;&gt; (32 - (bits))))\n\n/*\n * blk0() and blk() perform the initial expand.\n * I got the idea of expanding during the round function from SSLeay\n */\n#define blk0(i)                                                                \\\n  (block-&gt;l[i] = (rol(block-&gt;l[i], 24) &amp; 0xFF00FF00) |                         \\\n                 (rol(block-&gt;l[i], 8) &amp; 0x00FF00FF))\n#define blk(i)                                                                 \\\n  (block-&gt;l[i &amp; 15] = rol(block-&gt;l[(i + 13) &amp; 15] ^ block-&gt;l[(i + 8) &amp; 15] ^   \\\n                              block-&gt;l[(i + 2) &amp; 15] ^ block-&gt;l[i &amp; 15],       \\\n                          1))\n\n/*\n * (R0+R1), R2, R3, R4 are the different operations (rounds) used in SHA1\n */\n#define R0(v, w, x, y, z, i)                                                   \\\n  z += ((w &amp; (x ^ y)) ^ y) + blk0(i) + 0x5A827999 + rol(v, 5);                 \\\n  w = rol(w, 30);\n#define R1(v, w, x, y, z, i)                                                   \\\n  z += ((w &amp; (x ^ y)) ^ y) + blk(i) + 0x5A827999 + rol(v, 5);                  \\\n  w = rol(w, 30);\n#define R2(v, w, x, y, z, i)                                                   \\\n  z += (w ^ x ^ y) + blk(i) + 0x6ED9EBA1 + rol(v, 5);                          \\\n  w = rol(w, 30);\n#define R3(v, w, x, y, z, i)                                                   \\\n  z += (((w | x) &amp; y) | (w &amp; x)) + blk(i) + 0x8F1BBCDC + rol(v, 5);            \\\n  w = rol(w, 30);\n#define R4(v, w, x, y, z, i)                                                   \\\n  z += (w ^ x ^ y) + blk(i) + 0xCA62C1D6 + rol(v, 5);                          \\\n  w = rol(w, 30);\n\ntypedef union {\n  uint8_t c[64];\n  uint32_t l[16];\n} CHAR64LONG16;\n\n/*\n * Hash a single 512-bit block. This is the core of the algorithm.\n */\nvoid SHA1Transform(uint32_t state[5], const uint8_t buffer[SHA1_BLOCK_LENGTH]) {\n  uint32_t a, b, c, d, e;\n  uint8_t workspace[SHA1_BLOCK_LENGTH];\n  CHAR64LONG16 *block = (CHAR64LONG16 *)workspace;\n\n  (void)memcpy(block, buffer, SHA1_BLOCK_LENGTH);\n\n  /* Copy context-&gt;state[] to working vars */\n  a = state[0];\n  b = state[1];\n  c = state[2];\n  d = state[3];\n  e = state[4];\n\n  /* 4 rounds of 20 operations each. Loop unrolled. */\n  R0(a, b, c, d, e, 0);\n  R0(e, a, b, c, d, 1);\n  R0(d, e, a, b, c, 2);\n  R0(c, d, e, a, b, 3);\n  R0(b, c, d, e, a, 4);\n  R0(a, b, c, d, e, 5);\n  R0(e, a, b, c, d, 6);\n  R0(d, e, a, b, c, 7);\n  R0(c, d, e, a, b, 8);\n  R0(b, c, d, e, a, 9);\n  R0(a, b, c, d, e, 10);\n  R0(e, a, b, c, d, 11);\n  R0(d, e, a, b, c, 12);\n  R0(c, d, e, a, b, 13);\n  R0(b, c, d, e, a, 14);\n  R0(a, b, c, d, e, 15);\n  R1(e, a, b, c, d, 16);\n  R1(d, e, a, b, c, 17);\n  R1(c, d, e, a, b, 18);\n  R1(b, c, d, e, a, 19);\n  R2(a, b, c, d, e, 20);\n  R2(e, a, b, c, d, 21);\n  R2(d, e, a, b, c, 22);\n  R2(c, d, e, a, b, 23);\n  R2(b, c, d, e, a, 24);\n  R2(a, b, c, d, e, 25);\n  R2(e, a, b, c, d, 26);\n  R2(d, e, a, b, c, 27);\n  R2(c, d, e, a, b, 28);\n  R2(b, c, d, e, a, 29);\n  R2(a, b, c, d, e, 30);\n  R2(e, a, b, c, d, 31);\n  R2(d, e, a, b, c, 32);\n  R2(c, d, e, a, b, 33);\n  R2(b, c, d, e, a, 34);\n  R2(a, b, c, d, e, 35);\n  R2(e, a, b, c, d, 36);\n  R2(d, e, a, b, c, 37);\n  R2(c, d, e, a, b, 38);\n  R2(b, c, d, e, a, 39);\n  R3(a, b, c, d, e, 40);\n  R3(e, a, b, c, d, 41);\n  R3(d, e, a, b, c, 42);\n  R3(c, d, e, a, b, 43);\n  R3(b, c, d, e, a, 44);\n  R3(a, b, c, d, e, 45);\n  R3(e, a, b, c, d, 46);\n  R3(d, e, a, b, c, 47);\n  R3(c, d, e, a, b, 48);\n  R3(b, c, d, e, a, 49);\n  R3(a, b, c, d, e, 50);\n  R3(e, a, b, c, d, 51);\n  R3(d, e, a, b, c, 52);\n  R3(c, d, e, a, b, 53);\n  R3(b, c, d, e, a, 54);\n  R3(a, b, c, d, e, 55);\n  R3(e, a, b, c, d, 56);\n  R3(d, e, a, b, c, 57);\n  R3(c, d, e, a, b, 58);\n  R3(b, c, d, e, a, 59);\n  R4(a, b, c, d, e, 60);\n  R4(e, a, b, c, d, 61);\n  R4(d, e, a, b, c, 62);\n  R4(c, d, e, a, b, 63);\n  R4(b, c, d, e, a, 64);\n  R4(a, b, c, d, e, 65);\n  R4(e, a, b, c, d, 66);\n  R4(d, e, a, b, c, 67);\n  R4(c, d, e, a, b, 68);\n  R4(b, c, d, e, a, 69);\n  R4(a, b, c, d, e, 70);\n  R4(e, a, b, c, d, 71);\n  R4(d, e, a, b, c, 72);\n  R4(c, d, e, a, b, 73);\n  R4(b, c, d, e, a, 74);\n  R4(a, b, c, d, e, 75);\n  R4(e, a, b, c, d, 76);\n  R4(d, e, a, b, c, 77);\n  R4(c, d, e, a, b, 78);\n  R4(b, c, d, e, a, 79);\n\n  /* Add the working vars back into context.state[] */\n  state[0] += a;\n  state[1] += b;\n  state[2] += c;\n  state[3] += d;\n  state[4] += e;\n\n  /* Wipe variables */\n  a = b = c = d = e = 0;\n}\n\n/*\n * SHA1Init - Initialize new context\n */\nvoid SHA1Init(SHA1_CTX *context) {\n\n  /* SHA1 initialization constants */\n  context-&gt;count = 0;\n  context-&gt;state[0] = 0x67452301;\n  context-&gt;state[1] = 0xEFCDAB89;\n  context-&gt;state[2] = 0x98BADCFE;\n  context-&gt;state[3] = 0x10325476;\n  context-&gt;state[4] = 0xC3D2E1F0;\n}\n\n/*\n * Run your data through this.\n */\nvoid SHA1Update(SHA1_CTX *context, const uint8_t *data, size_t len) {\n  size_t i, j;\n\n  j = (size_t)((context-&gt;count &gt;&gt; 3) &amp; 63);\n  context-&gt;count += ((uint64_t)len &lt;&lt; 3);\n  if ((j + len) &gt; 63) {\n    (void)memcpy(&amp;context-&gt;buffer[j], data, (i = 64 - j));\n    SHA1Transform(context-&gt;state, context-&gt;buffer);\n    for (; i + 63 &lt; len; i += 64)\n      SHA1Transform(context-&gt;state, (uint8_t *)&amp;data[i]);\n    j = 0;\n  } else {\n    i = 0;\n  }\n  (void)memcpy(&amp;context-&gt;buffer[j], &amp;data[i], len - i);\n}\n\n/*\n * Add padding and return the message digest.\n */\nvoid SHA1Pad(SHA1_CTX *context) {\n  uint8_t finalcount[8];\n  size_t i;\n\n  for (i = 0; i &lt; 8; i++) {\n    finalcount[i] = (uint8_t)((context-&gt;count &gt;&gt; ((7 - (i &amp; 7)) * 8)) &amp;\n                              255); /* Endian independent */\n  }\n  SHA1Update(context, (uint8_t *)&quot;\\200&quot;, 1);\n  while ((context-&gt;count &amp; 504) != 448)\n    SHA1Update(context, (uint8_t *)&quot;\\0&quot;, 1);\n  SHA1Update(context, finalcount, 8); /* Should cause a SHA1Transform() */\n}\n\nvoid SHA1Final(uint8_t digest[SHA1_DIGEST_LENGTH], SHA1_CTX *context) {\n  size_t i;\n\n  SHA1Pad(context);\n  for (i = 0; i &lt; SHA1_DIGEST_LENGTH; i++) {\n    digest[i] =\n        (uint8_t)((context-&gt;state[i &gt;&gt; 2] &gt;&gt; ((3 - (i &amp; 3)) * 8)) &amp; 255);\n  }\n  explicit_bzero(context, sizeof(*context));\n} The SHA1_xxx functions are lifted from OpenBSD (there are similar variants, e.g. from Sqlite , etc - they are all nearly identical) Results When compiled in non-optimized mode with Address Sanitizer, we get this timing: $ hyperfine --warmup 3 \'./a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent\'\nBenchmark 1: ./a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent\n  Time (mean \u{b1} \u{3c3}):     26.312 s \u{b1}  0.734 s    [User: 26.164 s, System: 0.066 s]\n  Range (min \u{2026} max):   25.366 s \u{2026} 27.780 s    10 runs This is consistent with our real-life torrent program. I experimented with doing a read syscall for each piece (that\'s what sha1sum does) versus using mmap , and there was no difference; additionally the system time is nothing compared to user time, so I/O is not the limiting factor - SHA1 computation is, as confirmed by the CPU profile. Possible optimizations So what can we do about it? We can build the SHA1 code separately with optimizations on, always (and potentially without Address Sanitizer). That\'s a bit annoying, because I currently do a Unity build meaning there is only one compilation unit. So having suddenly multiple compilation units with different build flags makes the build system more complex. And clang has annotations to lower the optimization level for one function but not to raise it. We can compute the hash of each piece in parallel for example in a thread pool, since each piece is independent. That works and that\'s what libtorrent did/does , but that assumes that the target computer has cores to spare, and it creates some complexity: We need to implement a thread pool (spawning a new thread for each piece will not perform well) and pick a reasonable thread pool size given the number of cores, in a cross-platform way We need a M:N scheduling logic to compute the hash of M pieces on N threads. It could be a work-stealing queue where each thread picks the next item when its finished with its piece, or we read the whole file in memory and split the data in equal parts for each thread to plow through (but beware that the data for each thread must be aligned with the piece size!) We would have high contention: in the real program, right after we checked the actual hash against the expected hash, we update a bitfield to remember which piece is valid or not. Every thread would contend on this (probably more so with the work-stealing approach than with the \'split in equal parts\' approach where we could avoid locking and contention entirely). We can implement SHA1 with SIMD. That way, it\'s much faster regardless of the build level. Essentially, we do not rely on the compiler auto-vectorization that only occurs at higher optimization levels, we do it directly. It has the nice advantage that we have guaranteed performance even when using a different compiler, or an older compiler that cannot do auto-vectorization properly, or if a new compiler version comes along and auto-vectorization broke for this code. Since it uses lots of heuristics, this may happen. So let\'s do SIMD and learn cool new stuff! The nice thing about it is that we can always in the future also compute hashes in parallel, as well as use SIMD; the two approaches compose well together. I am reminded of an old adage: You can\'t have multiple cores until you\'ve shown you can use one efficiently. SIMD (SSE) implementation This is an implementation from the early 2000s in the public domain. Yes, SSE, which is the first widespread SIMD instruction set, is from the nineties to early 2000s. More than 25 years ago! There\'s basically no reason to write SIMD-less code for performance sensitive code for a SIMD-friendly problem - every CPU we care about has SIMD! Well, we have two write separate implementations for x64 and ARM, and there were lots of additions to SSE over the years, that\'s the downside. Intel references this implementation on their website . According to Intel, it was fundamental work at the time and influenced them. It\'s also not the fastest SSE implementation, the very article from Intel is about some performance enhancements they found for this code, but it has the advantage that if you have a processor from 2004 or after, it works, and it\'s simple. Explanation I really am a SIMD beginner but I found a few interesting nuggets of wisdom here: Just like the SIMD-less implementation, the loops are unrolled Going from little-endian to big-endian (or back) is done with a SIMD shuffle. The way it works is by providing a bit mask that indicates which bits to copy from the source to the destination, and where to place them: // `0x1b` == `0b0001_1011`.\n  // Will result in:\n  // [31:0] == [127:96] (due to bits [1:0] being `11`).\n  // [63:32] == [95:64] (due to bits [3:2] being `10`).\n  // [95:64] == [63:32] (due to bits [5:4] being `01`).\n  // [127:96] == [31:0] (due to bits [7:6] being `00`).\n  // I.e.: Transform state to big-endian.\n  ABCD = _mm_shuffle_epi32(ABCD, 0x1B); It\'s nifty because we can copy the data in and out of SIMD registers, while also doing the endianness conversion, in one operation that typically compiles down to one assembly instruction. And this approach also works from a SIMD register to another SIMD register or inside the same register. Typical SIMD code processes the data in groups of N bytes at a time, and the few excess bytes at the end use the normal SIMD-less code path. Here, we have to deal with an additional grouping: SHA1 processes data in chunks of 64 bytes and the last chunk is padded to be 64 bytes if it is too short. Hence, for the last short chunk we use the SIMD-less code path. We could try to be clever about doing the padding, and re-using the SIMD code path for this last chunk, since 64 bytes is a nice round number that is SIMD friendly, but this last chunk is not going to really make a difference in practice when we are dealing with megabytes or gigabytes of data. The code SHA1 with SSE typedef union {\n  uint32_t u32[4];\n  __m128i u128;\n} v4si __attribute__((aligned(16)));\n\nstatic const v4si K00_19 = {\n    .u32 = {0x5a827999, 0x5a827999, 0x5a827999, 0x5a827999}};\nstatic const v4si K20_39 = {\n    .u32 = {0x6ed9eba1, 0x6ed9eba1, 0x6ed9eba1, 0x6ed9eba1}};\nstatic const v4si K40_59 = {\n    .u32 = {0x8f1bbcdc, 0x8f1bbcdc, 0x8f1bbcdc, 0x8f1bbcdc}};\nstatic const v4si K60_79 = {\n    .u32 = {0xca62c1d6, 0xca62c1d6, 0xca62c1d6, 0xca62c1d6}};\n\n#define UNALIGNED 1\n#if UNALIGNED\n#define load(p) _mm_loadu_si128(p)\n#else\n#define load(p) (*p)\n#endif\n\n/*\n        the first 16 bytes only need byte swapping\n\n        prepared points to 4x uint32_t, 16-byte aligned\n\n        W points to the 4 dwords which need preparing --\n        and is overwritten with the swapped bytes\n*/\n#define prep00_15(prep, W)                                                     \\\n  do {                                                                         \\\n    __m128i r1, r2;                                                            \\\n                                                                               \\\n    r1 = (W);                                                                  \\\n    if (1) {                                                                   \\\n      r1 = _mm_shufflehi_epi16(r1, _MM_SHUFFLE(2, 3, 0, 1));                   \\\n      r1 = _mm_shufflelo_epi16(r1, _MM_SHUFFLE(2, 3, 0, 1));                   \\\n      r2 = _mm_slli_epi16(r1, 8);                                              \\\n      r1 = _mm_srli_epi16(r1, 8);                                              \\\n      r1 = _mm_or_si128(r1, r2);                                               \\\n      (W) = r1;                                                                \\\n    }                                                                          \\\n    (prep).u128 = _mm_add_epi32(K00_19.u128, r1);                              \\\n  } while (0)\n\n/*\n        for each multiple of 4, t, we want to calculate this:\n\n        W[t+0] = rol(W[t-3] ^ W[t-8] ^ W[t-14] ^ W[t-16], 1);\n        W[t+1] = rol(W[t-2] ^ W[t-7] ^ W[t-13] ^ W[t-15], 1);\n        W[t+2] = rol(W[t-1] ^ W[t-6] ^ W[t-12] ^ W[t-14], 1);\n        W[t+3] = rol(W[t]   ^ W[t-5] ^ W[t-11] ^ W[t-13], 1);\n\n        we\'ll actually calculate this:\n\n        W[t+0] = rol(W[t-3] ^ W[t-8] ^ W[t-14] ^ W[t-16], 1);\n        W[t+1] = rol(W[t-2] ^ W[t-7] ^ W[t-13] ^ W[t-15], 1);\n        W[t+2] = rol(W[t-1] ^ W[t-6] ^ W[t-12] ^ W[t-14], 1);\n        W[t+3] = rol(  0    ^ W[t-5] ^ W[t-11] ^ W[t-13], 1);\n        W[t+3] ^= rol(W[t+0], 1);\n\n        the parameters are:\n\n        W0 = &amp;W[t-16];\n        W1 = &amp;W[t-12];\n        W2 = &amp;W[t- 8];\n        W3 = &amp;W[t- 4];\n\n        and on output:\n                prepared = W0 + K\n                W0 = W[t]..W[t+3]\n*/\n\n/* note that there is a step here where i want to do a rol by 1, which\n * normally would look like this:\n *\n * r1 = psrld r0,$31\n * r0 = pslld r0,$1\n * r0 = por r0,r1\n *\n * but instead i do this:\n *\n * r1 = pcmpltd r0,zero\n * r0 = paddd r0,r0\n * r0 = psub r0,r1\n *\n * because pcmpltd and paddd are availabe in both MMX units on\n * efficeon, pentium-m, and opteron but shifts are available in\n * only one unit.\n */\n#define prep(prep, XW0, XW1, XW2, XW3, K)                                      \\\n  do {                                                                         \\\n    __m128i r0, r1, r2, r3;                                                    \\\n                                                                               \\\n    /* load W[t-4] 16-byte aligned, and shift */                               \\\n    r3 = _mm_srli_si128((XW3), 4);                                             \\\n    r0 = (XW0);                                                                \\\n    /* get high 64-bits of XW0 into low 64-bits */                             \\\n    r1 = _mm_shuffle_epi32((XW0), _MM_SHUFFLE(1, 0, 3, 2));                    \\\n    /* load high 64-bits of r1 */                                              \\\n    r1 = _mm_unpacklo_epi64(r1, (XW1));                                        \\\n    r2 = (XW2);                                                                \\\n                                                                               \\\n    r0 = _mm_xor_si128(r1, r0);                                                \\\n    r2 = _mm_xor_si128(r3, r2);                                                \\\n    r0 = _mm_xor_si128(r2, r0);                                                \\\n    /* unrotated W[t]..W[t+2] in r0 ... still need W[t+3] */                   \\\n                                                                               \\\n    r2 = _mm_slli_si128(r0, 12);                                               \\\n    r1 = _mm_cmplt_epi32(r0, _mm_setzero_si128());                             \\\n    r0 = _mm_add_epi32(r0, r0); /* shift left by 1 */                          \\\n    r0 = _mm_sub_epi32(r0, r1); /* r0 has W[t]..W[t+2] */                      \\\n                                                                               \\\n    r3 = _mm_srli_epi32(r2, 30);                                               \\\n    r2 = _mm_slli_epi32(r2, 2);                                                \\\n                                                                               \\\n    r0 = _mm_xor_si128(r0, r3);                                                \\\n    r0 = _mm_xor_si128(r0, r2); /* r0 now has W[t+3] */                        \\\n                                                                               \\\n    (XW0) = r0;                                                                \\\n    (prep).u128 = _mm_add_epi32(r0, (K).u128);                                 \\\n  } while (0)\n\nstatic inline uint32_t f00_19(uint32_t x, uint32_t y, uint32_t z) {\n  /* FIPS 180-2 says this: (x &amp; y) ^ (~x &amp; z)\n   * but we can calculate it in fewer steps.\n   */\n  return ((y ^ z) &amp; x) ^ z;\n}\n\nstatic inline uint32_t f20_39(uint32_t x, uint32_t y, uint32_t z) {\n  return (x ^ z) ^ y;\n}\n\nstatic inline uint32_t f40_59(uint32_t x, uint32_t y, uint32_t z) {\n  /* FIPS 180-2 says this: (x &amp; y) ^ (x &amp; z) ^ (y &amp; z)\n   * but we can calculate it in fewer steps.\n   */\n  return (x &amp; z) | ((x | z) &amp; y);\n}\n\nstatic inline uint32_t f60_79(uint32_t x, uint32_t y, uint32_t z) {\n  return f20_39(x, y, z);\n}\n\n#define step(nn_mm, xa, xb, xc, xd, xe, xt, input)                             \\\n  do {                                                                         \\\n    (xt) = (input) + f##nn_mm((xb), (xc), (xd));                               \\\n    (xb) = rol((xb), 30);                                                      \\\n    (xt) += ((xe) + rol((xa), 5));                                             \\\n  } while (0)\n\n[[maybe_unused]]\nstatic void sha1_sse_step(uint32_t *restrict H, const uint32_t *restrict inputu,\n                          size_t num_steps) {\n  const __m128i *restrict input = (const __m128i *)inputu;\n  __m128i W0, W1, W2, W3;\n  v4si prep0, prep1, prep2;\n  uint32_t a, b, c, d, e, t;\n\n  a = H[0];\n  b = H[1];\n  c = H[2];\n  d = H[3];\n  e = H[4];\n\n  /* i\'ve tried arranging the SSE2 code to be 4, 8, 12, and 16\n   * steps ahead of the integer code.  12 steps ahead seems\n   * to produce the best performance. -dean\n   */\n  W0 = load(&amp;input[0]);\n  prep00_15(prep0, W0); /* prepare for 00 through 03 */\n  W1 = load(&amp;input[1]);\n  prep00_15(prep1, W1); /* prepare for 04 through 07 */\n  W2 = load(&amp;input[2]);\n  prep00_15(prep2, W2); /* prepare for 08 through 11 */\n  for (;;) {\n    W3 = load(&amp;input[3]);\n    step(00_19, a, b, c, d, e, t, prep0.u32[0]); /* 00 */\n    step(00_19, t, a, b, c, d, e, prep0.u32[1]); /* 01 */\n    step(00_19, e, t, a, b, c, d, prep0.u32[2]); /* 02 */\n    step(00_19, d, e, t, a, b, c, prep0.u32[3]); /* 03 */\n    prep00_15(prep0, W3);\n    step(00_19, c, d, e, t, a, b, prep1.u32[0]); /* 04 */\n    step(00_19, b, c, d, e, t, a, prep1.u32[1]); /* 05 */\n    step(00_19, a, b, c, d, e, t, prep1.u32[2]); /* 06 */\n    step(00_19, t, a, b, c, d, e, prep1.u32[3]); /* 07 */\n    prep(prep1, W0, W1, W2, W3, K00_19);         /* prepare for 16 through 19 */\n    step(00_19, e, t, a, b, c, d, prep2.u32[0]); /* 08 */\n    step(00_19, d, e, t, a, b, c, prep2.u32[1]); /* 09 */\n    step(00_19, c, d, e, t, a, b, prep2.u32[2]); /* 10 */\n    step(00_19, b, c, d, e, t, a, prep2.u32[3]); /* 11 */\n    prep(prep2, W1, W2, W3, W0, K20_39);         /* prepare for 20 through 23 */\n    step(00_19, a, b, c, d, e, t, prep0.u32[0]); /* 12 */\n    step(00_19, t, a, b, c, d, e, prep0.u32[1]); /* 13 */\n    step(00_19, e, t, a, b, c, d, prep0.u32[2]); /* 14 */\n    step(00_19, d, e, t, a, b, c, prep0.u32[3]); /* 15 */\n    prep(prep0, W2, W3, W0, W1, K20_39);\n    step(00_19, c, d, e, t, a, b, prep1.u32[0]); /* 16 */\n    step(00_19, b, c, d, e, t, a, prep1.u32[1]); /* 17 */\n    step(00_19, a, b, c, d, e, t, prep1.u32[2]); /* 18 */\n    step(00_19, t, a, b, c, d, e, prep1.u32[3]); /* 19 */\n\n    prep(prep1, W3, W0, W1, W2, K20_39);\n    step(20_39, e, t, a, b, c, d, prep2.u32[0]); /* 20 */\n    step(20_39, d, e, t, a, b, c, prep2.u32[1]); /* 21 */\n    step(20_39, c, d, e, t, a, b, prep2.u32[2]); /* 22 */\n    step(20_39, b, c, d, e, t, a, prep2.u32[3]); /* 23 */\n    prep(prep2, W0, W1, W2, W3, K20_39);\n    step(20_39, a, b, c, d, e, t, prep0.u32[0]); /* 24 */\n    step(20_39, t, a, b, c, d, e, prep0.u32[1]); /* 25 */\n    step(20_39, e, t, a, b, c, d, prep0.u32[2]); /* 26 */\n    step(20_39, d, e, t, a, b, c, prep0.u32[3]); /* 27 */\n    prep(prep0, W1, W2, W3, W0, K20_39);\n    step(20_39, c, d, e, t, a, b, prep1.u32[0]); /* 28 */\n    step(20_39, b, c, d, e, t, a, prep1.u32[1]); /* 29 */\n    step(20_39, a, b, c, d, e, t, prep1.u32[2]); /* 30 */\n    step(20_39, t, a, b, c, d, e, prep1.u32[3]); /* 31 */\n    prep(prep1, W2, W3, W0, W1, K40_59);\n    step(20_39, e, t, a, b, c, d, prep2.u32[0]); /* 32 */\n    step(20_39, d, e, t, a, b, c, prep2.u32[1]); /* 33 */\n    step(20_39, c, d, e, t, a, b, prep2.u32[2]); /* 34 */\n    step(20_39, b, c, d, e, t, a, prep2.u32[3]); /* 35 */\n    prep(prep2, W3, W0, W1, W2, K40_59);\n    step(20_39, a, b, c, d, e, t, prep0.u32[0]); /* 36 */\n    step(20_39, t, a, b, c, d, e, prep0.u32[1]); /* 37 */\n    step(20_39, e, t, a, b, c, d, prep0.u32[2]); /* 38 */\n    step(20_39, d, e, t, a, b, c, prep0.u32[3]); /* 39 */\n\n    prep(prep0, W0, W1, W2, W3, K40_59);\n    step(40_59, c, d, e, t, a, b, prep1.u32[0]); /* 40 */\n    step(40_59, b, c, d, e, t, a, prep1.u32[1]); /* 41 */\n    step(40_59, a, b, c, d, e, t, prep1.u32[2]); /* 42 */\n    step(40_59, t, a, b, c, d, e, prep1.u32[3]); /* 43 */\n    prep(prep1, W1, W2, W3, W0, K40_59);\n    step(40_59, e, t, a, b, c, d, prep2.u32[0]); /* 44 */\n    step(40_59, d, e, t, a, b, c, prep2.u32[1]); /* 45 */\n    step(40_59, c, d, e, t, a, b, prep2.u32[2]); /* 46 */\n    step(40_59, b, c, d, e, t, a, prep2.u32[3]); /* 47 */\n    prep(prep2, W2, W3, W0, W1, K40_59);\n    step(40_59, a, b, c, d, e, t, prep0.u32[0]); /* 48 */\n    step(40_59, t, a, b, c, d, e, prep0.u32[1]); /* 49 */\n    step(40_59, e, t, a, b, c, d, prep0.u32[2]); /* 50 */\n    step(40_59, d, e, t, a, b, c, prep0.u32[3]); /* 51 */\n    prep(prep0, W3, W0, W1, W2, K60_79);\n    step(40_59, c, d, e, t, a, b, prep1.u32[0]); /* 52 */\n    step(40_59, b, c, d, e, t, a, prep1.u32[1]); /* 53 */\n    step(40_59, a, b, c, d, e, t, prep1.u32[2]); /* 54 */\n    step(40_59, t, a, b, c, d, e, prep1.u32[3]); /* 55 */\n    prep(prep1, W0, W1, W2, W3, K60_79);\n    step(40_59, e, t, a, b, c, d, prep2.u32[0]); /* 56 */\n    step(40_59, d, e, t, a, b, c, prep2.u32[1]); /* 57 */\n    step(40_59, c, d, e, t, a, b, prep2.u32[2]); /* 58 */\n    step(40_59, b, c, d, e, t, a, prep2.u32[3]); /* 59 */\n\n    prep(prep2, W1, W2, W3, W0, K60_79);\n    step(60_79, a, b, c, d, e, t, prep0.u32[0]); /* 60 */\n    step(60_79, t, a, b, c, d, e, prep0.u32[1]); /* 61 */\n    step(60_79, e, t, a, b, c, d, prep0.u32[2]); /* 62 */\n    step(60_79, d, e, t, a, b, c, prep0.u32[3]); /* 63 */\n    prep(prep0, W2, W3, W0, W1, K60_79);\n    step(60_79, c, d, e, t, a, b, prep1.u32[0]); /* 64 */\n    step(60_79, b, c, d, e, t, a, prep1.u32[1]); /* 65 */\n    step(60_79, a, b, c, d, e, t, prep1.u32[2]); /* 66 */\n    step(60_79, t, a, b, c, d, e, prep1.u32[3]); /* 67 */\n    prep(prep1, W3, W0, W1, W2, K60_79);\n    step(60_79, e, t, a, b, c, d, prep2.u32[0]); /* 68 */\n    step(60_79, d, e, t, a, b, c, prep2.u32[1]); /* 69 */\n    step(60_79, c, d, e, t, a, b, prep2.u32[2]); /* 70 */\n    step(60_79, b, c, d, e, t, a, prep2.u32[3]); /* 71 */\n\n    --num_steps;\n    if (num_steps == 0)\n      break;\n\n    input += 4;\n    W0 = load(&amp;input[0]);\n    prep00_15(prep2, W0); /* prepare for next 00 through 03 */\n    W1 = load(&amp;input[1]);\n    step(60_79, a, b, c, d, e, t, prep0.u32[0]); /* 72 */\n    step(60_79, t, a, b, c, d, e, prep0.u32[1]); /* 73 */\n    step(60_79, e, t, a, b, c, d, prep0.u32[2]); /* 74 */\n    step(60_79, d, e, t, a, b, c, prep0.u32[3]); /* 75 */\n    prep0 = prep2;        /* top of loop expects this in prep0 */\n    prep00_15(prep2, W1); /* prepare for next 04 through 07 */\n    W2 = load(&amp;input[2]);\n    step(60_79, c, d, e, t, a, b, prep1.u32[0]); /* 76 */\n    step(60_79, b, c, d, e, t, a, prep1.u32[1]); /* 77 */\n    step(60_79, a, b, c, d, e, t, prep1.u32[2]); /* 78 */\n    step(60_79, t, a, b, c, d, e, prep1.u32[3]); /* 79 */\n    prep1 = prep2;        /* top of loop expects this in prep1 */\n    prep00_15(prep2, W2); /* prepare for next 08 through 11 */\n    /* e, t, a, b, c, d */\n    H[0] += e;\n    H[1] += t;\n    H[2] += a;\n    H[3] += b;\n    H[4] += c;\n\n    a = H[0];\n    b = H[1];\n    c = H[2];\n    d = H[3];\n    e = H[4];\n  }\n  /* no more input to prepare */\n  step(60_79, a, b, c, d, e, t, prep0.u32[0]); /* 72 */\n  step(60_79, t, a, b, c, d, e, prep0.u32[1]); /* 73 */\n  step(60_79, e, t, a, b, c, d, prep0.u32[2]); /* 74 */\n  step(60_79, d, e, t, a, b, c, prep0.u32[3]); /* 75 */\n  /* no more input to prepare */\n  step(60_79, c, d, e, t, a, b, prep1.u32[0]); /* 76 */\n  step(60_79, b, c, d, e, t, a, prep1.u32[1]); /* 77 */\n  step(60_79, a, b, c, d, e, t, prep1.u32[2]); /* 78 */\n  step(60_79, t, a, b, c, d, e, prep1.u32[3]); /* 79 */\n  /* e, t, a, b, c, d */\n  H[0] += e;\n  H[1] += t;\n  H[2] += a;\n  H[3] += b;\n  H[4] += c;\n} Our is_piece_valid function now becomes: static bool is_piece_valid(uint8_t *piece, uint64_t piece_len,\n                           uint8_t digest_expected[20]) {\n  SHA1_CTX ctx = {0};\n  SHA1Init(&amp;ctx);\n\n  // Process as many SHA1 64 bytes chunks as possible.\n  uint64_t len_rounded_down = (piece_len / 64) * 64;\n  uint64_t rem = piece_len % 64;\n  uint64_t steps = len_rounded_down / 64;\n  sha1_sse_step(ctx.state, piece, steps);\n\n  // Process the excess.\n  memcpy(ctx.buffer, piece + len_rounded_down, rem);\n\n  // `count` is in bits: multiple the number of bytes by 8.\n  ctx.count = piece_len * 8;\n\n  uint8_t digest_actual[20] = {0};\n  SHA1Final(digest_actual, &amp;ctx);\n\n  return !memcmp(digest_actual, digest_expected, 20);\n} Results So predictably, since we now process 4 uint32_t at a time instead of one, we observe roughly a 4x speed-up (still in debug + Address Sanitizer mode): $ hyperfine --warmup 3 \'./a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent\'\nBenchmark 1: ./a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent\n  Time (mean \u{b1} \u{3c3}):      8.093 s \u{b1}  0.272 s    [User: 8.010 s, System: 0.060 s]\n  Range (min \u{2026} max):    7.784 s \u{2026}  8.684 s    10 runs That\'s better but still not great. We could apply the tweaks suggested by Intel, but that probably would not give us the order of magnitude improvement we need. They cite x1.2 to x1.5 improvements in their article. We need more. So... did you know that in all likelihood, your CPU has dedicated silicon to accelerate SHA1 computations? Let\'s use that! We paid for it, we get to use it! Intel SHA extension implementation Despite the \'Intel\' name, Intel as well as AMD CPUs have been shipping with this extension , since around 2016-2017. It adds a few SIMD instructions dedicated to compute SHA1 (and SHA256, and other variants). Note that ARM also has an equivalent (albeit incompatible, of course) extension so the same can be done there. There is an irony here, because 2017 is also the year where the first SHA1 public collision was published, which incited many developers to move away from SHA1... The advantage is that the structure of the code can remain the same: we still are using 128 bits SIMD registers, still computing SHA1 chunks of 64 bytes at a time. It\'s just that a few operations get faster and the code is generally shorter and clearer, and the main part is branchless. The implementation is a pure work of art, and comes from this Github repository . I have commented lots of it for clarity. Explanations The unit of work here is still 128 bits (or 4 uint32_t ). Unfortunately, the SHA1 state that we are continuously updating, and from which the final digest is extracted, is 5 uint32_t . So we are in a pickle since it does not fit neatly in one SIMD register.  Thus, we have to do one SIMD operation on the first 4 uint32_t , named ABCD , and another one with the last uint32_t , named E . So this second operation is a bit wasteful: our 128 bits only contain 1/4 of useful data, and our CPU does computations on a bunch of zeroes which will be thrown away. But there is no other way: SIMD uses a different set of registers from the standard ones. We want to stay in SIMD land as much as possible, that\'s where the performance is. Endianness conversion is done with one SIMD instruction, same as before (so 4 uint32_t at a time). The SHA Intel extension provides 4 operations: sha1rnds4 to compute the next ABCD state sha1nexte : to compute the next E state (remember, E is alone in its 128 bits register) sha1msg1 and sha1msg2 : they perform the SHA1 computations solely based on the input data Thus we alternate between SHA1 computations with sha1msg1/sha1msg2 , and state calculations with sha1rnds4/sha1nexte , always 4 uint32_t at a time. What\'s a &quot;SHA computation&quot;? It\'s basically a recombination, or shuffling, of its input. For example, sha1msg1 in pseudo-code does: W0 &lt;- SRC1[127:96] ;\n  W1 &lt;- SRC1[95:64] ;\n  W2 &lt;- SRC1[63: 32] ;\n  W3 &lt;- SRC1[31: 0] ;\n  W4 &lt;- SRC2[127:96] ;\n  W5 &lt;- SRC2[95:64] ;\n  DEST[127:96] &lt;- W2 XOR W0;\n  DEST[95:64] &lt;- W3 XOR W1;\n  DEST[63:32] &lt;- W4 XOR W2;\n  DEST[31:0] &lt;- W5 XOR W3; The first 16 rounds, we do that on the input data (i.e. the download file). But for the remaining rounds (SHA1 does 80 rounds for a 64 byte chunk), the input is computations from previous rounds. sha1msg2 does slightly different computations but still very similar. The code SHA1 with the Intel SHA extension // Process as many 64 bytes chunks as possible.\n[[maybe_unused]]\nstatic void sha1_sha_ext(uint32_t state[5], const uint8_t data[],\n                         uint32_t length) {\n  __m128i ABCD, ABCD_SAVE, E0, E0_SAVE, E1;\n  __m128i MSG0, MSG1, MSG2, MSG3;\n  const __m128i MASK =\n      // As 16 u8: `0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15`.\n      _mm_set_epi64x(0x0001020304050607ULL, 0x08090a0b0c0d0e0fULL);\n\n  /* Load initial values */\n  ABCD = _mm_loadu_si128((const __m128i *)(void *)state);\n  E0 = _mm_set_epi32((int)state[4], 0, 0, 0);\n\n  // Transform state to big-endian.\n  ABCD = _mm_shuffle_epi32(ABCD, 0x1B);\n\n  while (length &gt;= 64) {\n    /* Save current state  */\n    ABCD_SAVE = ABCD;\n    E0_SAVE = E0;\n\n    /* Rounds 0-3 */\n    // Load `data[0:16]`.\n    MSG0 = _mm_loadu_si128((const __m128i *)(void *)(data + 0));\n\n    // Convert MSG0 to big-endian.\n    MSG0 = _mm_shuffle_epi8(MSG0, MASK);\n    // E0 += MSG0\n    E0 = _mm_add_epi32(E0, MSG0);\n    E1 = ABCD;\n    //  Perform 4 rounds of SHA1 operation.\n    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E0, 0);\n\n    /* Rounds 4-7 */\n    // Load `data[16:32]`.\n    MSG1 = _mm_loadu_si128((const __m128i *)(void *)(data + 16));\n    // Convert to big-endian.\n    MSG1 = _mm_shuffle_epi8(MSG1, MASK);\n    // Compute the SHA1 state variable E after 4 rounds.\n    // It is added to the source operand (`E1`).\n    E1 = _mm_sha1nexte_epu32(E1, MSG1);\n    E0 = ABCD;\n\n    //  Perform 4 rounds of SHA1 operation.\n    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E1, 0);\n    // Perform the intermediate calculation for the next four SHA1 message dwords (128 bits).\n    MSG0 = _mm_sha1msg1_epu32(MSG0, MSG1);\n\n    /* Rounds 8-11 */\n    // Load `data[32:48]`.\n    MSG2 = _mm_loadu_si128((const __m128i *)(void *)(data + 32));\n    // Convert to big-endian.\n    MSG2 = _mm_shuffle_epi8(MSG2, MASK);\n    // Compute the SHA1 state variable E after 4 rounds.\n    E0 = _mm_sha1nexte_epu32(E0, MSG2);\n    E1 = ABCD;\n    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E0, 0);\n    MSG1 = _mm_sha1msg1_epu32(MSG1, MSG2);\n    MSG0 = _mm_xor_si128(MSG0, MSG2);\n\n    /* Rounds 12-15 */\n    // Load `data[48:64]`.\n    MSG3 = _mm_loadu_si128((const __m128i *)(void *)(data + 48));\n    // Convert to big-endian.\n    MSG3 = _mm_shuffle_epi8(MSG3, MASK);\n    // Compute the SHA1 state variable E after 4 rounds.\n    E1 = _mm_sha1nexte_epu32(E1, MSG3);\n    E0 = ABCD;\n    // Perform a final calculation for the next four SHA1 message dwords.\n    MSG0 = _mm_sha1msg2_epu32(MSG0, MSG3);\n    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E1, 0);\n    MSG2 = _mm_sha1msg1_epu32(MSG2, MSG3);\n    MSG1 = _mm_xor_si128(MSG1, MSG3);\n\n    /* Rounds 16-19 */\n    E0 = _mm_sha1nexte_epu32(E0, MSG0);\n    E1 = ABCD;\n    MSG1 = _mm_sha1msg2_epu32(MSG1, MSG0);\n    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E0, 0);\n    MSG3 = _mm_sha1msg1_epu32(MSG3, MSG0);\n    MSG2 = _mm_xor_si128(MSG2, MSG0);\n\n    /* Rounds 20-23 */\n    E1 = _mm_sha1nexte_epu32(E1, MSG1);\n    E0 = ABCD;\n    MSG2 = _mm_sha1msg2_epu32(MSG2, MSG1);\n    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E1, 1);\n    MSG0 = _mm_sha1msg1_epu32(MSG0, MSG1);\n    MSG3 = _mm_xor_si128(MSG3, MSG1);\n\n    /* Rounds 24-27 */\n    E0 = _mm_sha1nexte_epu32(E0, MSG2);\n    E1 = ABCD;\n    MSG3 = _mm_sha1msg2_epu32(MSG3, MSG2);\n    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E0, 1);\n    MSG1 = _mm_sha1msg1_epu32(MSG1, MSG2);\n    MSG0 = _mm_xor_si128(MSG0, MSG2);\n\n    /* Rounds 28-31 */\n    E1 = _mm_sha1nexte_epu32(E1, MSG3);\n    E0 = ABCD;\n    MSG0 = _mm_sha1msg2_epu32(MSG0, MSG3);\n    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E1, 1);\n    MSG2 = _mm_sha1msg1_epu32(MSG2, MSG3);\n    MSG1 = _mm_xor_si128(MSG1, MSG3);\n\n    /* Rounds 32-35 */\n    E0 = _mm_sha1nexte_epu32(E0, MSG0);\n    E1 = ABCD;\n    MSG1 = _mm_sha1msg2_epu32(MSG1, MSG0);\n    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E0, 1);\n    MSG3 = _mm_sha1msg1_epu32(MSG3, MSG0);\n    MSG2 = _mm_xor_si128(MSG2, MSG0);\n\n    /* Rounds 36-39 */\n    E1 = _mm_sha1nexte_epu32(E1, MSG1);\n    E0 = ABCD;\n    MSG2 = _mm_sha1msg2_epu32(MSG2, MSG1);\n    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E1, 1);\n    MSG0 = _mm_sha1msg1_epu32(MSG0, MSG1);\n    MSG3 = _mm_xor_si128(MSG3, MSG1);\n\n    /* Rounds 40-43 */\n    E0 = _mm_sha1nexte_epu32(E0, MSG2);\n    E1 = ABCD;\n    MSG3 = _mm_sha1msg2_epu32(MSG3, MSG2);\n    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E0, 2);\n    MSG1 = _mm_sha1msg1_epu32(MSG1, MSG2);\n    MSG0 = _mm_xor_si128(MSG0, MSG2);\n\n    /* Rounds 44-47 */\n    E1 = _mm_sha1nexte_epu32(E1, MSG3);\n    E0 = ABCD;\n    MSG0 = _mm_sha1msg2_epu32(MSG0, MSG3);\n    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E1, 2);\n    MSG2 = _mm_sha1msg1_epu32(MSG2, MSG3);\n    MSG1 = _mm_xor_si128(MSG1, MSG3);\n\n    /* Rounds 48-51 */\n    E0 = _mm_sha1nexte_epu32(E0, MSG0);\n    E1 = ABCD;\n    MSG1 = _mm_sha1msg2_epu32(MSG1, MSG0);\n    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E0, 2);\n    MSG3 = _mm_sha1msg1_epu32(MSG3, MSG0);\n    MSG2 = _mm_xor_si128(MSG2, MSG0);\n\n    /* Rounds 52-55 */\n    E1 = _mm_sha1nexte_epu32(E1, MSG1);\n    E0 = ABCD;\n    MSG2 = _mm_sha1msg2_epu32(MSG2, MSG1);\n    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E1, 2);\n    MSG0 = _mm_sha1msg1_epu32(MSG0, MSG1);\n    MSG3 = _mm_xor_si128(MSG3, MSG1);\n\n    /* Rounds 56-59 */\n    E0 = _mm_sha1nexte_epu32(E0, MSG2);\n    E1 = ABCD;\n    MSG3 = _mm_sha1msg2_epu32(MSG3, MSG2);\n    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E0, 2);\n    MSG1 = _mm_sha1msg1_epu32(MSG1, MSG2);\n    MSG0 = _mm_xor_si128(MSG0, MSG2);\n\n    /* Rounds 60-63 */\n    E1 = _mm_sha1nexte_epu32(E1, MSG3);\n    E0 = ABCD;\n    MSG0 = _mm_sha1msg2_epu32(MSG0, MSG3);\n    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E1, 3);\n    MSG2 = _mm_sha1msg1_epu32(MSG2, MSG3);\n    MSG1 = _mm_xor_si128(MSG1, MSG3);\n\n    /* Rounds 64-67 */\n    E0 = _mm_sha1nexte_epu32(E0, MSG0);\n    E1 = ABCD;\n    MSG1 = _mm_sha1msg2_epu32(MSG1, MSG0);\n    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E0, 3);\n    MSG3 = _mm_sha1msg1_epu32(MSG3, MSG0);\n    MSG2 = _mm_xor_si128(MSG2, MSG0);\n\n    /* Rounds 68-71 */\n    E1 = _mm_sha1nexte_epu32(E1, MSG1);\n    E0 = ABCD;\n    MSG2 = _mm_sha1msg2_epu32(MSG2, MSG1);\n    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E1, 3);\n    MSG3 = _mm_xor_si128(MSG3, MSG1);\n\n    /* Rounds 72-75 */\n    E0 = _mm_sha1nexte_epu32(E0, MSG2);\n    E1 = ABCD;\n    MSG3 = _mm_sha1msg2_epu32(MSG3, MSG2);\n    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E0, 3);\n\n    /* Rounds 76-79 */\n    E1 = _mm_sha1nexte_epu32(E1, MSG3);\n    E0 = ABCD;\n    ABCD = (__m128i)_mm_sha1rnds4_epu32(ABCD, E1, 3);\n\n    /* Combine state */\n    E0 = _mm_sha1nexte_epu32(E0, E0_SAVE);\n    // ABCD += ABCD_SAVE\n    ABCD = _mm_add_epi32(ABCD, ABCD_SAVE);\n\n    data += 64;\n    length -= 64;\n  }\n\n  /* Save state */\n  // Convert back to little-endian.\n  ABCD = _mm_shuffle_epi32(ABCD, 0x1B);\n  _mm_storeu_si128((__m128i *)(void *)state, ABCD);\n  // Convert back to little-endian.\n  state[4] = (uint32_t)_mm_extract_epi32(E0, 3);\n} Our is_piece_valid function is practically identical to the last section: static bool is_piece_valid(uint8_t *piece, uint64_t piece_len,\n                           uint8_t digest_expected[20]) {\n  SHA1_CTX ctx = {0};\n  SHA1Init(&amp;ctx);\n\n  // Process as many SHA1 64 bytes chunks as possible.\n  uint64_t len_rounded_down = (piece_len / 64) * 64;\n  uint64_t rem = piece_len % 64;\n  sha1_sha_ext(ctx.state, piece, (uint32_t)len_rounded_down);\n\n  memcpy(ctx.buffer, piece + len_rounded_down, rem);\n\n  ctx.count = piece_len * 8;\n\n  uint8_t digest_actual[20] = {0};\n  SHA1Final(digest_actual, &amp;ctx);\n\n  return !memcmp(digest_actual, digest_expected, 20);\n} Results How fast? $ hyperfine --warmup 3 \'./a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent\'\nBenchmark 1: ./a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent\n  Time (mean \u{b1} \u{3c3}):     866.9 ms \u{b1}  17.4 ms    [User: 809.6 ms, System: 54.4 ms]\n  Range (min \u{2026} max):   839.7 ms \u{2026} 901.4 ms    10 runs Now that\'s what I\'m talking about, ~ 571 MiB/s . Around a 10x speed-up compared to the basic SSE implementation! And now we are running under a second. Also the variability is much reduced which is nice. What about a release build (without Address Sanitizer), for comparison? This is the SIMD-less version with -O2 -march=native , benefiting from some auto-vectorization: $ hyperfine --warmup 3 \'./a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent\'\nBenchmark 1: ./a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent\n  Time (mean \u{b1} \u{3c3}):     617.8 ms \u{b1}  20.9 ms    [User: 573.6 ms, System: 42.2 ms]\n  Range (min \u{2026} max):   598.7 ms \u{2026} 669.1 ms    10 runs That\'s ~ 802 MiB/s . And this is the code using the SHA extension, again with -O2 -march=native : $ hyperfine --warmup 3 \'./a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent\'\nBenchmark 1: ./a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent\n  Time (mean \u{b1} \u{3c3}):     281.2 ms \u{b1}   5.4 ms    [User: 240.6 ms, System: 39.6 ms]\n  Range (min \u{2026} max):   276.1 ms \u{2026} 294.3 ms    10 runs That\'s ~ 1.8 GiB/s . Unsurprisingly, when inspecting the generated assembly code for the SIMD-less version, the auto-vectorization is very limited and does not use the SHA extension (compilers are smart, but not that smart). As such, it\'s still very impressive that it reaches such a high performance. My guess is that the compiler does a good job at analyzing data dependencies and reordering statements to maximize utilization. Also, SHA1 does a lot of bit rotation, and the compiler makes heavy use of the ror , rol , and shr instructions to do just that instead of doing multiple naive bit operations like in the unoptimized code. The version using the SHA extension performs very well, be it in debug + Address Sanitizer mode, or release mode. Also, in both versions, as the SHA1 code got much faster, we start to see on the CPU profile mmap show up, as confirmed by the system time part becoming a fifth of the whole runtime. That means that we are starting to be limited by I/O. Which is good!  Using hdparm to measure my disk performance, I get: Timing buffered disk reads: 6518 MB in  3.00 seconds = 2171.34 MB/sec Since our benchmark first warms up a few times, we know that the file data is in the cache, so buffered disk reads seems like a good metric to go by. Thus, our program performance is near the disk I/O limit for cached reads. Sounds pretty good to me! I tried to give the OS some hints to improve a bit on that front with madvise(file_download_data, file_download_size, MADV_SEQUENTIAL | MADV_WILLNEED) , but it did not have any impact on the timings. OpenSSL hand crafted assembly implementation The whole point of this article is to do SHA1 computations from scratch and avoid dependencies. Let\'s see how OpenSSL (in this case, aws-lc but I don\'t believe they changed that part at all) fares out of curiosity. $ hyperfine --warmup 3 \'./a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent\'\nBenchmark 1: ./a.out ./NetBSD-9.4-amd64.iso ~/Downloads/NetBSD-9.4-amd64.iso.torrent\n  Time (mean \u{b1} \u{3c3}):     281.5 ms \u{b1}   3.9 ms    [User: 245.7 ms, System: 35.1 ms]\n  Range (min \u{2026} max):   276.3 ms \u{2026} 288.9 ms    10 runs So, the performance is essentially identical to our version. Pretty good. OpenSSL picks at runtime the best code path based on what features the CPU supports. Interestingly on my system, even when compiled with -march=native , it does not decide to use the SHA extension, and instead goes for hand-optimized SIMD. That\'s mind-blowing that this SIMD code performs as well that dedicated silicon, including the cycles spent on the runtime check. So props to the developers! I can see really low-level tricks like prefetcht0 to ask for the prefetcher to cache some data ahead of time to reduce latency. And they mention they had help from some folks at Intel. Additional improvements I have not talked about AVX2, AVX512, etc. These could be fun to implement and benchmark. If you are interested in this kind of thing, the OpenSSL project (and the various clones and forks) has a Perl script to generate assembly code to do SHA1 with various variants of SIMD and SHA extension. I think the numbers are pretty dated but it\'s a goldmine of information. Oh, and I almost forgot: we can compute SHA1 on the GPU ! Conclusion That was a fun deep dive about performance, SIMD, and a deprecated hash algorithm that is still in use in many applications (e.g. Git). What I have learned is that Address Sanitizer really likes SIMD code because it reduces significantly the runtime checks it has to do, and thus the performance impact is greatly reduced. It is often recommended to do fuzzing with Address Sanitizer on, so performance matters here. SIMD code is like a math puzzle, it\'s weird and fun. I\'m happy that I finally had my first real contact with it. It has the useful property to have a very stable performance across runs. I hope I did not get anything wrong in this article. And it\'s wild to see different implementations range from 30s to 300 ms (a factor of 100!) to do the same thing. Also, optimizers these days are god damn impressive. If you want to learn more about SIMD, I recommend this talk from the Titanfall developers at GDC where they explain how they use a lot of SIMD in their game engine, but also their thought process to go from a standard procedural code to a SIMD version: ",
titles:[
{
title:"Why is it a problem at all and how did it come to be?",
slug:"why-is-it-a-problem-at-all-and-how-did-it-come-to-be",
offset:2896,
},
{
title:"Non-SIMD implementation",
slug:"non-simd-implementation",
offset:5659,
},
{
title:"Explanation",
slug:"explanation",
offset:8781,
},
{
title:"The code",
slug:"the-code",
offset:11006,
},
{
title:"Results",
slug:"results",
offset:18230,
},
{
title:"Possible optimizations",
slug:"possible-optimizations",
offset:18981,
},
{
title:"SIMD (SSE) implementation",
slug:"simd-sse-implementation",
offset:21458,
},
{
title:"Explanation",
counter:1,
slug:"explanation",
offset:22339,
},
{
title:"The code",
counter:1,
slug:"the-code",
offset:24018,
},
{
title:"Results",
counter:1,
slug:"results",
offset:39049,
},
{
title:"Intel SHA extension implementation",
slug:"intel-sha-extension-implementation",
offset:39915,
},
{
title:"Explanations",
slug:"explanations",
offset:40843,
},
{
title:"The code",
counter:2,
slug:"the-code",
offset:42780,
},
{
title:"Results",
counter:2,
slug:"results",
offset:50392,
},
{
title:"OpenSSL hand crafted assembly implementation",
slug:"openssl-hand-crafted-assembly-implementation",
offset:53424,
},
{
title:"Additional improvements",
slug:"additional-improvements",
offset:54664,
},
{
title:"Conclusion",
slug:"conclusion",
offset:55113,
},
],
},
{
html_file_name:"tip_of_the_day_5.html",
title:"Tip of the day #5: Install Go tools with a specific version",
text:"I had an issue with Go tools around versioning, and here\'s how I solved it. It could be useful to others. This was the error: $ staticcheck  ./...\n-: module requires at least go1.23.6, but Staticcheck was built with go1.23.1 (compile)\n$ go version\ngo version go1.23.6 linux/amd64 Indeed the project was specifying go 1.23.6 in go.mod . Even after removing the staticcheck binary and re-installing it I still had the same issue: $ which staticcheck\n/home/pg/go/bin/staticcheck\n$ rm /home/pg/go/bin/staticcheck\n$ which staticcheck\nwhich: no staticcheck \n$ go install honnef.co/go/tools/cmd/staticcheck@v0.5.1\n$ staticcheck  ./...\n-: module requires at least go1.23.6, but Staticcheck was built with go1.23.1 (compile) I even tried the -a flag for go install to force a clean build (since go install fetches the sources and builds them) to no avail. Solution: following https://go.dev/doc/manage-install , I installed the specific version of Go I needed and used that to install the tool: $ go install golang.org/dl/go1.23.6@latest\n$ go1.23.6 download\n$ go1.23.6 install honnef.co/go/tools/cmd/staticcheck@v0.5.1\n$ staticcheck  -tags=integration_tests ./... # Works! That was a TIL for me. Note that Go 1.24 supports the project listing tools directly in go.mod which would probably solve this issue directly. ",
titles:[
],
},
{
html_file_name:"making_my_static_blog_generator_11_times_faster.html",
title:"Making my static blog generator <del>11</del> 33 times faster",
text:"This blog is statically generated from Markdown files. It used to be fast, but nowadays it\'s not: $ hyperfine --warmup 2 ./master.bin \nBenchmark 1: ./master.bin\n  Time (mean \u{b1} \u{3c3}):      1.873 s \u{b1}  0.053 s    [User: 1.351 s, System: 0.486 s]\n  Range (min \u{2026} max):    1.806 s \u{2026}  1.983 s    10 runs ~ 2 seconds is not the end of the world, but it\'s just enough to be annoying when doing lots of edit-view cycles. Worse, it seemed to become slower and slower as I wrote more articles. So today I finally dedicated some time to tackle this problem. The investigation In the early days of this blog, there were only a few articles, and the build process was a simple Makefile, something like this (simplified): %.html: %.md header.html footer.html\n        cat header.html &gt;&gt; $@\n        pandoc --toc $&lt; &gt;&gt; $@\n        cat footer.html &gt;&gt; $@ For each markdown file, say wayland_from_scratch.md , we transform the markdown into HTML (at the time with pandoc , which proved to be super slow, now with cmark which is extremely fast) and save that in the file wayland_from_scratch.html , with a HTML header prepended and footer appended. Later on, I added the publication date: %.html: %.md header.html footer.html\n        cat header.html &gt;&gt; $@\n        printf \'&lt;p id=&quot;publication_date&quot;&gt;Published on %s.&lt;/p&gt;\\n\' $$(git log --format=\'%as\' --reverse -- $&lt; | head -n1)  &gt;&gt; $@; fi\n        pandoc --toc $&lt; &gt;&gt; $@\n        cat footer.html &gt;&gt; $@ The publication date is the creation date, that is: the date of the first Git commit for this file. So we ask Git for the list of commits for this file (they are provided by default from newest to oldest, so we --reverse the list), take the first one with head , done. It\'s simple. Note: My initial approach was to get the creation and modification date from the file system, but it\'s incorrect, as soon as you work on more than one machine. The way Git works is that when you pull commits that created a file, it creates the file on the file system and does not try to hack the creation date. Thus the file creation date is the time of the Git pull, not the date of the commit that first created it. As I added more and more features to this blog, like a list of article by tags, a home page that automatically lists all of the articles, a RSS feed, the \'last modified\' date for an article, etc, I outgrew the Makefile approach and wrote a small program (initially in Zig, then in Odin , now in C) to do all that. But the core approach remained: List all markdown files in the current directory (e.g. ls *.md , the Makefile did that for us with %.md ) For each markdown file, sequentially: Run git log article.md to get the date of the first and last commits for this file (respectively \'created at\' and \'modified at\') Convert the markdown content to HTML Save this HTML to article.html For long time, it was all good. It was single-threaded, but plenty fast. So I wrote more and more articles. But now it\'s too slow. Why? Let\'s profile it: Yeah...I think it might be [git] [git] [git] [git] [git] [git] [git] [git]... Another way to confirm this is with strace : $ strace --summary-only ./src.bin\n% time     seconds  usecs/call     calls    errors syscall\n------ ----------- ----------- --------- --------- ----------------\n 94.85    0.479290        3928       122           waitid\n[...] So ~ 95 % of the running time is spent waiting on a subprocess. It\'s mainly git - we also run cmark as a subprocess but it\'s really really fast. We could further investigate with strace which process we are waiting on but the CPU profile already points the finger at Git and cmark is not even visible on it. At this point it\'s important to mention that this program is a very simplistic static site generator: it is stateless and processes every markdown file in the repository one by one. You could say that it\'s a regression compared to the Makefile because make has built-in parallelism with -j and change detection. But in reality, change detection in make is primitive and I often want to reprocess everything because of a change that applies to every file. For example, I reword the Donate section at the end of each article (wink wink), or the header, or the footer, etc. Also, I really am fond of this \'pure function\' approach. There is no caching issue to debug, no complicated code to write, no data races, no async callbacks, etc. My performance target was to process every file within 1s, or possibly even 0.5s. I could see a few options: Do not block on git log . We can use a thread pool, or an asynchronous approach to spawn all the git processes at once, and wait for all of them to finish. But it\'s more complex. Implement caching so that only the changed markdown files get regenerated. Make git log faster somehow. The last option was my preferred one because it did not force me to re-architect the whole program. Note that the other two options are sill on the table regardless of whether our experiment works out or not. When there\'s one, there\'s many My intuition was to do a deep dive in the git log options, to see if I could instruct it to do less work. But then something struck me: we invoke git log to get all commits for one markdown file (even though we only are interested in the first and last, but that\'s the only interface Git provides us as far as I know). What if we invoked it once for all markdown files ? Yes, the output might be a bit big... How big? Is it really faster? Let\'s measure! Conceptually we can simply do git log \'*.md\' and parse the output. We can refine that approach later with more options, but that\'s the gist of it: $ time git log \'*.md\' | wc -c\n191196\n\n________________________________________________________\nExecuted in   73.69 millis    fish           external\n   usr time   61.04 millis  738.00 micros   60.30 millis\n   sys time   15.95 millis  191.00 micros   15.76 millis So it\'s much faster than doing it per file, and also it\'s entire output is ~ 186 KiB. And these numbers should grow very slowly because each new commit only adds 20-100 bytes to the output. Looks like we got our solution. There is one added benefit: we do not need to list all .md files in the directory at startup. Git gives us this information (in my case there are no markdown files not tracked by Git). Mike Acton and Data Oriented Design are right once again: Rule of thumb: When there is one, there is many. 1 Or: try to think in terms of arrays, not in terms of one isolated object at a time. The new approach We only want git to tell us, for each commit: The date Which files were affected Hence we pass to git log : --format=\'%aI\' to get the date in ISO format --name-status to know which files this commit added ( A ), modified ( M ), deleted ( D ), or renamed ( R ) --no-merges to skip merge commits since they do not directly affect any file --diff-filter=AMRD to only get commits that add/modify/delete/rename files. We are not interested in commits changing the permissions on a file, or modifying symbolic links, etc. With these options we get even better numbers: $ time git log --format=\'%aI\' --name-status --no-merges --diff-filter=AMDR  -- \'*.md\' | wc -c\n77832\n\n________________________________________________________\nExecuted in  108.38 millis    fish           external\n   usr time   83.70 millis  231.00 micros   83.47 millis\n   sys time   27.99 millis  786.00 micros   27.20 millis The output looks like this (I annotated each part along with the commit number): 2024-11-05T15:43:44+01:00                                                            | [1] A commit starts with the date.\n                                                                                     | [1] Empty line\nM       how_to_rewrite_a_cpp_codebase_successfully.md                                | [1] A list of files affected by this commit.\nM       write_a_video_game_from_scratch_like_1987.md                                 | [1] Each starts with a letter describing the action.\nM       x11_x64.md                                                                   | [1] Here it\'s all modifications.\nM       you_inherited_a_legacy_cpp_codebase_now_what.md                              | [1]\n2025-02-02T22:54:23+01:00                                                            | [2] The second entry starts.\n                                                                                     | [2] \nR100    cross-platform-timers.md        the_missing_cross_platform_api_for_timers.md | [2] Rename with the (unneeded) confidence score.\n[...]                                                                                | Etc. Parsing this commit log is tedious but not extremely difficult. We maintain a map while inspecting each commit: map&lt;Path, (creation_date, modification_date, tombstone)&gt; . In case of a rename or delete, we set the tombstone to true . Why not remove the entry from the map directly? Well, we are inspecting the list of commits from newest to oldest.\nSo first we\'ll encounter the delete/rename commit for this file, and then later in the stream, a number of add/modify commits. When we are done, we need to remember that this markdown file should be ignored, otherwise, we\'ll try to open it, read it, and convert it to HTML, but we\'ll get a ENOENT error because it does not exist anymore on disk. We could avoid having this tombstone field and just bail on ENOENT , that\'s a matter of taste I guess, but this field was useful to me to ensure that the parsing code is correct. Alternatively, we could pass --reverse to git log and parse the commits in chronological order. When we see a delete/rename commit for a file, we can safely remove the entry from the map since no more commits about this file should show up after that. The new implementation GitStat :: struct {\n\tcreation_date:     string,\n\tmodification_date: string,\n\tpath_rel:          string,\n}\n\nget_articles_creation_and_modification_date :: proc() -&gt; ([]GitStat, os2.Error) {\n\tfree_all(context.temp_allocator)\n\tdefer free_all(context.temp_allocator)\n\n\tstate, stdout_bin, stderr_bin, err := os2.process_exec(\n\t\t{\n\t\t\tcommand = []string {\n\t\t\t\t&quot;git&quot;,\n\t\t\t\t&quot;log&quot;,\n\t\t\t\t// Print the date in ISO format.\n\t\t\t\t&quot;--format=\'%aI\'&quot;,\n\t\t\t\t// Ignore merge commits since they do not carry useful information.\n\t\t\t\t&quot;--no-merges&quot;,\n\t\t\t\t// Only interested in creation, modification, renaming, deletion.\n\t\t\t\t&quot;--diff-filter=AMRD&quot;,\n\t\t\t\t// Show which modification took place:\n\t\t\t\t// A: added, M: modified, RXXX: renamed (with percentage score), etc.\n\t\t\t\t&quot;--name-status&quot;,\n\t\t\t\t&quot;*.md&quot;,\n\t\t\t},\n\t\t},\n\t\tcontext.temp_allocator,\n\t)\n\tif err != nil {\n\t\tfmt.eprintf(&quot;git failed: %d %v %s&quot;, state, err, string(stderr_bin))\n\t\tpanic(&quot;git failed&quot;)\n\t}\n\n\tstdout := strings.trim_space(string(stdout_bin))\n\tassert(stdout != &quot;&quot;)\n\n\tGitStatInternal :: struct {\n\t\tcreation_date:     string,\n\t\tmodification_date: string,\n\t\ttombstone:         bool,\n\t}\n\tstats_by_path := make(map[string]GitStatInternal, allocator = context.temp_allocator)\n\n\t// Sample git output:\n\t// 2024-10-31T16:09:02+01:00\n\t// \n\t// M       lessons_learned_from_a_successful_rust_rewrite.md\n\t// A       tip_of_day_3.md\n\t// 2025-02-18T08:07:55+01:00\n\t//\n\t// R100    sha.md  making_my_debug_build_run_100_times_faster.md\n\n\t// For each commit.\n\tfor {\n\t\t// Date\n\t\tdate: string\n\t\t{\n\t\t\tline := strings.split_lines_iterator(&amp;stdout) or_break\n\n\t\t\tassert(strings.starts_with(line, &quot;\'20&quot;))\n\t\t\tline_without_quotes := line[1:len(line) - 1]\n\t\t\tdate = strings.clone(strings.trim(line_without_quotes, &quot;\' \\n&quot;))\n\t\t}\n\n\t\t// Empty line\n\t\t{\n\t\t\t// Peek.\n\t\t\tline, ok := strings.split_lines_iterator(&amp;stdout)\n\t\t\tassert(ok)\n\t\t\tassert(line == &quot;&quot;)\n\t\t}\n\n\t\t// Files.\n\t\tfor {\n\t\t\t// Start of a new commit?\n\t\t\tif strings.starts_with(stdout, &quot;\'20&quot;) do break\n\n\t\t\tline := strings.split_lines_iterator(&amp;stdout) or_break\n\t\t\tassert(line != &quot;&quot;)\n\n\t\t\taction := line[0]\n\t\t\tassert(action == \'A\' || action == \'M\' || action == \'R\' || action == \'D\')\n\n\t\t\told_path: string\n\t\t\tnew_path: string\n\t\t\t{\n\t\t\t\t// Skip the \'action\' part.\n\t\t\t\t_, ok := strings.split_iterator(&amp;line, &quot;\\t&quot;)\n\t\t\t\tassert(ok)\n\n\t\t\t\told_path, ok = strings.split_iterator(&amp;line, &quot;\\t&quot;)\n\t\t\t\tassert(ok)\n\t\t\t\tassert(old_path != &quot;&quot;)\n\n\t\t\t\tif action == \'R\' { \t// Rename has two operands.\n\t\t\t\t\tnew_path, ok = strings.split_iterator(&amp;line, &quot;\\t&quot;)\n\t\t\t\t\tassert(ok)\n\t\t\t\t\tassert(new_path != &quot;&quot;)\n\t\t\t\t} else { \t// The others have only one.\n\t\t\t\t\tnew_path = old_path\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t_, v, inserted, err := map_entry(&amp;stats_by_path, new_path)\n\t\t\tassert(err == nil)\n\n\t\t\tif inserted {\n\t\t\t\tv.modification_date = date\n\t\t\t\tv.creation_date = date\n\t\t\t} else {\n\t\t\t\tassert(v.modification_date != &quot;&quot;)\n\t\t\t\t// Keep updating the creation date, when we reach the end of the commit log, it has the right value.\n\t\t\t\tv.creation_date = date\n\t\t\t}\n\n\n\t\t\t// We handle the action separately from the fact that this is the first commit we see for the path.\n\t\t\t// Because a file could have only one commit which is a rename.\n\t\t\t// Or its first commit is a rename but then there additional commits to modify it. \n\t\t\t// Case being: these two things are orthogonal.\n\n\t\t\tif action == \'R\' {\n\t\t\t\t// Mark the old path as \'deleted\'.\n\t\t\t\tstats_by_path[old_path] = GitStatInternal {\n\t\t\t\t\tmodification_date = date,\n\t\t\t\t\ttombstone         = true,\n\t\t\t\t}\n\n\t\t\t\t// The creation date of the new path is the date of the rename operation.\n\t\t\t\tv.creation_date = date\n\t\t\t}\n\t\t\tif action == \'D\' {\n\t\t\t\t// Mark as \'deleted\'.\n\t\t\t\tv.tombstone = true\n\t\t\t}\n\t\t}\n\t}\n\n\tgit_stats := make([dynamic]GitStat)\n\tfor k, v in stats_by_path {\n\t\tassert(k != &quot;&quot;)\n\t\tassert(v.creation_date != &quot;&quot;)\n\t\tassert(v.modification_date != &quot;&quot;)\n\t\tassert(v.creation_date &lt;= v.modification_date)\n\n\t\tif !v.tombstone {\n\t\t\tgit_stat := GitStat {\n\t\t\t\tpath_rel          = strings.clone(k),\n\t\t\t\tcreation_date     = strings.clone(v.creation_date),\n\t\t\t\tmodification_date = strings.clone(v.modification_date),\n\t\t\t}\n\t\t\tfmt.printf(&quot;%v\\n&quot;, git_stat)\n\t\t\tappend(&amp;git_stats, git_stat)\n\t\t}\n\t}\n\n\treturn git_stats[:], nil\n} A few things of interest: Odin has first class support for allocators so we allocate everything in this function with the temporary allocator. It is backed by an arena and emptied at the start and end of the function. Only the final result is allocated with the standard allocator. That way, even if Git starts spewing lots of data, as soon as we exit the function, all of that is gone, in one call, and the the program carries on with only the necessary data heap-allocated. In this program, the main allocator and the temporary allocator are both arenas. The memory usage is a constant ~ 4 MiB, mainly located in the Odin standard library. The memory usage of my code is around ~ 65 KiB. A map is a bit of an overkill for ~30 entries, but it\'s fine, and we expect the number of articles to grow We can log the final result: [...]\nGitStat{creation_date = &quot;2020-09-07T20:49:20+02:00&quot;, modification_date = &quot;2024-11-04T09:24:17+01:00&quot;, path_rel = &quot;compile_ziglang_from_source_on_alpine_2020_9.md&quot;}\nGitStat{creation_date = &quot;2024-09-10T12:59:04+02:00&quot;, modification_date = &quot;2024-09-12T12:14:42+02:00&quot;, path_rel = &quot;odin_and_musl.md&quot;}\nGitStat{creation_date = &quot;2023-11-23T11:26:11+01:00&quot;, modification_date = &quot;2025-02-06T20:55:27+01:00&quot;, path_rel = &quot;roll_your_own_memory_profiling.md&quot;}\n[...] Alright, so how does our new implementation fare compared to the old one? First, we can confirm with strace that the time spent on waiting for subprocesses (mainly Git) shrinked: $ strace --summary-only ./src.bin\n% time     seconds  usecs/call     calls    errors syscall\n------ ----------- ----------- --------- --------- ----------------\n 56.59    0.043176         674        64           waitid\n [...] Then we benchmark: $ hyperfine --warmup 2 \'./src-main.bin\' \'./src.bin\'\nBenchmark 1: ./src-main.bin\n  Time (mean \u{b1} \u{3c3}):      1.773 s \u{b1}  0.022 s    [User: 1.267 s, System: 0.472 s]\n  Range (min \u{2026} max):    1.748 s \u{2026}  1.816 s    10 runs\n \nBenchmark 2: ./src.bin\n  Time (mean \u{b1} \u{3c3}):     158.7 ms \u{b1}   6.6 ms    [User: 128.4 ms, System: 133.7 ms]\n  Range (min \u{2026} max):   151.7 ms \u{2026} 175.6 ms    18 runs\n \nSummary\n  ./src.bin ran\n   11.17 \u{b1} 0.48 times faster than ./src-main.bin Around 11 times faster, and well within our ideal target of 500 ms ! And all we had to do was convert many git log invocations (one per markdown file) to just one. Here\'s the CPU profile now: Overall it\'s a pretty simple change, located in one function. Almost all of the complexity is due to parsing Git custom text output and skipping over irrelevant commits. We don\'t really have a choice either: that\'s all Git provides to query the commit log. The alternatives are all worse: Parse directly the Git object files - no thank you Use a library (e.g. libgit2 ) and hope that it offers a saner interface to query the commit log I wonder if there is a better way... Fossil fossil is an alternative version control system created by the same folks that created, and are still working on, SQLite. Naturally, a fossil repository is basically just one SQLite database file. That sounds very queryable ! Let\'s import our git repository into a Fossil repository and enter the SQLite prompt: $ git fast-export --all | fossil import --git new-repo.fossil\n$ file new-repo.fossil \nnew-repo.fossil: SQLite 3.x database (Fossil repository), [...]\n$ fossil sql -R new-repo.fossil There are lots of tables in this database. We craft this query after a few trials and errors (don\'t know if it is optimal or not): sqlite&gt; .mode json\nsqlite&gt; SELECT \n            f.name as filename,\n            datetime(min(e.mtime)) as creation_date,\n            datetime(max(e.mtime)) as last_modified\n        FROM repository.filename f\n        JOIN repository.mlink m ON f.fnid = m.fnid\n        JOIN repository.event e ON m.mid = e.objid\n        WHERE filename LIKE \'%.md\'\n        GROUP BY f.name\n        ORDER BY f.name; Which outputs what we want: [...]\n{&quot;filename&quot;:&quot;body_of_work.md&quot;,&quot;creation_date&quot;:&quot;2023-12-19 13:27:40&quot;,&quot;last_modified&quot;:&quot;2024-11-05 15:11:55&quot;},\n{&quot;filename&quot;:&quot;communicate_by_sharing_code.md&quot;,&quot;creation_date&quot;:&quot;2024-03-07 09:48:39&quot;,&quot;last_modified&quot;:&quot;2024-03-07 10:14:09&quot;},\n{&quot;filename&quot;:&quot;compile_ziglang_from_source_on_alpine_2020_9.md&quot;,&quot;creation_date&quot;:&quot;2020-09-07 18:49:20&quot;,&quot;last_modified&quot;:&quot;2024-11-04 08:24:17&quot;},\n[...] Note that this does not filter out deleted/removed files yet. I\'m sure that it can be done by tweaking the query a bit, but there\'s not time! We need to benchmark! $ hyperfine --shell=none \'fossil sql -R new-repo.fossil &quot;SELECT [...]&quot;\'\nBenchmark 1: fossil sql -R new-repo.fossil &quot;[...]&quot;\n  Time (mean \u{b1} \u{3c3}):       3.0 ms \u{b1}   0.5 ms    [User: 1.5 ms, System: 1.4 ms]\n  Range (min \u{2026} max):     2.2 ms \u{2026}   5.6 ms    776 runs Damn that\'s fast. I do not use Fossil, but I eye it from time to time - generally when I need to extract some piece of information from Git and I discover it does not let me, or when I see the Gitlab bill  my (ex-) company pays, or when the Jira page takes more than 10 seconds to load... yeah, Fossil is the complete package, with issues, forums, a web UI, a timeline, a wiki, a chat... it has it all! But the golden ticket idea is really to store everything inside SQLite. Suddenly, we can query anything! And there is no weird parsing needed - SQLite supports various export formats and (some? all?) fossil commands support the --sql option to show you which SQL query they use to get the information. After all, the only thing the fossil command line does in this case, is craft a SQL query and run it on the SQLite database. It\'s quite magical to me that I can within a few seconds import my 6 years-long git repository into a SQLite database and start querying it, and the performance is great. Now I am not quite ready yet to move to Fossil, and the import is a one time thing as far as I know, so it is not a viable option for the problem at hand as long as git is the source of truth. But still, while I was trying to tackle git log into submission, I was thinking the whole time: why can\'t I do an arbitrary query of git data? Generally, the more generic approach is slower than the ad-hoc one, but here it\'s not even the case. Fossil is for this use case objectively more powerful, more generic, and faster. Conclusion The issue was effectively a N+1 query problem. We issued a separate \'query\' (in this case, git log ) for each markdown file, in a sequential blocking fashion. This approach worked until it didn\'t because the number of entities (i.e. articles, and commits) grew over time. The solution is instead to do only one query for all entities. It may return a bit more data that we need, but that\'s much faster, and scales better, than the original version. It\'s obvious in retrospect but it\'s easy to let it happen when the \'codebase\' (a big word for only one file that started as a basic Makefile) is a few years old, it\'s only looked at briefly from time to time, and the initial implementation did not really allow for the correct approach - who wants to parse the git log output in the Makefile language? Furthermore, the initial approach was fine because it only looked at the creation date, so we could do git log --reverse article.md | head -n1 which is faster than sifting through the whole commit log for this file. However, as it is always the case, requirements (again, a big word for: my taste concerning what should my personal blog look like) change and the modification date now had to also be extracted from git. This forced us, with the current Git functionality, to scan through the whole commit log, for each file, which became too slow. As Mike Acton and Data Oriented Design state: Different problems require different solutions. 2 And: If you have different data, you have a different problem. 3 It also does not help that any querying in Git is ad-hoc and outputs a weird text format that we have to tediously parse. Please everyone, let\'s add the option to output structured data in our command line programs, damn it! String programming is no fun at all - that\'s why I moved away from concatenating the output of shell commands in a Makefile, to a real programming language, to do the static generation. All in all, I am pleased with my solution - I can now see any edit materialize instantly . It\'s a bit funny that my previous article was about SIMD and inspecting assembly instructions, while this issue is so obvious and high-level in retrospect. To the next 5 years of blogging, till I need to revisit the performance of this function! Addendum Running git gc and git prune also helps because all unreachable objects are removed, so git has to do less work scanning and parsing them on disk. Having done that, we get almost twice as fast: $ hyperfine --shell=none --warmup 2 \'./src.bin\'\nBenchmark 1: ./src.bin\n  Time (mean \u{b1} \u{3c3}):      89.7 ms \u{b1}   2.6 ms    [User: 63.6 ms, System: 59.5 ms]\n  Range (min \u{2026} max):    85.0 ms \u{2026}  94.3 ms    31 runs But we have to remember to run it frequently or set up a periodic job that does it for us. That\'s a ~21x speed-up from the original time. Addendum 2 Since spawning the cmark process, having cmark parsing the command line options, over and over, is still taking a good chunk of the time, we can switch to using libcmark directly. This is something I wanted to do anyway to extract a table of content, etc from the markdown. Since the running time is getting lower and lower, we add --shell=none and increase the warm-up to reduce statistical outliers: $ hyperfine --shell=none --warmup 10 ./src.bin\nBenchmark 1: ./src.bin\n  Time (mean \u{b1} \u{3c3}):      55.4 ms \u{b1}   1.5 ms    [User: 49.0 ms, System: 35.0 ms]\n  Range (min \u{2026} max):    53.1 ms \u{2026}  61.0 ms    55 runs In fine : a x33 speed-up from the original time. CppCon 2014: Mike Acton &quot;Data-Oriented Design and C++&quot; \u{21a9} CppCon 2014: Mike Acton &quot;Data-Oriented Design and C++&quot; \u{21a9} CppCon 2014: Mike Acton &quot;Data-Oriented Design and C++&quot; \u{21a9} ",
titles:[
{
title:"The investigation",
slug:"the-investigation",
offset:549,
},
{
title:"When there\'s one, there\'s many",
slug:"when-there-s-one-there-s-many",
offset:5032,
},
{
title:"The new approach",
slug:"the-new-approach",
offset:6527,
},
{
title:"The new implementation",
slug:"the-new-implementation",
offset:9788,
},
{
title:"Fossil",
slug:"fossil",
offset:17169,
},
{
title:"Conclusion",
slug:"conclusion",
offset:20756,
},
{
title:"Addendum",
slug:"addendum",
offset:23025,
},
{
title:"Addendum 2",
slug:"addendum-2",
offset:23577,
},
],
},
{
html_file_name:"tip_of_the_day_6.html",
title:"Tip of the day #6: Use Bpftrace to estimate how much memory an in-memory cache will use",
text:"Context I have a Go service that has an in-memory LRU (Least Recently Used) cache to speed up some things.\nHere I am, writing documentation for this service, and it happens that you can specify in its configuration the maximum number of cache entries.\nThat\'s useful to limit the overall memory usage. Obviously this value is directly related to the Kubernetes memory limit for this deployment. But then I am wondering: what value should the docs recommend for this configuration field? A 1000 entries, 10 000? One factor is how many distinct entries do we expect, but another is: How big is a cache entry ? An entry in the cache in this case is a slice of bytes (a blob) so it\'s not statically possible to determine, just looking at the code, how much memory it will consume. This distribution of entry sizes is however easy to uncover: all entries in the cache are inserted by one callback. It happens to be a Go function that is passed to a C library (via CGO) but this trick works with any language. This function takes as argument a slice of bytes to be inserted in the cache. So, add a log in this callback, print the slice length, process all the relevant logs, compute some statistics, and done? Or, add a custom Prometheus metric, deploy, done? Well... why modify the source code when we don\'t have too? Let\'s use bpftrace to determine the distribution of entry sizes at runtime on the unmodified program! In the past I have used dtrace on macOS/FreeBSD which is similar and the direct inspiration for bpftrace . I find dtrace more powerful in some regards - although bpftrace has support for loops whereas dtrace does not. Point being, the bpftrace incantation can be adapted for dtrace pretty easily. Both of these tools are essential workhorses of exploratory programming and troubleshooting. Bpftrace So, the plan is: I run the tests under bpftrace , collect a histogram of the slice of bytes to be inserted in the cache, and voila! We can also run the real service with a load test to generate traffic, or simply wait for real traffic to come - all of that works, and dtrace / bpftrace are designed to inspect production programs without the risk of crashing them, or adversely impacting the system. The bpftrace incantation will be the same in all of these cases, only the binary (or process id) will change. Here, my function to insert a slice of bytes in the cache is called cache_insert , the executable is called itest.test , and the length of the slice of bytes happens to be passed as the third function argument. Arguments are zero-indexed so that means arg2 : $ sudo bpftrace -e \'uprobe:./itest.test:cache_insert {@bytes=lhist(arg2, 0 , 16384, 128)}\' -c \'./itest.test -test.count=1\' lhist creates a linear histogram with the minimum value here being 0 , the maximum value 16384 and the bucket size 128 . I used the hist function initially which uses a power-of-two bucket size but my values were all in one big bucket so that was a bit imprecise. Still a good first approximation. But we can get a better estimate by using a small bucket size with lhist . bpftrace prints the histogram by default at the end: @bytes: \n[512, 640)            96 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@| So all slices of bytes have their length between 512 and 640 in this case, all in one bucket. Alternatively, we can point bpftrace at the Go function instead of the C function: func (c Cache) Insert(ctx context.Context, key [32]byte, value []byte, expiryDate time.Time) error { [...] } We are interested in len(value) which happens to be accessible in arg5 : $ sudo bpftrace -e \'uprobe:./itest.test:/path/to/my/pkg/Cache.Insert {@bytes=lhist(arg5, 0 , 16384, 128)}\' -c \'./itest.test -test.count=1\' and we get the same output. Note that we are doing very basic runtime inspection in this case, but we could also for example look at the hit rate of cache lookups, how much time inserting a new entry takes, etc. bpftrace and dtrace are really designed to be lightweight swiss-army knives. Addendum: Function arguments in bpftrace bpftrace reads neither debug information nor C headers by default so all function arguments are register sized, i.e. 64 bits on x86_64. bpftrace does not even know how many arguments the function accepts! My function signature is (simplified): struct ByteSliceView {\n    uint8_t* data;\n    size_t len;\n}\n\nvoid cache_insert(const uint8_t *key, struct ByteSliceView value, [...]); The value of interest is value.len . So initially I tried to access it in bpftrace using arg1.len , however it did not work. Here is an excerpt from the documentation: Function arguments are available through the argN for register args. Arguments passed on stack are available using the stack pointer, e.g. $stack_arg0 = (int64)reg(&quot;sp&quot;) + 16. Whether arguments passed on stack or in a register depends on the architecture and the number or arguments used, e.g. on x86_64 the first 6 non-floating point arguments are passed in registers and all following arguments are passed on the stack. Note that floating point arguments are typically passed in special registers which don\u{2019}t count as argN arguments which can cause confusion So, it\'s a mess ... I fired up gdb and printed registers directly when the cache_insert function is entered. I discovered by doing info registers that (on my machine, with this compiler and build flags, yada yada yada), the rdx register contains value.len . I.e. the compiler unpacks value which is a struct of two fields, into arg1 (i.e. the rsi register) and arg2 (i.e. the rdx register). Thus, this call: cache_insert(foo, bar) gets transformed by the compiler into cache_insert(foo, bar.data, bar.len) , and the third function argument (aka arg2 ) is our length. ",
titles:[
{
title:"Context",
slug:"context",
offset:0,
},
{
title:"Bpftrace",
slug:"bpftrace",
offset:1804,
},
{
title:"Addendum: Function arguments in bpftrace",
slug:"addendum-function-arguments-in-bpftrace",
offset:4007,
},
],
},
{
html_file_name:"build-pie-executables-with-pie.html",
title:"Build PIE executables in Go: I got nerd-sniped",
text:"Context Lately I have been hardening the build at work of our Go services and one (seemingly) low-hanging fruit was PIE . This is code built to be relocatable, meaning it can be loaded by the operating system at any memory address. That complicates the work of an attacker because they typically want to manipulate the return address of the current function (e.g. by overwriting the stack due to a buffer overflow) to jump to a specific function e.g. system() from libc to pop a shell. That\'s easy if system is always at the same address. If the target function is loaded at a different address each time, it makes it more difficult for the attacker to find it. This approach was already used in the sixties (!) and has been the default for years now in most OSes and toolchains when building system executables. There is practically no downside. Some people report a single digit percent slowdown in rare cases although it\'s not the rule. Amusingly this randomness can be used in interesting ways for example seeding a random number generator with the address of a function. Go supports PIE as well, however this is not the default so we have to opt in. PIE is especially desirable when using CGO to call C functions from Go which is my case at work.\nBut also Go is not entirely memory safe so I\'d argue having PIE enabled in all cases is preferable. So let\'s look into enabling it. And this is also a good excuse to learn more about how surprisingly complex it is for an OS to just execute a program. How hard can it be? go help buildmode states: -buildmode=pie\nBuild the listed main packages and everything they import into\nposition independent executables (PIE). Packages not named\nmain are ignored. Easy enough, right? Let\'s build a hello world program ( not using CGO) with default options: $ go build main.go\n$ file ./main\n./main: ELF 64-bit LSB executable [..] statically linked [..] Now, let\'s add the -buildmode=pie option: $ go build -buildmode=pie ./main.go\n$ file ./main\n./main: ELF 64-bit LSB pie executable [..] dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2 [..] Ok, it worked, but also: why did we go from a statically linked to a dynamically linked executable? PIE should be orthogonal to static/dynamic linking! Or is it? We\'ll come back to that in a second. When we run our freshly built Go executable in a bare-bone Docker image (distroless), we get a nice cryptic error at runtime: exec /home/nonroot/my-service: no such file or directory Oh oh. Let\'s investigate. A helpful mental model I\'d argue that the wording of the tools and the online content is confusing because it conflates two different things. For example, invoking lld with the above executable prints statically linked . But file prints dynamically linked for the exact same file! So which is it? A helpful mental model is to split linking from loading and have thus two orthogonal dimensions: static linking, static loading static linking, dynamic loading dynamic linking, static loading dynamic linking, dynamic loading The first dimension (static vs dynamic linking) is, from the point of view of the OS trying to launch our program, decided by the field \'Type\' in the ELF header (bytes 16-20): if it\'s EXEC , it\'s a statically linked executable. If it\'s DYN , it\'s a shared object file or a statically linked PIE executable (note that the same file can be both a shared library and an executable. Pretty cool, no?). The second dimension (static vs dynamic loading) is decided by the ELF program headers: if there is one program header of type INTERP (which specifies the loader to use), our executable is using dynamic loading meaning it requires a loader (a.k.a. interpreter) at runtime. Otherwise it does not. This aspect can be observed with readelf : $ readelf --program-headers ./main\n[..]\nElf file type is DYN (Position-Independent Executable file)\n[..]\nProgram Headers:\n  Type           Offset             VirtAddr           PhysAddr\n                 FileSiz            MemSiz              Flags  Align\n  [..]\n  INTERP         0x0000000000000fe4 0x0000000000400fe4 0x0000000000400fe4\n                 0x000000000000001c 0x000000000000001c  R      0x1\n      [Requesting program interpreter: /lib64/ld-linux-x86-64.so.2]\n  [..] Our newly built Go executable is in the second category: static linking, dynamic loading. And that\'s an issue because we deploy it in a distroless Docker container where there is not even a libc available or the loader ld.so . It means we now need to change our Docker image to include a loader at runtime. So not ideal. What can we do? We\'d like to be in the first category: static linking, static loading. I suppose that folks that ship executables to customer workstations would also have interest in doing that to remove one moving piece (the loader on each target machine). Also possibly people who want to obfuscate what their program does at startup and do not want anyone monkeying around with environment variables that impact the loader such as LD_PRELOAD (so, perhaps malware or anti-malware programs?). Addendum: I have found at least one CVE in the glibc loader, and it seems it\'s a frequent occurrence, so in my opinion that\'s reason enough to remove one moving piece from the equation and prefer static loading! Troubleshooting the problem It turns out that PIE was historically designed for executables using dynamic loading.\nThe loader loads at startup the sections of the executable at different places in memory, fills (eagerly or lazily) in a global table (the GOT ) the locations of symbols. And voila, functions are placed randomly in memory and function calls go through the GOT which is a level of indirection to know at runtime where the function they want to call is located. Blue team, rejoice! Red team, sad. So how does it work with a statically linked executable where a loader is not even present on the system? Here\'s a bare-bone C program that uses PIE and is statically linked: #include &lt;stdio.h&gt;\n\nint main() { printf(&quot;%p %p hello!\\n&quot;, &amp;main, &amp;printf); } We compile it, create an empty chroot with only our executable in it, and run it multiple times, to observe that the functions main and printf are indeed loaded in different places of memory each time: $ musl-gcc pie.c -fPIE -static-pie\n$ file ./a.out\n./a.out: ELF 64-bit LSB pie executable [..] static-pie linked [..]\n$ mkdir /tmp/scratch\n$ sudo chroot /tmp/scratch ./a.out\n0x7fcf33688419 0x7fcf336887e0 hello!\n$ sudo chroot /tmp/scratch ./a.out\n0x7f2b44f20419 0x7f2b44f207e0 hello!\n$ sudo chroot /tmp/scratch ./a.out\n0x7f891d95e419 0x7f891d95e7e0 hello! So... how does it work when no loader is present in the environment? Well, what is the only thing that we link in our bare-bone program? Libc! And what does libc contain? You guessed it, a loader! For musl, it\'s the file ldso/dlstart.c and that\'s the code that runs before our main . Effectively libc doubles as a loader. And when statically linked, the loader gets embedded in our application and runs at startup before our code. That means that we can have our cake and eat it too: static linking and PIE! No loader required in the environment. So, how can we coerce Go to do the same? The solution The only way I have found is to ask Go to link with an external linker and pass it the flag -static-pie . Due to the explanation above that means that CGO gets enabled automatically and we need to link a libc statically: $ CGO_ENABLED=0 go build -buildmode=pie -ldflags \'-linkmode external -extldflags &quot;-static-pie&quot;\' main.go\n-linkmode requires external (cgo) linking, but cgo is not enabled We use musl-gcc again for simplicity but you can also use the Zig build system to automatically build musl from source, or provide your own build of musl, etc: $ CC=musl-gcc go build -ldflags \'-linkmode external -extldflags &quot;--static-pie&quot;\' -buildmode=pie main.go\n$ file ./main\n./main: ELF 64-bit LSB pie executable [..] static-pie linked Yeah! We can check that it works in our empty chroot again. Here\'s is our Go program: package main\n\nimport (\n\t&quot;fmt&quot;\n)\n\nfunc main() {\n\tfmt.Println(main, fmt.Println, &quot;hello&quot;)\n} And here\'s how we build and run it: $ CC=musl-gcc go build -ldflags \'-linkmode external -extldflags &quot;--static-pie&quot;\' -buildmode=pie main.go\n$ cp ./main /tmp/scratch/\n$ sudo chroot /tmp/scratch ./main\n0x7f0701b17220 0x7f0701b122a0 hello\n$ sudo chroot /tmp/scratch ./main\n0x7f0f27f3b220 0x7f0f27f362a0 hello\n$ sudo chroot /tmp/scratch ./main\n0x7f61f8fd7220 0x7f61f8fd22a0 hello Compare that with the non-PIE default build where the function addresses are fixed: $ go build main.go\n$ cp ./main /tmp/scratch/\n$ sudo chroot /tmp/scratch ./main\n0x48f0e0 0x48a160 hello\n$ sudo chroot /tmp/scratch ./main\n0x48f0e0 0x48a160 hello\n$ sudo chroot /tmp/scratch ./main\n0x48f0e0 0x48a160 hello Conclusion \'Static PIE\', or statically linked PIE executables, are a relatively new development: OpenBSD added that in 2015 and builds all system executables in that mode, Clang only added the flag in 2019, etc. Apparently the Go linker does not support this yet, I suppose because it does not ship with a loader and so has to rely on the libc loader (if someone knows for sure, I\'d be curious to know!). After all, the preferred and default way for Go on Linux is \'static linking, static loading\'. Still I think it\'s great to do since we get the best of both worlds, only requiring a little bit of finagling with linker flags. Also it would be nice that the Go documentation talks at least a little about this topic. In the meantime, there is this article, which I hope does not contain inaccuracies and helps a bit. A further hardening on top of PIE, that I have not yet explored yet, but is on my to do list, is read-only relocations which makes the Global Offset Table read-only to prevent an attacker from overwriting the relocation entries there. On Fedora for example, all system executables are built with this mitigation on. ",
titles:[
{
title:"Context",
slug:"context",
offset:0,
},
{
title:"How hard can it be?",
slug:"how-hard-can-it-be",
offset:1503,
},
{
title:"A helpful mental model",
slug:"a-helpful-mental-model",
offset:2500,
},
{
title:"Troubleshooting the problem",
slug:"troubleshooting-the-problem",
offset:5264,
},
{
title:"The solution",
slug:"the-solution",
offset:7194,
},
{
title:"Conclusion",
slug:"conclusion",
offset:8840,
},
],
},
{
html_file_name:"what_should_your_mutexes_be_named.html",
title:"What should your mutexes be named?",
text:"Discussions: /r/golang , HN . I have started a new job recently and the main challenge, I find, is to ingest and adapt to a big codebase that does things slightly differently than what I am used to. This article explores ways to make this phase go smoother. The other day a Pull Request popped up at work, which made me think for a bit. It looked like this (Go, simplified): type Foo struct {\n    bar int\n    barMux sync.Mutex\n} It\'s a typical approach with concurrent code: a mutex protects the variable it is named after. So using the variable bar looks like this: barMux.Lock()\nbar += 1\nbarMux.Unlock() Yes, in this simplistic case an atomic would likely be used instead of a mutex but that\'s just to illustrate. But I paused for a second: What should the mutex be named? I usually use the xxxMtx convention, so I\'d have named it barMtx . To avoid a sterile \'you vs me\' debate, I thought: What do other people do? What naming convention is in use in the project, if any? I\'ll demonstrate this method with the code of the Go standard library. And more generally, what is the best way to find out what naming conventions or code patterns are used in a project you don\'t know? I need a good tool to find these answers quickly. Structural search I use ripgrep and awk all the time when developing, probably at least once a minute, and these tools can give us the answers... kind of, since they operate on raw text. Complex code constructs or contextual searches e.g. \'A function call whose third argument is a number and the function name starts with get \' may be impossible to find correctly. What I often actually need to do is search the code structurally , meaning search the Abstract Syntax Tree (AST). And the good news is, there are tools nowadays than can do that! I never took the time to learn one, even though this came up a few times, so I felt this is finally the occasion. Enter ast-grep . Surprisingly, the main way to search with it is to write a rule file in YAML. A command line search also exists but seems much more limited. Let\'s thus search for \'structure fields\' whose type is a \'mutex\': id: find-mtx-fields\nmessage: Mutex fields found\nseverity: info\nlanguage: go\nrule:\n  kind: field_declaration\n  all:\n    - has: \n        field: name\n        # Ignore nameless fields.\n        regex: &quot;.+&quot;\n    - has:\n        field: type\n        regex: &quot;Mutex&quot; A potential match must pass all conditions under the all section to be a result. There are other ways to write this rule, but this works. Note that the regexp is loose enough to match different kinds of mutex types such as sync.Mutex and sync.RWMutex . When we run the tool on the Go standard library, we get something like this (excerpt): $ ast-grep scan --rule ~/scratch/mtx.yaml\n[...]\n\nnote[find-mtx-fields]: Mutex fields found\n   \u{250c}\u{2500} net/textproto/pipeline.go:29:2\n   \u{2502}\n29 \u{2502}     mu       sync.Mutex\n   \u{2502}     ^^^^^^^^^^^^^^^^^^^\n\n\nnote[find-mtx-fields]: Mutex fields found\n    \u{250c}\u{2500} net/rpc/server.go:191:2\n    \u{2502}\n191 \u{2502}     reqLock    sync.Mutex // protects freeReq\n    \u{2502}     ^^^^^^^^^^^^^^^^^^^^^\n\nnote[find-mtx-fields]: Mutex fields found\n   \u{250c}\u{2500} net/rpc/jsonrpc/server.go:31:2\n   \u{2502}\n31 \u{2502}     mutex   sync.Mutex // protects seq, pending\n   \u{2502}     ^^^^^^^^^^^^^^^^^^\n\n[...] Very useful. The tool can do much more, but that\'s enough for us to discover that there isn\'t one naming convention in use. Also, the mutex is not always named after the variable it protects (e.g.: mutex protects seq ). So, is there at least a convention used in the majority of cases? How can we aggregate the results? A naming convention to rule them all Unfortunately I did not find a built-in way to post-process the results from ast-grep , so I resorted to outputting the matches as JSON, extracting the matched text with jq , and finally aggregating the results with good old AWK: $ ast-grep scan --rule ~/scratch/mtx.yaml --json | jq \'.[].text\' -r | awk -f ~/scratch/ast-grep-post.awk  And this is the ad-hoc AWK script: # ~/scratch/ast-grep-post.awk\n\n# `$1` is the variable name.\n{\n  if ($1 ~ /(m|M)u$/) { \n    stats[&quot;mu&quot;] += 1\n  }\n  else if ($1 ~ /(m|M)ux$/) { \n    stats[&quot;mux&quot;] += 1\n  }\n  else if ($1 ~ /(m|M)tx$/) { \n    stats[&quot;mtx&quot;] += 1\n  }\n  else if ($1 ~ /(m|M)utex$/) { \n    stats[&quot;mutex&quot;] += 1\n  }\n  else if ($1 ~ /(l|L)ock$/) { \n    stats[&quot;lock&quot;] += 1\n  } else {\n    stats[&quot;other&quot;] += 1\n  }\n}\n\nEND {\n  for (k in stats) {\n    print k, stats[k]\n  }\n} And here are the statistics (commit 7800f4f , 2025-06-08): Variable name suffix Count mu 131 mutex 11 something else 11 lock 6 mux 0 So according to these statistics: if you want to follow the same naming convention as the Go project, use xxxMu as a name for your mutexes. More importantly, I would add, and this is subjective: name the mutex after the variable it protects for clarity, e.g.: bar and barMu . In nearly every case in the Go project where this rule of thumb was not followed, a code comment was present to explain which variable the mutex protects. We might as well have this information in the mutex variable name. Even for cases where the mutex protects multiple variables, the Go developers often picked one of the variables and named the mutex after it: type Link struct {\n\thashmu             sync.Mutex       // protects hash, funchash\n\thash               map[string]*LSym \n\tfunchash           map[string]*LSym\n\n        [...]\n} Low-tech alternatives A quick and dirty way to achieve the same with a regexp is: $ rg -t go \'^\\s+\\w+\\s+sync\\.Mutex$\' This works since Go is a language with only one way to define a struct field, but some languages would be more difficult. A slightly smarter way, to only find field declarations, would be to use AWK to remember whether or not we are inside a struct definition: /\\s+struct\\s+/ { in_struct = 1 }\n\nin_struct &amp;&amp; /\\s+\\w+\\s+sync\\.Mutex/ { print }\n\nin_struct &amp;&amp; /^}$/ { in_struct = 0 } But this might not work, or at least need to be adapted, to cover complex constructs such as defining a struct within a struct : type Foo struct {\n\tbar struct {\n\t\tx int\n\t\ty int\n\t}\n\tbarMtx sync.Mutex\n} These approaches are not bullet-proof, but they will find most relevant code locations, which is enough. Limitations of structural search tools Most if not all structural search tools only work on a valid AST, and not every language is supported by every tool. The rule syntax is arcane and in parts language specific (see the addendum for details). Speed can be an issue: ast-grep is relatively fast but still slower than ripgrep and it states that it is one of the fastest in its category. On my (admittedly very old) laptop: ast-grep takes  ~10s to scan ~2 millions LOC. Which is pretty good! find + awk takes ~1.5s. ripgrep takes ~100ms (I\'m impressed). Conclusion I think having one structural search program in your toolbox is a good idea, especially if you intend to use it as a linter and mass refactoring tool. If all you want to do is a one-time search, text search programs such as ripgrep and awk should probably be your first stab at it. Also, I think I would prefer using a SQL-like syntax to define rules, over writing YAML with pseudo-code constructs. Addendum: What were mutexes named in the C implementation of the Go compiler and runtime? I wondered if the way mutexes are named in the Go project actually comes from the time were the Go compiler and much of the runtime were implemented in C.\nWe can easily check this out with the same approach. This illustrates that ast-grep works for different languages, and also the slight differences. 1.4 was the last major version to use the C compiler and runtime apparently, so we checkout this commit: $ git checkout go1.4beta1 I initially simply changed the language: go field in the rule to language: c but was surprised nothing turned up. After toying with the treesitter playground ( treesitter is used under the covers), I realized that for C, the AST is structured differently and the nodes have different names. Here is the rule working for struct fields: id: find-mtx-fields-struct-fields\nmessage: Mutex fields found\nseverity: info\nlanguage: c\nrule:\n  kind: field_declaration\n  all:\n    - has: \n        field: declarator\n        regex: &quot;.+&quot;\n    - has:\n        field: type\n        regex: &quot;(M|m)utex&quot; Here are the results of this rule (excerpt): note[find-mtx-fields]: Mutex fields found\n    \u{250c}\u{2500} runtime/malloc.h:430:2\n    \u{2502}\n430 \u{2502}     Mutex   specialLock;    // guards specials list\n    \u{2502}     ^^^^^^^^^^^^^^^^^^^^\n\nnote[find-mtx-fields]: Mutex fields found\n    \u{250c}\u{2500} runtime/malloc.h:451:2\n    \u{2502}\n451 \u{2502}     Mutex  lock;\n    \u{2502}     ^^^^^^^^^^^^ Right after, I realized that some mutexes are defined as global variables, so here is an additional rule file for that: id: find-mtx-fields-vars\nmessage: Mutex variables found\nseverity: info\nlanguage: c\nrule:\n  kind: declaration\n  all:\n    - has: \n        field: declarator\n        regex: &quot;.+&quot;\n    - has:\n        field: type\n        regex: &quot;(M|m)utex&quot; And here are the results (excerpt): note[find-mtx-fields-vars]: Mutex variables found\n   \u{250c}\u{2500} runtime/panic.c:18:1\n   \u{2502}\n18 \u{2502} static Mutex paniclk;\n   \u{2502} ^^^^^^^^^^^^^^^^^^^^^\n\nnote[find-mtx-fields-vars]: Mutex variables found\n    \u{250c}\u{2500} runtime/panic.c:162:3\n    \u{2502}\n162 \u{2502}         static Mutex deadlock;\n    \u{2502}         ^^^^^^^^^^^^^^^^^^^^^^\n\nnote[find-mtx-fields-vars]: Mutex variables found\n   \u{250c}\u{2500} runtime/mem_plan9.c:15:1\n   \u{2502}\n15 \u{2502} static Mutex memlock;\n   \u{2502} ^^^^^^^^^^^^^^^^^^^^^\n\nnote[find-mtx-fields-vars]: Mutex variables found\n    \u{250c}\u{2500} runtime/os_windows.c:586:2\n    \u{2502}\n586 \u{2502}     static Mutex lock; Funnily, it seems that: 1) the C code has much less variability in naming than the Go code: it\'s mostly xxxlock , and 2) The naming in the Go code does not stem from the C code since it\'s completely different. ",
titles:[
{
title:"Structural search",
slug:"structural-search",
offset:1227,
},
{
title:"A naming convention to rule them all",
slug:"a-naming-convention-to-rule-them-all",
offset:3604,
},
{
title:"Low-tech alternatives",
slug:"low-tech-alternatives",
offset:5459,
},
{
title:"Limitations of structural search tools",
slug:"limitations-of-structural-search-tools",
offset:6278,
},
{
title:"Conclusion",
slug:"conclusion",
offset:6831,
},
{
title:"Addendum: What were mutexes named in the C implementation of the Go compiler and runtime?",
slug:"addendum-what-were-mutexes-named-in-the-c-implementation-of-the-go-compiler-and-runtime",
offset:7241,
},
],
},
{
html_file_name:"a_subtle_data_race_in_go.html",
title:"A subtle data race in Go",
text:"Discussions: /r/golang . At work, a good colleague of mine opened a PR titled: \'fix data race\'. Ok, I thought, let\'s see. They probably forgot a mutex or an atomic. Or perhaps they returned a pointer to an object when they intended to return a copy of the object. Easy to miss. Then I was completely puzzled. The diff for the fix was very small, just a few lines, and contained neither mutexes, nor pointers, nor goroutines, nor any concurrency construct for that matter. How could it be? The original code I have managed to reproduce the race in a self-contained Go program resembling the real production code, and the diff for the fix is basically the same as the real one. It\'s a web server with a middleware to do rate limiting. The actual rate limiting code is omitted because it does not matter: package main\n\nimport (\n\t&quot;fmt&quot;\n\t&quot;net/http&quot;\n\t&quot;strings&quot;\n)\n\nfunc NewMiddleware(handler http.Handler, rateLimitEnabled bool) http.Handler {\n\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tif strings.HasPrefix(r.URL.Path, &quot;/admin&quot;) {\n\t\t\trateLimitEnabled = false\n\t\t}\n\n\t\tif rateLimitEnabled {\n\t\t\tfmt.Printf(&quot;path=%s rate_limit_enabled=yes\\n&quot;, r.URL.Path)\n\t\t\t// Rate limiting logic here ...\n\t\t} else {\n\t\t\tfmt.Printf(&quot;path=%s rate_limit_enabled=no\\n&quot;, r.URL.Path)\n\t\t}\n\n\t\thandler.ServeHTTP(w, r)\n\t})\n}\n\nfunc handle(w http.ResponseWriter, r *http.Request) {\n\tw.Write([]byte(&quot;hello!\\n&quot;))\n}\n\nfunc main() {\n\thandler := http.HandlerFunc(handle)\n\tmiddleware := NewMiddleware(handler, true)\n\thttp.Handle(&quot;/&quot;, middleware)\n\n\thttp.ListenAndServe(&quot;:3001&quot;, nil)\n} It\'s very typical Go code I would say. The only interesting thing going on here, is that we never do rate-limiting for the admin section of the site. That\'s handy if a user is abusing the site, and the admin has to go disable their account as fast as possible in the admin section. The intent behind the rateLimitEnabled parameter was likely to have it \'off\' in development mode, and \'on\' in production, based on some environment variable read in main (also omitted here). Can you spot the data race? Feel free to pause for a second. I glanced at code very similar to this for like, 10 minutes, to even begin to form hypotheses about a data race, while knowing from the get go there is a data race, and having the fix in front of me. Symptoms of the bug Let\'s observe the behavior with a few HTTP requests: $ curl http://localhost:3001/\n$ curl http://localhost:3001/admin\n$ curl http://localhost:3001/\n$ curl http://localhost:3001/ We see these server logs: path=/ rate_limit_enabled=yes\npath=/admin rate_limit_enabled=no\npath=/ rate_limit_enabled=no\npath=/ rate_limit_enabled=no The actual output could vary from machine to machine due to the data race. This is what I have observed on my machine. Reading the Go memory model, another legal behavior in this case could be to immediately abort the program. No symptoms at all is not possible. The only question is when the race will manifest. When receiving lots of HTTP requests, it might not happen right after the first request. See the \'Conclusion and recommendations\' section for more information. The third and fourth log are definitely wrong. We would have expected: path=/ rate_limit_enabled=yes\npath=/admin rate_limit_enabled=no\npath=/ rate_limit_enabled=yes\npath=/ rate_limit_enabled=yes The non-admin section of the site should be rate limited, always. But it\'s apparently not, starting from the second request. Trying to access the admin section disables rate limiting for everyone, until the next server restart! So this data race just became a security vulnerability as well! In the real code at work it was actually not a security issue, since the parameter in question governed something related to metrics about rate limiting, not rate limiting itself, so the consequence was only that some metrics were wrong. Which is how the bug was initially spotted. The fix The diff for the fix looks like this: diff --git a/http-race.go b/http-race.go\nindex deff273..6c73b7e 100644\n--- a/http-race.go\n+++ b/http-race.go\n@@ -6,8 +6,10 @@ import (\n \t&quot;strings&quot;\n )\n \n-func NewMiddleware(handler http.Handler, rateLimitEnabled bool) http.Handler {\n+func NewMiddleware(handler http.Handler) http.Handler {\n \treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n+\t\trateLimitEnabled := true\n+\n \t\tif strings.HasPrefix(r.URL.Path, &quot;/admin&quot;) {\n \t\t\trateLimitEnabled = false\n \t\t}\n@@ -29,7 +31,7 @@ func handle(w http.ResponseWriter, r *http.Request) {\n \n func main() {\n \thandler := http.HandlerFunc(handle)\n-\tmiddleware := NewMiddleware(handler, true)\n+\tmiddleware := NewMiddleware(handler)\n \thttp.Handle(&quot;/&quot;, middleware)\n \n \thttp.ListenAndServe(&quot;:3001&quot;, nil) Just transforming a function argument to a local variable. No other change. How can it be? We can confirm that the behavior is now correct: $ curl http://localhost:3001/\n$ curl http://localhost:3001/admin\n$ curl http://localhost:3001/\n$ curl http://localhost:3001/ Server logs: path=/ rate_limit_enabled=yes\npath=/admin rate_limit_enabled=no\npath=/ rate_limit_enabled=yes\npath=/ rate_limit_enabled=yes As expected. Explanation Go, like many languages, has closures: functions that implicitly capture their environment as needed so that surrounding variables can be accessed within the function scope. The NewMiddleware function returns a closure (the middleware) that implicitly captures the rateLimitEnabled variable. The intent in the original code was to treat the function argument, which is passed by value (as most programming languages do), as essentially an automatic local variable. When mutated, nothing is visible outside of the scope of the function, as if we just used a local variable. But...it\'s neither a plain local variable nor a plain function argument: it\'s a variable existing outside of the closure, captured by it. So when the closure mutates it, this mutation is visible to the outside. We can confirm our hypothesis by toggling a knob to ask the compiler to log this: $ go build -gcflags=\'-d closure=1\' http-race.go\n./http-race.go:9:26: heap closure, captured vars = [rateLimitEnabled handler]\n[...] We can plainly see that rateLimitEnabled is being captured. The Go compiler conceptually transforms the closure into code like this: type ClosureEnv struct {\n\trateLimitEnabled *bool\n\thandler          *http.Handler\n}\n\nfunc rateLimitMiddleware(w http.ResponseWriter, r *http.Request, env *ClosureEnv) {\n\tif strings.HasPrefix(r.URL.Path, &quot;/admin&quot;) {\n\t\tenv.rateLimitEnabled = false\n\t}\n\n\tif env.rateLimitEnabled {\n\t\tfmt.Printf(&quot;path=%s rate_limit_enabled=yes\\n&quot;, r.URL.Path)\n\t\t// Rate limiting logic here ...\n\t} else {\n\t\tfmt.Printf(&quot;path=%s rate_limit_enabled=no\\n&quot;, r.URL.Path)\n\t}\n\n\tenv.handler.ServeHTTP(w, r)\n} Note that ClosureEnv holds a pointer to rateLimitEnabled . If it was not a pointer, the closure could not modify the outer values. That\'s why closures capturing their environment lead in the general case to heap allocations so that environment variables live long enough. Ok, it\'s a logic bug. Not yet a data race, right? (We could debate whether all data races are also logic bugs. But let\'s move on). Well, when is this closure called? When handling every single incoming HTTP request, concurrently. It\'s as if we spawned many goroutines which all called this function concurrently, and said function reads and writes an outer variable without any synchronization. So it is indeed a data race. We can confirm with the compiler that no variable is captured by accident now that the patch is applied: $ go build -gcflags=\'-d closure=1\' http-race.go\n./http-race.go:9:26: heap closure, captured vars = [handler] Conclusion and recommendations This was quite a subtle data race which took me time to spot and understand. The go race detector did not notice it, even when throwing lots of concurrent requests at it. LLMs, when asked to analyze the code, did not spot it. The good news is: The Go memory model gives us some guarantees for data races. Contrary to C or C++, where a data race means the wild west of undefined behavior, it only means in Go that we may read/write the wrong value, when reading/writing machine word sizes or smaller (which is the case of booleans). We even find quite a strong guarantee in the Implementation Restrictions for Programs Containing Data Races section: each read must observe a value written by a preceding or concurrent write. However, the official documentation also warns us that reading more than one machine word at once may have dire consequences: This means that races on multiword data structures can lead to inconsistent values not corresponding to a single write. When the values depend on the consistency of internal (pointer, length) or (pointer, type) pairs, as can be the case for interface values, maps, slices, and strings in most Go implementations, such races can in turn lead to arbitrary memory corruption. So how can we avoid this kind of data race from occurring? How can we adapt our code style, if tools do not spot it? Well, you may have been bitten in the past by logic bugs using Go closures, when the wrong variable is captured by accident. The recommendation in these cases is typically: do not capture, pass variables you need inside the closure as function arguments to the closure (i.e., pass explicitly by value instead of implicitly by reference). That\'s probably a good idea but also: it\'s so easy to forget to do it. The root cause is that the environment is captured implicitly. When I was writing C++ I actually liked the lambda syntax because it started with a capture list. Every capture was explicit! It was slightly verbose but as we have seen: it serves to avoid real production bugs! For example: int a = 1, b = 2, c = 3;\nauto fn =  [a, b, c]() { return a + b + c; };\nint res = fn(); After writing quite a lot of code in C, Zig and Odin, which all do not have support for closures, I actually do not miss them. I even think it might have been a mistake to have them in most languages. Every single time I have to deal with code with closures, it is always harder to understand and debug than code without them. It can even lead to performance issues due to hidden memory allocations, and makes the compiler quite a bit more complex. The code that gives me the most headaches is functions returning functions returning functions... And some of these functions in the chain capture their environment implicitly... Urgh. So here\'s my personal, very subjective recommendation when writing code in any language including in Go: Avoid closures if possible. Write standard functions instead. This will avoid accidental captures and make all arguments explicit. Avoid writing callback-heavy code a la JavaScript. Closures usually show up most often in this type of code. It makes debugging and reading hard. Prefer using Go channels, events like io_uring , or polling functions like poll / epoll / kqueue . In other words, let the caller pull data, instead of pushing data to them (by calling their callback with the new data at some undetermined future point). Prefer, when possible, using OS processes over threads/goroutines, to have memory isolation between tasks and to remove entire categories of bugs. You can always map memory pages that are accessible to two or more processes if you absolutely need shared mutable state. Although message passing (e.g. over pipes or sockets) would be less error-prone. Another advantage with OS processes is that you can set resource limits on them e.g. on memory usage. Yes, some cases will require using threads. I\'m talking about most cases here. Related to the previous point: Reduce global mutable state to the absolute minimum, ideally zero. And here\'s my personal, very subjective recommendation for future programming language designers: Consider not having closures in your language, at all. Plain function (pointers) are still fine. If you really must have closures: Consider forcing the developer to explicitly write which variables are captured (like C++ does). Have a knob in your compiler to easily see what variables are being captured in closures (like Go does). Have good statical analysis to spot common problematic patterns ( golangci-lint finds the bug neither in our reproducer nor in the real production service). Consider showing in the editor captured variables in a different way, for example with a different color, from normal variables Implement a race detector (even if that just means using Thread sanitizer). It\'s not a perfect solution because some races will not be caught, but it\'s better than nothing. Document precisely what is the memory model you offer and what are legal behaviors in the presence of data races. Big props to Go for doing this very well. Consider making function arguments not re-assignable. Force the developer to define a local variable instead. Addendum: A reproducer program for the Go race detector Here is a reproducer program with the same issue but this time the Go race detector finds the data race: package main\n\nimport (\n\t&quot;fmt&quot;\n\t&quot;sync&quot;\n\t&quot;time&quot;\n)\n\nfunc NewMiddleware(rateLimitEnabled bool) func() {\n\treturn func() {\n\t\tif time.Now().Nanosecond()%2 == 0 {\n\t\t\trateLimitEnabled = false\n\t\t}\n\n\t\tif rateLimitEnabled {\n\t\t\tfmt.Printf(&quot;rate_limit_enabled=yes\\n&quot;)\n\t\t\t// Rate limiting logic here ...\n\t\t} else {\n\t\t\tfmt.Printf(&quot;rate_limit_enabled=no\\n&quot;)\n\t\t}\n\t}\n}\n\nfunc main() {\n\tmiddleware := NewMiddleware(true)\n\tcount := 100\n\twg := sync.WaitGroup{}\n\twg.Add(count)\n\n\tfor range count {\n\t\tgo func() {\n\t\t\tmiddleware()\n\t\t\twg.Done()\n\t\t}()\n\t}\n\n\twg.Wait()\n} Here\'s the output: $ go run -race race_reproducer.go\nrate_limit_enabled=no\n==================\nWARNING: DATA RACE\nRead at 0x00c00001216f by goroutine 8:\n  main.main.NewMiddleware.func2()\n      /home/pg/scratch/http-race/reproducer/race_reproducer.go:15 +0x52\n  main.main.func1()\n      /home/pg/scratch/http-race/reproducer/race_reproducer.go:32 +0x33\n\nPrevious write at 0x00c00001216f by goroutine 7:\n  main.main.NewMiddleware.func2()\n      /home/pg/scratch/http-race/reproducer/race_reproducer.go:12 +0x45\n  main.main.func1()\n      /home/pg/scratch/http-race/reproducer/race_reproducer.go:32 +0x33\n\nGoroutine 8 (running) created at:\n  main.main()\n      /home/pg/scratch/http-race/reproducer/race_reproducer.go:31 +0xe4\n\nGoroutine 7 (running) created at:\n  main.main()\n      /home/pg/scratch/http-race/reproducer/race_reproducer.go:31 +0xe4\n================== It\'s not completely clear to me why the Go race detector finds the race in this program but not in the original program since each HTTP request is handled in its own goroutine, both programs should be analogous. Maybe not enough concurrent HTTP traffic? Addendum: The fixed code in full package main\n\nimport (\n\t&quot;fmt&quot;\n\t&quot;net/http&quot;\n\t&quot;strings&quot;\n)\n\nfunc NewMiddleware(handler http.Handler) http.Handler {\n\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\trateLimitEnabled := true\n\n\t\tif strings.HasPrefix(r.URL.Path, &quot;/admin&quot;) {\n\t\t\trateLimitEnabled = false\n\t\t}\n\n\t\tif rateLimitEnabled {\n\t\t\tfmt.Printf(&quot;path=%s rate_limit_enabled=yes\\n&quot;, r.URL.Path)\n\t\t\t// Rate limiting logic here ...\n\t\t} else {\n\t\t\tfmt.Printf(&quot;path=%s rate_limit_enabled=no\\n&quot;, r.URL.Path)\n\t\t}\n\n\t\thandler.ServeHTTP(w, r)\n\t})\n}\n\nfunc handle(w http.ResponseWriter, r *http.Request) {\n\tw.Write([]byte(&quot;hello!\\n&quot;))\n}\n\nfunc main() {\n\thandler := http.HandlerFunc(handle)\n\tmiddleware := NewMiddleware(handler)\n\thttp.Handle(&quot;/&quot;, middleware)\n\n\thttp.ListenAndServe(&quot;:3001&quot;, nil)\n} ",
titles:[
{
title:"The original code",
slug:"the-original-code",
offset:489,
},
{
title:"Symptoms of the bug",
slug:"symptoms-of-the-bug",
offset:2393,
},
{
title:"The fix",
slug:"the-fix",
offset:3981,
},
{
title:"Explanation",
slug:"explanation",
offset:5236,
},
{
title:"Conclusion and recommendations",
slug:"conclusion-and-recommendations",
offset:7795,
},
{
title:"Addendum: A reproducer program for the Go race detector",
slug:"addendum-a-reproducer-program-for-the-go-race-detector",
offset:13004,
},
{
title:"Addendum: The fixed code in full",
slug:"addendum-the-fixed-code-in-full",
offset:14867,
},
],
},
{
html_file_name:"an_optimization_and_debugging_story_go_dtrace.html",
title:"An optimization and debugging story with Go and DTrace",
text:"Today at work, I was hacking on Kratos (a Go application), and I noticed running the tests took a bit too long to my liking. So I profiled the tests, and unknowingly embarked on a fun optimization and debugging adventure. I thought it could be interesting and perhaps educational. I just started this job, not two months ago. I want to show methods that make it feasible to understand and diagnose a big body of software you don\'t know, where you did not write any of it, but you still have to fix it. If you want you can jump to the PR directly. Oh, and if you always dreamt of playing the \'DTrace drinking game\' with your friends, where you have to drink each time \'DTrace\' gets mentioned, oh boy do I have something for you here. Setting the stage The nice thing when you work on an open-source project for work is that it\'s easy to write blog posts about it, and it\'s easy for readers to reproduce it! And when I\'m done, it benefits the community. I like it. So, Kratos uses a database. Each schema change is done with a SQL migration file, like add_some_column.up.sql and its counterpart add_some_column.down.sql . Since each test is independent, each test creates a new database, collects all migrations, and applies them, before doing its thing. Now, there are a number of things we could do to speed things up, like only collect migrations once at startup, or merge them all in one (i.e. a snapshot). But that\'s how things are right now. And there are 10 years worth of SQL migrations piled up in the project. Profiling When I profile the test suite (the profiler collects the raw data behind the scenes using DTrace ), I notice some weird things: Pretty much all of the time (97%) in the test is spent in NewMigrationBox which applies SQL migrations. Strange. Pretty much all of the time (90+%) in NewMigrationBox is spent sorting. Maybe it is fine, but still surprising and worth investigating. Get a precise timing The CPU profile unfortunately does not show how much time is spent exactly in NewMigrationBox . At this point, I also do not know how many SQL files are present. If there are a bazillion SQL migrations, maybe it\'s expected that sorting them indeed takes the most time. Let\'s find out with DTrace how long the function really runs. DTrace in 2 minutes DTrace is a \'dynamic instrumentation tracing framework [...] to concisely answer arbitrary questions about the behavior of the operating system and user programs\'. Let\'s unpack: \'Tracing\' means that it can show which functions run, what is the value of their arguments, etc, at runtime. \'Instrumentation\' means that it inserts code in programs of interest to see into what they are doing. When DTrace is not in use, there is no negative effect on performance. Thus, it can instrument programs that do not even know what DTrace is. Consequently, there is no need to recompile programs in a certain way, or even have debug information present, or even have the source code. \'Dynamic\' means that it can be turned on and off at will, or with conditionals based on whatever you want, like the time of day, after a specific file gets opened on the system, etc. That\'s done by writing a script in a custom language (D).\nThat\'s the core idea behind DTrace: it exists to answer questions we did not know we would need to answer . Ahead of time tracing is great and all, but the reality is that there will be plenty of cases where this is not sufficient. \'Framework\' means that it is general purpose, with the D language, and also that it\'s more of a toolbox than a user-friendly product. You typically do not call a ready-made command to answer a question, you write a small custom script. Although, lots of reusable scripts are available on Github, and I suppose LLMs can help nowadays. \'Answer questions\': DTrace is not something that is on all the time like logs or OpenTelemetry traces (all of these work well together and are not exclusive). It\'s more of a detective tool to investigate, akin to the forensic police on a crime scene. \'Operating system and user programs\': That\'s the big advantage of DTrace, it can equally look into a user-space binary program, a program running in a virtual machine (like a Java or JavaScript program), the virtual machine itself (JVM, V8, etc), and the kernel, all in one script . That\'s really important when you are looking for a bug that could be at any level of the stack, in a complex program that perhaps does things in multiple programming languages: a web browser, an OS, a database, etc. What should also be added is that DTrace comes with the system on macOS (requires disabling System Integrity), which is where I did this whole investigation, FreeBSD, Illumos, Windows, etc. The advantage over a debugger is that DTrace does not stop the program. When inspecting a network heavy program, like a web server or a database, or a data race, or a scheduling bug, in production, this property is very important. Also, it can inspect the whole system, not just one single program: we can observe all writes happening on the system right now, or all files opened, etc. Sometimes this is a crucial property to have. The disadvantage is that it is less granular: it cannot (without much effort and assembly knowledge) inspect local variables, inlined functions, etc. However DTrace also supports static tracing where we define traces in our source code for these cases, assuming we know ahead of time that we need this information. Timing a function Back to the problem at hand. We check if the function NewMigrationBox is visible to DTrace by listing ( -l ) all probes matching the pattern *ory* in the executable code.test.before (i.e. before the fix): $ sudo dtrace -n \'pid$target:code.test.before:*ory*: \' -c ./code.test.before -l | grep NewMigrationBox\n209591   pid47446  code.test.before github.com/ory/x/popx.NewMigrationBox return\n209592   pid47446  code.test.before github.com/ory/x/popx.NewMigrationBox entry\n209593   pid47446  code.test.before github.com/ory/x/popx.NewMigrationBox.(*MigrationBox).findMigrations.func4 return\n209594   pid47446  code.test.before github.com/ory/x/popx.NewMigrationBox.(*MigrationBox).findMigrations.func4 entry\n[...] Ok, the first two are the ones we need. Let\'s time the function duration then with a D script time.d : pid$target::*NewMigrationBox:entry { self-&gt;t=timestamp } \n\npid$target::*NewMigrationBox:return {\n  this-&gt;duration = (timestamp - self-&gt;t)/1000000;\n\n  @histogram[&quot;NewMigrationBox&quot;] = lquantize(this-&gt;duration, 0, 800, 1);\n} Explanation: timestamp is an automatically defined variable that stores the current monotonic time at the nanosecond granularity. When we enter the function, we read the current timestamp and store it in a thread-local variable t (with the self-&gt;t syntax). When we exit the function, we do the same again, compute the difference in terms of milliseconds, store it as a clause-local variable (with this-&gt;duration ), and record it in a linear histogram with a minimum of 0 and a maximum of 800 (in milliseconds). Due to the M:N concurrency model of Go,  in the general case, multiple goroutines run on the same thread concurrently, which means the thread-local variable self-&gt;t gets overriden by multiple goroutines all the time, and we observe as a result some non-sensical durations (negative or very high). The DTrace histogram is a nice way to see outliers and exclude them. The real fix would be to not use thread-local variables but instead goroutine-local variables... Which does not come out of the box with DTrace. Fortunately I later found a way to avoid this pitfall, see the addendum at the end. Since the tests log verbose stuff by default and I do not know how to silence them, I save the output of DTrace in a separate file /tmp/time.txt : dtrace -s time.d -c ./code.test.before -o /tmp/time.txt We see these results and the last line shows the aggregation (I trimmed empty lines for brevity): $ sudo dtrace -s time.d -c ./code.test.before -o /tmp/time.txt\n\n  NewMigrationBox                                   \n           value  ------------- Distribution ------------- count    \n             &lt; 0 |@@@@@@                                   3        \n               0 |                                         0        \n               1 |                                         0        \n               2 |                                         0        \n               3 |                                         0        \n               4 |                                         0        \n               5 |                                         0        \n                 [...]\n             173 |                                         0        \n             174 |                                         0        \n             175 |                                         0        \n             176 |                                         0        \n             177 |@@                                       1        \n             178 |                                         0        \n             179 |                                         0        \n             180 |@@@@@@@@                                 4        \n             181 |@@@@@@@@                                 4        \n             182 |@@@@                                     2        \n             183 |                                         0        \n             184 |                                         0        \n             185 |                                         0        \n             186 |                                         0        \n                 [...]\n          &gt;= 800 |@@@@@@@@@@@@                             6   So the duration is ~180 ms, ignoring impossible values. Establishing a baseline When doing some (light) optimization work, it is crucial to establish a baseline. What is the ideal time? How much do we differ from this time? And to establish this baseline, it is very important to understand what kind of work we are doing: Are we doing any I/O at all or is the workload purely in-memory? How much work is there? How many items are we handling? Is it a handful, a few thousands, a few millions? So let\'s see what the baseline is when simply finding all the SQL files on disk: hyperfine --shell=none --warmup=5 &quot;find ./persistence/sql/migrations -name \'*.sql\' -type f&quot;\nBenchmark 1: find ./persistence/sql/migrations -name \'*.sql\' -type f\n  Time (mean \u{b1} \u{3c3}):     206.6 ms \u{b1}   4.8 ms    [User: 2.4 ms, System: 5.0 ms]\n  Range (min \u{2026} max):   199.5 ms \u{2026} 214.4 ms    14 runs Ok, so 200ms, which is pretty close to our 180ms in Go... Wait a minute... Are we doing any I/O in Go at all? Could it be that we embed all the SQL files in our binary at build time ?! Let\'s print with DTrace the files that are being opened, whose extension is .sql : syscall::open:entry { \n  self-&gt;filename = copyinstr(arg0);\n\n  if (rindex(self-&gt;filename, &quot;.sql&quot;) == strlen(self-&gt;filename)-4) {\n    printf(&quot;%s\\n&quot;, self-&gt;filename)\n  }\n}  copyinstr is required because our D script runs inside the kernel but we are trying to access user-space memory. When we run this script on go test -c which builds the test executable, we see that all the SQL files are being opened by the Go compiler and subsequently embedded in the test binary: $ sudo dtrace -s ~/scratch/sql_files.d -c \'go test -tags=sqlite -c\'\n\nCPU     ID                    FUNCTION:NAME\n 10    178                       open:entry /Users/philippe.gaultier/company-code/x/networkx/migrations/sql/20150100000001000000_networks.cockroach.down.sql\n\n  5    178                       open:entry /Users/philippe.gaultier/company-code/x/networkx/migrations/sql/20150100000001000000_networks.postgres.down.sql\n\n[...] And if we run the same script on the test binary, we see that no SQL files on disk are being opened. Damn, so we are doing purely in-memory work. Alright, so is 180 ms still good in that light? How many files are we dealing with? According to find , ~1.6k. So, you\'re saying we are spending 180 ms to sort a measly 1.6k items in a linear array? That\'s nothing for a modern computer! It should be a small number of milliseconds! What the hell is my program even doing? If you\'re like me, you\'re always asking yourself: what is taking my computer so damn long? Why is Microsoft Teams hanging again? (thankfully I do not have to use it anymore, thank God). Why is Slack taking 15s to simply start up? The problem with computer programs is that there are a black box. You normally have no idea what they are doing. If you\'re lucky they will print some stuff. If you\'re lucky. If only there was a tool that shows me precisely what the hell my program is doing... And I could dynamically choose what to show and what to hide to avoid noise... Oh wait this has been existing for 20 years. DTrace of course! A debugger would also work in that case (command line program running on a developer workstation), but it pretty much requires recompiling with different Go build options which kills iteration times. Contrary to popular belief, Go is not a crazy fast compiler. It\'s a smart compiler that avoids compiling stuff it already compiled in the past. But if a lot of code does need to be recompiled, it\'s not that fast. If you\'re still not convinced to use DTrace yet, let me show you its superpower. It can show you every function call your program does! That\'s sooo useful when you do not know the codebase. Let\'s try it, but we are only interested in calls from within NewMigrationBox , and when we exit NewMigrationBox , we should stop tracing, because each invocation will anyway be the same: pid$target:code.test.before:*NewMigrationBox:entry { self-&gt;t = 1}\n\npid$target:code.test.before:*NewMigrationBox:return { exit(0) }\n\npid$target:code.test.before:sort*: /self-&gt;t != 0/ {} I have written more specific probes in this script by specifying more parts of the probe (the second part of the probe is the module name, here it is the executable name), to try to reduce noise (by accidentally matching probes we do not care about) and also to help with performance (the more probes are being matched, the more the performance tanks). Since we know that the performance issue is located in the sorting part, we only need to trace that. For example if all your company code is under some prefix, like for me, github.com/ory , and you want to see all calls to company code, the probe can be pid$target::github.com?ory*: . The only issue is that the Go stdlib code has no prefix and we want to see it as well... The self-&gt;t variable is used to toggle tracing on when we enter a specific function of interest, and to toggle it off when we leave the function. Very useful to reduce noise and avoid a post-processing filtering step. So, let\'s run our script with the -F option to get a nicely formatted output: $ sudo dtrace -s ~/scratch/trace_calls.d -c \'./code.test.before\' -F\n\nCPU FUNCTION                                 \n  7  -&gt; sort.Sort                             \n  7    -&gt; sort.pdqsort                        \n  7      -&gt; sort.insertionSort                \n  7      &lt;- sort.insertionSort                \n  7    &lt;- sort.Sort                           \n  7    -&gt; sort.Sort                           \n  7      -&gt; sort.pdqsort                      \n  7        -&gt; sort.insertionSort              \n  7        &lt;- sort.insertionSort              \n  7      &lt;- sort.Sort                         \n  7      -&gt; sort.Sort                         \n  7        -&gt; sort.pdqsort                    \n  7          -&gt; sort.choosePivot              \n  7            -&gt; sort.median                 \n  7            &lt;- sort.median                 \n  7            -&gt; sort.median                 \n  7            &lt;- sort.median                 \n  7            -&gt; sort.median                 \n  7            &lt;- sort.median                 \n  7            -&gt; sort.median                 \n  7            &lt;- sort.median                 \n  7          &lt;- sort.choosePivot              \n  7          -&gt; sort.partialInsertionSort     \n  7          &lt;- sort.partialInsertionSort \n[...] The -&gt; arrow means we enter the function, and &lt;- means we exit it. We see a nice call tree. Also I notice something weird (looking at the full output which is huge): we are calling sort.Sort way too much. It should be once, but we call it much more than that. That\'s the canary in the coal mine and it matches what we see on the CPU profile from the beginning. It\'s always DNS - wait no, it\'s always: superlinear algorithmic complexity Time to inspect the code in NewMigrationBox , finally. Pretty quickly I stumble upon code like this (I simplified a bit - you can find the original code in the PR): fs.WalkDir(fm.Dir, &quot;.&quot;, func(p string, info fs.DirEntry, err error) error {\n    migrations = append(migrations, migration)\n    mod := sort.Interface(migrations)\n    sort.Sort(mod)\n\n    return nil\n} For the uninitiated: fs.Walkdir recursively traverses a directory and calls the passed function on each entry. Aaah... We are sorting the slice of files every time we find a new file . That explains it. The sort has O(n * log(n)) complexity and we turned that into O(n\u{b2} * log(n)) . That\'s \'very very super-linear\', as the scientists call it. Furthermore, most sort algorithms have worst-case performance when the input is already sorted, so we are paying full price each time, essentially doing sort(sort(sort(...))) . Let\'s confirm this finding with DTrace by printing how many elements are being sorted in sort.Sort . We rely on the fact that the first thing sort.Sort does, is to call .Len() on its argument: pid$target::*NewMigrationBox:entry { self-&gt;t = 1}\n\npid$target::*NewMigrationBox:return { self-&gt;t = 0}\n\npid$target::sort*Len:return /self-&gt;t != 0/ {printf(&quot;%d\\n&quot;, uregs[0])} The variable uregs is an array of user-space registers and the first one contains, in a :return probe, the return value of the function. So here we are simply printing the length of the slice being sorted. We see: CPU     ID                    FUNCTION:NAME\n  5  52085       sort.(*reverse).Len:return 1\n\n  5  52085       sort.(*reverse).Len:return 2\n\n  5  52085       sort.(*reverse).Len:return 3\n\n  5  52085       sort.(*reverse).Len:return 4\n\n  5  52085       sort.(*reverse).Len:return 5\n\n  [...]\n\n 11  52085       sort.(*reverse).Len:return 1690\n\n 11  52085       sort.(*reverse).Len:return 1691\n\n 11  52085       sort.(*reverse).Len:return 1692\n\n 11  52085       sort.(*reverse).Len:return 1693 So it\'s confirmed. Remember when I said at the beginning: That\'s the core idea behind DTrace: it exists to answer questions we did not know we would need to answer . This is a perfect example: we would never add an OpenTelemetry trace or a log to the .Len() function ahead of time - that would be too costly and almost never useful. But DTrace can dynamically instrument this function when we need it. The fix is easy: collect all files into the slice and then sort them once at the end: fs.WalkDir(fm.Dir, &quot;.&quot;, func(p string, info fs.DirEntry, err error) error {\n    migrations = append(migrations, migrations)\n    return nil\n}\n\nmod := sort.Interface(migrations)\nsort.Sort(mod) The real fix uses slices.SortFunc instead of sort.Sort because the official docs mention the performance of the former is better than the latter. Likely because slices.SortFunc uses compile-time generics whereas sort.Sort uses runtime interfaces. And also we see that the Go compiler inlines the call to slices.SortFunc , which probably helps further. With this done, we can measure again the duration of NewMigrationBox (with a lower maximum value for the histogram since we know it got faster): $ sudo dtrace -s ~/scratch/time.d -c \'./code.test.after\' \n\n  NewMigrationBox                                   \n           value  ------------- Distribution ------------- count    \n             &lt; 0 |@@@@@@@@                                 4        \n               0 |                                         0        \n               1 |                                         0        \n               2 |                                         0        \n               3 |                                         0        \n               4 |                                         0        \n               5 |                                         0        \n               6 |                                         0        \n               7 |                                         0        \n               8 |                                         0        \n               9 |                                         0        \n              10 |                                         0        \n              11 |                                         0        \n              12 |@@@@@@@@@@@@@@@@@@@@                     10       \n              13 |@@@@@@@@                                 4        \n              14 |                                         0        \n              15 |                                         0        \n              16 |                                         0        \n              17 |                                         0        \n                 [...]\n          &gt;= 100 |@@@@                                     2 Conclusion So we went from ~180 ms to ~11ms for the problematic function, a 16x improvement that applies to every single test in the test suite. Not too bad. And we used DTrace at every turn along the way. What I find fascinating is that DTrace is a general purpose tool. Go was never designed with DTrace in mind, and vice-versa. Our code: same thing. And still, it works, no recompilation needed. I can instrument the highest levels of the stack to the kernel with one tool. That\'s pretty cool! Of course DTrace is not perfect. User friendliness is, I think, pretty rough quirky. It\'s an arcane power tool for people who took the time to decipher incomplete manuals. For example registers on aarch64 are not documented, but the ones on SPARC are (because that\'s were DTrace originated from...). Fortunately I found that piece of information after some digging: the file /usr/lib/dtrace/arm64/regs_arm64.d on macOS. My favorite quirk is when an error in the script leads to an error message pointing at the bytecode (like Java and others, the D script gets compiled to bytecode before being sent to the kernel). What am I supposed to do with this :) ? It took me a while as a user to even understand that bytecode was involved at all. But it\'s very often a life-saver. So thank you to their creators. Another learning for me is that super-linear algorithms will go unnoticed and seem fine for the longest time and then bite you hard years later. If the issue is not addressed, each time a new item (here, a new SQL migration) is added, things will slow down until they halt to a crawl. So if you see something, say something. Or better yet, use DTrace to diagnose the problem! Addendum: The sorting function was wrong If you look at the PR you\'ll see that the diff is bigger than what I described. Initially I wanted to simply remove the sorting altogether, because fs.WalkDir already sorts lexically all the files, in order to always process the files in the same order. However this code uses a custom sorting logic so we need to still sort at the end, after walking the directory. So, the diff is bigger because I noticed that the sorting function had a flaw and did not abide by the requirements of the Go standard library. Have a look, I think it is pretty clear. Interestingly, sort.Sort and slices.SortFunc have different requirements! The first one requires the sorting function to be a transitive ordering whereas the second one requires it to be a strict weak ordering . The more you know! I encourage you, if you write a custom sorting function, to carefully read which requirements you have to comply with, and write tests that ensure that these requirements are met, lest you face subtle sorting bugs. Addendum: A goroutine-aware D script At the beginning I mentioned that self-&gt;t = timestamp means we are storing the current timestamp in a thread-local variable. However, since in the general case, multiple goroutines run on the same thread concurrently, this variable gets overriden by multiple goroutines all the time, and we observe as a result some non-sensical durations (negative or very high). I also mentioned that the fix would be to store this variable in a goroutine-aware way instead. Well, the good news is, there is a way! Reading carefully the Go ABI document again, we see that on ARM64 (a.k.a. AARCH64), the register R28 stores the current goroutine. Great! That means that we can treat the value in this register as the \'goroutine id\' and we can store all timestamps per goroutine. My approach is to store all timestamps in a global map where the key is the goroutine id and the value is the timestamp. It\'s conceptually the same as thread-local DTrace variables, from the docs: You can think of a thread-local variable as an associative array that is implicitly indexed by a tuple that describes the thread\'s identity in the system. So here goes: pid$target::*NewMigrationBox:entry { \n  this-&gt;goroutine_id = uregs[R_X28];\n  durations_goroutines[this-&gt;goroutine_id] = timestamp;\n} \n\npid$target::*NewMigrationBox:return {\n  this-&gt;goroutine_id = uregs[R_X28];\n  this-&gt;duration = (timestamp - durations_goroutines[this-&gt;goroutine_id]) / 1000000;\n\n  @histogram[&quot;NewMigrationBox&quot;] = lquantize(this-&gt;duration, 0, 100, 1);\n\n  durations_goroutines[this-&gt;goroutine_id] = 0;\n} At the end, when the duration has been duly recorded in the histogram, we set the value in the map to 0 per the documentation : Assigning an associative array element to zero causes DTrace to deallocate the underlying storage. This behavior is important because the dynamic variable space out of which associative array elements are allocated is finite; if it is exhausted when an allocation is attempted, the allocation will fail and an error message will be generated indicating a dynamic variable drop. Always assign zero to associative array elements that are no longer in use. When we run it, we see that all durations are now nice and correct: $ sudo dtrace -s time.d -c ./code.test.before -o /tmp/time.txt\n\n  NewMigrationBox                                   \n           value  ------------- Distribution ------------- count    \n              11 |                                         0        \n              12 |@@@@@@@@@@@@@@@@@@@@@@                   11       \n              13 |@@@@@@@@@@@@@@                           7        \n              14 |@@@@                                     2        \n              15 |                                         0       This approach is: Safe: reading and writing to global variables, including global maps, is thread-safe by design in DTrace. Correct: because the (current) Go garbage collector is non-moving , and because goroutine pointers are handled specially by the Go runtime, it means in practice that between our two probes (the entry and the exit of the function being traced): The \'goroutine id\', which is a pointer to the current goroutine, cannot change. Another goroutine cannot take the place of the current goroutine pointed to by the goroutine pointer we read. However, you should know that if our function body panics between the entry and return probes, and the panic is not recovered, the return probe will not fire, because of the way Go implements panics: the program simply exits the OS process in the middle of the function. In conclusion, if you are using DTrace with Go, I would encourage you to use this trick. Note that the right register to use differs per architecture: R14 on AMD64, R28 on ARM64, etc. ",
titles:[
{
title:"Setting the stage",
slug:"setting-the-stage",
offset:733,
},
{
title:"Profiling",
slug:"profiling",
offset:1518,
},
{
title:"Get a precise timing",
slug:"get-a-precise-timing",
offset:1905,
},
{
title:"DTrace in 2 minutes",
slug:"dtrace-in-2-minutes",
offset:2257,
},
{
title:"Timing a function",
slug:"timing-a-function",
offset:5443,
},
{
title:"Establishing a baseline",
slug:"establishing-a-baseline",
offset:9740,
},
{
title:"What the hell is my program even doing?",
slug:"what-the-hell-is-my-program-even-doing",
offset:12197,
},
{
title:"It\'s always DNS - wait no, it\'s always: superlinear algorithmic complexity",
slug:"it-s-always-dns-wait-no-it-s-always-superlinear-algorithmic-complexity",
offset:16561,
},
{
title:"Conclusion",
slug:"conclusion",
offset:21383,
},
{
title:"Addendum: The sorting function was wrong",
slug:"addendum-the-sorting-function-was-wrong",
offset:23061,
},
{
title:"Addendum: A goroutine-aware D script",
slug:"addendum-a-goroutine-aware-d-script",
offset:24099,
},
],
},
{
html_file_name:"subtle_bug_with_go_errgroup.html",
title:"Subtle bug with Go\'s errgroup",
text:"Yesterday I got bitten by an insidious bug at work while working on Kratos . Fortunately a test caught it before it got merged. The more I work on big, complex software, the more I deeply appreciate tests, even though I do not necessarily enjoy writing them. Anyways, I lost a few hours investigating this issue, and this could happen to anyone, I think. Let\'s get into it. I minimized the issue in a stand-alone program in just 100 lines. You can have a look at the real production code here if you are interested. After all, it\'s open source! The program Today, we are writing a program validating passwords. Well, the most minimal version thereof. It contains the old password, takes the new password on the command line, and runs a few checks to see if this password is fine: Checks if the new password is different from the old password. This can catch the case where the old password has leaked, we want to change it, and inadvertently use the same value as before. Which would leave us exposed. Check the Have I Been Pawned API, which stores millions of leaked passwords. This serves to avoid commonly used and leaked passwords. The real production program has a in-memory cache in front of the API for performance, but we still have to do an API call at start-up and from time to time. Check that the password is long enough For simplicity, the Have I Been Pawned API in our reproducer is just a text file with passwords in clear. One last thing: passwords are (obviously, I hope) never stored in clear, and we instead store a hash using a password hashing function specially designed to take up a lot of computational power to hinder brute-force attacks. Typically, that can take hundreds of milliseconds or even seconds (depending on the cost factor) for one hash. For performance, if we have to compute this hash, we try to do other things in parallel. To achieve this, we use an errgroup , which has become pretty common place now in Go. Here goes: package main\n\nimport (\n\t&quot;context&quot;\n\t&quot;fmt&quot;\n\t&quot;io&quot;\n\t&quot;net/http&quot;\n\t&quot;os&quot;\n\t&quot;slices&quot;\n\t&quot;strings&quot;\n\n\t&quot;golang.org/x/crypto/bcrypt&quot;\n\t&quot;golang.org/x/sync/errgroup&quot;\n)\n\n// Best effort: if the external API is down, swallow the error: the user should be able to change their password nonetheless.\nfunc checkHaveIBeenPawned(ctx context.Context, pw string) error {\n\treq, err := http.NewRequestWithContext(ctx, &quot;GET&quot;, &quot;http://localhost:8000/haveibeenpawned.txt&quot;, strings.NewReader(pw))\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\tresp, err := http.DefaultClient.Do(req)\n\n\tif err != nil {\n\t\treturn nil\n\t}\n\tdefer resp.Body.Close()\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn nil\n\t}\n\n\trespBody, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn nil\n\t}\n\tlines := strings.Split(string(respBody), &quot;\\n&quot;)\n\n\tif slices.Contains(lines, pw) {\n\t\treturn fmt.Errorf(&quot;the password appears in a leak&quot;)\n\t}\n\n\treturn nil\n}\n\nfunc changePassword(ctx context.Context, oldHash []byte, newPassword string) ([]byte, error) {\n\tvar newHash []byte\n\n\tg, ctx := errgroup.WithContext(ctx)\n\t// Do in parallel:\n\t// - Compute the hash of the new password\n\t// - Check that the new password is not the same as the old password\n\tg.Go(func() error {\n\t\tvar err error\n\t\tnewHash, err = bcrypt.GenerateFromPassword([]byte(newPassword), 10)\n\t\treturn err\n\t})\n\tg.Go(func() error {\n\t\tif err := bcrypt.CompareHashAndPassword(oldHash, []byte(newPassword)); err == nil {\n\t\t\treturn fmt.Errorf(&quot;old and new password are the same&quot;)\n\t\t}\n\t\treturn nil\n\t})\n\n\tif err := g.Wait(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := checkHaveIBeenPawned(ctx, newPassword); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Reject passwords that are too short.\n\t// In real programs, this value should be much higher!\n\tif len(newPassword) &lt; 4 {\n\t\treturn nil, fmt.Errorf(&quot;password is too short&quot;)\n\t}\n\n\treturn newHash, nil\n}\n\nfunc main() {\n\tctx := context.Background()\n\n\toldPassword := &quot;hello, world&quot;\n\toldHash, err := bcrypt.GenerateFromPassword([]byte(oldPassword), 8)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tnewPassword := os.Args[1]\n\tnewHash, err := changePassword(ctx, oldHash, newPassword)\n\tif err != nil {\n\t\tfmt.Printf(&quot;failed to change password: %v&quot;, err)\n\t} else {\n\t\tfmt.Printf(&quot;new password set: %s&quot;, string(newHash))\n\t}\n} I think it is pretty straightforward. The only \'clever\' thing is using errgroup , which cancels all tasks as soon as one fails. This is handy to avoid doing unecessary expensive computations. We serve the static text file haveibeenpawned.txt using a HTTP server to act as the Have I Been Pawned API, it just contains one password per line e.g.: hello\nabc123\n123456 Let\'s try it then: # Serve the leaked password file\n$ python3 -m http.server -d . &amp;\n\n# Fine new password, succeeds\n$ go run main.go \'correct horse battery staple\'\nnew password set: $2a$10$6tIRwMIKaOZuzsT1dT3EVu5boCtautewpJYT6r5Fc6PV13i9ezcNS\n\n# Password too short\n$ go run main.go \'hi\'\nfailed to change password: password is too short\n\n# Leaked password\n$ go run main.go \'hello\'\nnew password set: $2a$10$pmOOg5VOFEIjFk47hstoQeFbgHFTzxChpsp77SuuE4yvlSK9Ds4SG\u{23ce}                          Wait, the last one should have been rejected, what\'s going on? Bug investigation We can use the debugger, or strace/dtrace, or add logs, or simply look at the python static HTTP server, the verdict is the same: no HTTP request is made in checkHaveIBeenPawned . How is this possible? I can see the Go function call! At first I thought a data race was happening between goroutines, having been recently burnt by that . But it turned out it was something different. Let\'s log the errors inside checkHaveIBeenPawned that we so conveniently swallowed: diff --git a/main.go b/main.go\nindex ae602df..17d0228 100644\n--- a/main.go\n+++ b/main.go\n@@ -23,6 +23,7 @@ func checkHaveIBeenPawned(ctx context.Context, pw string) error {\n        resp, err := http.DefaultClient.Do(req)\n \n        if err != nil {\n+               fmt.Fprintf(os.Stderr, &quot;http request error: %v\\n&quot;, err)\n                return nil\n        }\n        defer resp.Body.Close() And we see: http request error: Get &quot;http://localhost:8000/haveibeenpawned.txt&quot;: context canceled Uh...what? We do not even have a timeout set! How can the context be canceled? At that point, a great collegue of mine helped me debug and found the issue. He sent me this one line from the errgroup documentation : The derived Context is canceled the first time a function passed to Go returns a non-nil error or the first time Wait returns, whichever occurs first. Ah... that\'s why. I mean, it makes sense that the context is canceled on the first error occurring, that\'s how operations in other goroutines can notice and also stop early. It\'s just surprising to me that this applies also when Wait returns and no error happened. Ok, how do we fix it then? This was my fix in the real program: since the HTTP call is the one that does not get to run, and this could take up some time, why not also run it in a goroutine in the errgroup ? This task is completely independent from the others. Here it is, quite a short fix: diff --git a/main.go b/main.go\nindex ae602df..74371a5 100644\n--- a/main.go\n+++ b/main.go\n@@ -62,15 +62,14 @@ func changePassword(ctx context.Context, oldHash []byte, newPassword string) ([]\n                }\n                return nil\n        })\n+       g.Go(func() error {\n+               return checkHaveIBeenPawned(ctx, newPassword)\n+       })\n \n        if err := g.Wait(); err != nil {\n                return nil, err\n        }\n \n-       if err := checkHaveIBeenPawned(ctx, newPassword); err != nil {\n-               return nil, err\n-       }\n-\n        // Reject passwords that are too short.\n        // In real programs, this value should be much higher!\n        if len(newPassword) &lt; 4 { Let\'s test it then: $ go run main.go \'hi\'\nfailed to change password: password is too short\n\n$ go run main.go \'hello\'\nfailed to change password: the password appears in a leak\n\n$ go run main.go \'correct horse battery staple\'\nnew password set: $2a$10$.oyEO/cSmTWugfwdpoADYOB/AM.uHjz1HodOysS3ksIS.FS4RvTx.\u{23ce}                    It works correctly now! This is the fix I went for in the real production code . Alternative fix What if I told you there is a one character change that fixes the issue? diff --git a/main.go b/main.go\nindex ae602df..6bcbfa3 100644\n--- a/main.go\n+++ b/main.go\n@@ -47,7 +47,7 @@ func checkHaveIBeenPawned(ctx context.Context, pw string) error {\n func changePassword(ctx context.Context, oldHash []byte, newPassword string) ([]byte, error) {\n        var newHash []byte\n \n-       g, ctx := errgroup.WithContext(ctx)\n+       g, _ := errgroup.WithContext(ctx)\n        // Do in parallel:\n        // - Compute the hash of the new password\n        // - Check that the new password is not the same as the old password Why does it work? Well, the ctx we get from errgroup.WithContext(ctx) is a child of the ctx passed to our function, and also shadows it in the current scope. Then, when we call checkHaveIBeenPawned(ctx, newPassword) , we use this child context that just got canceled by g.Wait() the line before. By not shadowing the parent context, the same call checkHaveIBeenPawned(ctx, newPassword) now uses this parent context, which has not been canceled in any way. So it works as expected. We could also name the two contexts differently to avoid this pitfall. Conclusion The errgroup concept is pretty great. It greatly simplifies equivalent code written using Go channels which get real hairy real soon (you can check the diff for the real production code: the old code used two naked channels + a goroutine, the new code uses the errgroup ). But, as it is often the case in Go, you do need to read the fine print, because the type system is not expressive enough to reflect the pre- and post-conditions of APIs. Shadowing is another concept that made this issue less visible. I have had quite a few bugs due to shadowing in Go and Rust, both languages that idiomatically use this approach a lot. Some newer programming languages have outright banned variable shadowing, like Zig , I presume to prevent such issues. If you\'ve ever heard of linear types , and never saw their utility, that\'s actually exactly what they are good for: a variable gets \'consumed\' by a function, and the type system prevents us from using it after that point. Conceptually, g.Wait(ctx) consumes ctx and there is no point using this ctx afterwards. But the current Go type system does not prevent us from doing this, at all. Things get even muddier when we notice that after g.Wait(ctx) , we do the password length check, and that gets to run, contrary to the checkHaveIBeenPawned call. Since the length check does not care about a context, it runs just fine. And that\'s what puzzled me during the investigation for so long. My main take-away: if you decide to use an API, take time to first read the docs in full. Probably more than once. And read the fine prints. Consider also skimming through the implementation. It will save time overall. Finally: if the code is trivially simple, except for this one clever thing, and there is a bug, well... The bug is probably in this one clever bit of code. And again, big thanks to my colleague Patrik for finding the root cause! ",
titles:[
{
title:"The program",
slug:"the-program",
offset:545,
},
{
title:"Bug investigation",
slug:"bug-investigation",
offset:5285,
},
{
title:"Alternative fix",
slug:"alternative-fix",
offset:8301,
},
{
title:"Conclusion",
slug:"conclusion",
offset:9480,
},
],
},
],
};
export default { raw_index };